\chapter{Literature Review}
% vim: textwidth=72
% vim: foldmethod=manual

The remainder of this chapter is sectionalized. It begins with several
sections that describe other comparisons of real estate price prediction
models. These are organized by data of publication of the study. The
final section describes the contributions of this work.

\section{2003 Study of Tuscon Prices in 1998} 

In \cite{fik-03-spatial}, authors Timothy J. Fik, David C. Ling, and
Gordon F.  Mulligan study approaches for incorporating location into
house-price estimation models using data from 1998 for Tuscon, Arizona.
The data are for 2,971 sales transactions as recorded in nine
multiple-listing services systems. These system partition the real
estate space studied.

Four models were compared. All all linear in form in which the log price
is predicted using features in natural units.

\begin{itemize}

\item Model 1 was an ``aspatial'' model. It contains no location
features. Features used include the interior square footage, lot size,
and age of the house, as well as the squares of these features. Some
products and cubes of these features may have been included (the text is
ambiguous, see the note with Table 2 page 634).

\item Model 2 used indicator variables for the multiple-listing systems.
In addition the features from model 1 were used and well as interactions
with the indicator variables. Though nine multiple-listing systems were
used, only 3 indicator variables for them were included. The choice of
these variables reflects the implied expert opinions of the real estate
agents.

\item Model 3 used GPS coordinates after transforming them into $x,y$ offsets
from the most southwestern house. In addition, features from model 1
were used as well as some interactions of the $x$ and $y$ location features.

\item Model 4 used by the three multiple-listing system indicator
variables and the $x,y$ offsets. In addition, features from model 1 were
used as well as interactions of the $x,y$ features as well their squares
and cubes.
\end{itemize}



The text claims model 4 performed best as measured by the mean absolute
error and fraction of estimates with 10 percent of observed prices. The
error rates were identified by randomly selecting 500 of the 2971
transactions, training on remaining 2491 transactions and testing on the
500.

Model 3 is claimed to perform almost as well as model 4, and does
not require the expert knowledge of the real estate agents, just the
property descriptions and GPS coordinates.

Model 3 is claimed to perform better than model 2 as measured by mean
absolute error and only slightly worse as measured by fraction of
estimates within 10 percent.

Stengths of the work include:

\begin{itemize}

\item Incorporation of GPS coordinates. This work seems to be among the
first to do so.

\item Demonstrating the GPS coordinates provide a useful feature in
predicting real estate prices.

\item Demonstrating that the expert knowledge of real estate agents is
not needed: GPS coordinates provide at least as accurate measurements
and avoid the risk of using an expert who lacks insight.

\end{itemize}

Limitations of the work include: 

\begin{itemize}

\item Inclusion of only a few non-location features of the house.

\item There is some art in detemining which interactions to include and
whether the include $x$ and $y$ features in squared and cubed forms in
these interaction features.

\item The approach for estimating the generalization error is relative
weak: it uses a single hold-out test sample instead of a cross
validation procedure that would use the labelled data more thoroughly.

\end{itemize}

\section{2010 Study of Louisville Prices in 1999}

In \cite{bourassa-10-predicting}, authors Steven C. Bourassa, Evan
Cantoni, and Martin Hoesli compare hedonic methods for predicting
residential real estate price. The data are 13,000 single-family house
sales in the year 1999 in Louisville, Kentucky. Data came from the
Property Valuation Administrator for Jefferson Country, which contains
Louisville. Census data at the census block level were used. The
regressor was the log of the price.  Predictors included the house's age
and age squared and the lot size and size squared. Factors were included
for quarterly time periods.

These models were compared:

\begin{itemize}
\item An OLS model.
\item A two-stage OLS model. The first stage is used to calculate
residuals. The second stage uses the average residual of the ten nearest
neighbors as an additional feature.
\item A geostatistical model, an approach that models the covariance
matrix of residuals from a first-stage model. The key assumption of
geostatistical models is that ``the covariance [of prices] between locations depends
only on the distance between them (\cite{bourassa-10-predicting} page 142).'' 
%That is, consider
%all locations $s$ in space $F$. The data are hypothesized to be
%generated by random process $Y(s)$ so that when $s \in F$, $E(Y(s)) = \mu$, in
%other words, $Y$ has mean $\mu$ within region $F$. Now consider any two
%points $s_1$ and $s_2$. We can draw samples from the random process $Y$
%and measure the covariance between the value of $Y$ at each $s$,
%$Cov(Y(s_1), Y(s_2)).$ The key assumption in geostatistical analysis is
%that the coveriance $Cov$ of the measured values does not depend on the
%locations themselves, but only on the distance between, a great
%simplification made for modeling purposes. Thus, one assumes that
%function $C$ exists such that
%$Cov(Y(s_1), Y(s_2)) = C(s_1 - s_2)$. (This presentation follows the
%BCH paper.)
\item A ``trend surface'' method. Five features are used: lot size,
interior space, age, latitude, and longitude. Then the squares and cubes
of these features are added, giving 15 features. Then all $15 \times 14$ pairwise
interactions of the 15 features are also added. A linear model is then
fit to the $5 + 15 + 15 \times 14$ features.
\end{itemize}

Most models were fit once for the entire market, and several times for
submarkets. The submarket models were sometimes defined by indicator
variables for submarket and sometimes by models trained for entire
submarkets.  Submarkets were defined in several ways, including starting
with census blocks and building up neighborhoods by merging census
blocks with similar house values.

The comparison approach was to train models on 74 percent of the date
and determine the errors on the remaining 26 percent of the data. This
procedure was repeated 100 times for different random draws. 

The key error metrics used were the fraction of the test transactions
within 10 and 20 percent of the known true values.

The study claims that accuracy is improved by including submarkets and
that more narrowly defined submarkets are better than more broadly
defined submarkets. The study reached no conclusion on whether the
indicator variable or entire submarket approach was more accurate. The
most accurate model was a geostatistical model with indicator variables
for submarkets, a surprise to me given the simplified assumptions in the
geostatistical model. The triumph of the simplifying assumptions in the
geostatistical model suggests that improvement are to be found.

There are several limitations of the study. One is that the computer
codes were implemented in Splus, a commercial package, and the S+
Spatial Toolbox was used. Updating the work to open source tools would
encourage reproducability.  Another limitation is that the data are
proprietary to the study team. I spoke with Bourassa and was told that
his license for the data did not allow sharing of the data with me.
Another limitation is that the data are for one year and the size of the
data set is relative small (13,000 transactions).

%In the OLS models at
%least, indicator variables for calendar quarters were used, raising the
%possibility that the models know future prices (there is not sufficient
%discussion in the text to know whether this is actually true or false.)
%We take extradinary care in managing our training data to exclude any
%data from the future of a query transaction.

The main strengths of the work are the use of a single data set to
compare very different models. It is a shame that the modeling work
cannot be extended to also test other techniques.

\section{Contributions of This Work}

This work extends the literature in several ways.

\begin{itemize}

\item Open sources all of the software. The implementation is entirely
in the R programming language \cite{r-14}. All the source code is available in
the author's github acount \code{rlowrance}. The license is the GNU
General Public License Version 3.

\item Most prior studies focus on one model form, often the log-level
form in which the log of the price is estimated using features in
natural measurement units. This work studies whether this model form is
in fact best for predictive purposes.

\item Most prior studies fix a feature set and use it for prediction.
This work studies potential feature sets and proposes a simple heuristic
that is then used to pick the best feature set.

\item Most prior studies are focused on on year of data. We use multiple
years of data.

\item Most prior studies are for relatively small data sets. Our data
set is for all of Los Angeles County, the most populus country in the
United States \cite{census-bureau-14-population-clock}.

\item Most prior studies measure goodness of fit of the tested model
using some variant of the metric ``fraction of estimated values within
10 percent of the actual value.'' We test this metric against other choices.

\item Most prior studies use only information available from tax
asessors and records of deeds. We extend this information to include
information available from the U.S. Census Bureau.

\end{itemize}


Some limitations of this work include:

\begin{itemize}

\item The dataset is proprietary to the study. Thus even though the
source code is available, results cannot be replicated on these data.

\item The work reported on here does not use the GPS coordinates for the
properties. Other work has found that leveraging the GPS coordinates can
improve predictive accuracy.

\end{itemize}
