\chapter{Literature Review}
% vim: textwidth=72
% vim: foldmethod=manual

The remainder of this chapter is sectionalized. It begins with several
sections that describe other comparisons of real estate price prediction
models. These are organized by data of publication of the study. The
final section describes the contributions of this work.

\section{2010 Louisville}

In \cite{bourassa-10-predicting}, authors Steven C. Bourassa, Evan
Cantoni, and Martin Hoesli compare hedonic methods for predicting
residential real estate price. The data are 13,000 single-family house
sales in the year 1999 in Louisville, Kentucky. Data came from the
Property Valuation Administrator for Jefferson Country, which contains
Louisville. Census data at the census block level were used. The
regressor was the log of the price.  Predictors included the house's age
and age squared and the lot size and size squared. Factors were included
for quarterly time periods.

These models were compared:

\begin{itemize}
\item An OLS model.
\item A two-stage OLS model. The first stage is used to calculate
residuals. The second stage uses the average residual of the ten nearest
neighbors as an additional feature.
\item A geostatistical model, an approach that models the covariance
matrix of residuals from a first-stage model. The key assumption of
geostatistical models is that ``the covariance [of prices] between locations depends
only on the distance between them (\cite{bourassa-10-predicting} page 142).'' 
%That is, consider
%all locations $s$ in space $F$. The data are hypothesized to be
%generated by random process $Y(s)$ so that when $s \in F$, $E(Y(s)) = \mu$, in
%other words, $Y$ has mean $\mu$ within region $F$. Now consider any two
%points $s_1$ and $s_2$. We can draw samples from the random process $Y$
%and measure the covariance between the value of $Y$ at each $s$,
%$Cov(Y(s_1), Y(s_2)).$ The key assumption in geostatistical analysis is
%that the coveriance $Cov$ of the measured values does not depend on the
%locations themselves, but only on the distance between, a great
%simplification made for modeling purposes. Thus, one assumes that
%function $C$ exists such that
%$Cov(Y(s_1), Y(s_2)) = C(s_1 - s_2)$. (This presentation follows the
%BCH paper.)
\item A ``trend surface'' method. Five features are used: lot size,
interior space, age, latitude, and longitude. Then the squares and cubes
of these features are added, giving 15 features. Then all $15 \times 14$ pairwise
interactions of the 15 features are also added. A linear model is then
fit to the $5 + 15 + 15 \times 14$ features.
\end{itemize}

Most models were fit once for the entire market, and several times for
submarkets. The submarket models were sometimes defined by indicator
variables for submarket and sometimes by models trained for entire
submarkets.  Submarkets were defined in several ways, including starting
with census blocks and building up neighborhoods by merging census
blocks with similar house values.

The comparison approach was to train models on 74 percent of the date
and determine the errors on the remaining 26 percent of the data. This
procedure was repeated 100 times for different random draws. 

The key error metrics used were the fraction of the test transactions
within 10 and 20 percent of the known true values.

The study claims that accuracy is improved by including submarkets and
that more narrowly defined submarkets are better than more broadly
defined submarkets. The study reached no conclusion on whether the
indicator variable or entire submarket approach was more accurate. The
most accurate model was a geostatistical model with indicator variables
for submarkets, a surprise to me given the simplified assumptions in the
geostatistical model. The triumph of the simplifying assumptions in the
geostatistical model suggests that improvement are to be found.

There are several limitations of the study. One is that the computer
codes were implemented in Splus, a commercial package, and the S+
Spatial Toolbox was used. Updating the work to open source tools would
encourage reproducability.  Another limitation is that the data are
proprietary to the study team. I spoke with Bourassa and was told that
his license for the data did not allow sharing of the data with me.
Another limitation is that the data are for one year and the size of the
data set is relative small (13,000 transactions).

%In the OLS models at
%least, indicator variables for calendar quarters were used, raising the
%possibility that the models know future prices (there is not sufficient
%discussion in the text to know whether this is actually true or false.)
%We take extradinary care in managing our training data to exclude any
%data from the future of a query transaction.

The main strengths of the work are the use of a single data set to
compare very different models. It is a shame that the modeling work
cannot be extended to also test other techniques.

\section{Contributions of This Work}

This work extends the literature in several ways.

\begin{itemize}
\item Open sources all of the software. The implementation is entirely
in the R programming language \cite{r-14}. All the source code is available in
the author's github acount \code{rlowrance}. The license is the GNU
General Public License Version 3.

\item Most prior studies focus on one model form, often the log-level
form in which the log of the price is estimated using features in
natural measurement units. This work studies whether this model form is
in fact best for predictive purposes.

\item Most prior studies fix a feature set and use it for prediction.
This work studies potential feature sets and proposes a simple heuristic
that is then used to pick the best feature set.

\item Most prior studies are focused on on year of data. We use multiple
years of data.

\item Most prior studies are for relatively small data sets. Our data
set is for all of Los Angeles County, the most populus country in the
United States \cite{census-bureau-14-population-clock}.

\item Most prior studies measure goodness of fit of the tested model
using some variant of the metric ``fraction of estimated values within
10 percent of the actual value.'' We test this metric against other choices.

\end{itemize}
