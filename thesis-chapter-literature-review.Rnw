\chapter{Literature Review}
% vim: textwidth=72
% vim: foldmethod=manual

The literature on what works in predicting housing prices is thin.
Several detractors keep researchers away from the field.  The
primary one is the absence of sharable data sets.  Yes, it is true that
all the data one needs are in the public domain: tax assessments are
held by county tax assessors, deeds are recorded publicly by the county
recorder of deeds, the U.S. Census Bureau publishes the census data,
GPS coordinates of houses are in the public domain.

However, public and free are two very different things. Many counties
sell their tax assessment and deeds data. For example, I was offered the
opportunity to buy data for one county for a dollar a record. Sometimes
the data are priced and sometimes are free. It seems that the
Louisville, Kentucky tax assessor has made Louisville data freely
available to some researchers at universities in Louisville, but I (in
New York City) was given the chance to buy the data. 

Even freely available data may be difficult to use. For example,
CoreLogic is one of the largest resellers of U.S. real estate data. They
have a partnership which obtains the data sets from the tax authorities
and then cleans and augments them and finally places them into a
canonical form, because not all counties produce the same data sets for
their tax rolls and deeds. The resulting data are available under
license. The licensed data are much easier to use than the raw,
from-the-tax-authorities data. However, the licenses tend to be
restrictive, in order to protect the significant downstream work after
the data are gathered. The license under which I have access to the Los
Angeles County data is from CoreLogic. That license permits access only
by specifically-named people. Most of the people who have worked with me
on this project may not see the raw data. 

The licenses are expensive. One reason is the high value of the
data to many companies that want to invest in real estate, in mortgages,
or in securities derived from mortgages. Another reason for high prices
in that there are relatively few providers of the cleaned-up data. Yet
another reason for high prices is that companies selling the data also
sell real estate price-prediction models that use the same data.  The
predictions are said to sell for \$8 to \$20 each, creating a
substantial market in price estimates, so the model providers have
little incentive to undermine their own models. I approached one
potential data provider asking for a donation of real estate data. The
key issue quickly became: Why should we help you compete with us?

A secondary reason for thin literature is an absence of a tradition of
making one's software available in open source form. Of course, there is
less motivation to do so if no one can run your software because it is
tied to specific, proprietary data sets. Indeed, there is a lack of
generally usable tools such as an R library that supports real estate
analytics. Why develop generalized tools if no one can get the data to
use the tools?

This literature review is focused on studies that compare real estate
price prediction models with one another. What follows are sections that
correspond to the studies. The final section describes the contributions
of this work.

Here are the studies we review:

\begin{itemize}

\item A 2003 study of Auckland prices in 1996
\item A 2003 study of Tucson prices in 1998
\item A 2004 study of Fairfax Country prices in 1967 through 1991
\item A 2009 study of Los Angeles prices in 2004
\item A 2010 study of Louisville prices in 1999
\item A 2011 study of Louisville prices in 2003 through 2007.
\end{itemize}

\section{2003 Study of Auckland Prices in 1996}

In \cite{bourassa-03-submarkets} authors Steven C. Bourassa, Martin E.
Hoesli, and Vincent S. Peng studied housing prices in Auckland, New
Zealand in 1996. Their objective was to determine the extent to which
housing submarkets matter.

The idea of introducing housing submarkets into real estate price models
is this: rather than develop a model for an entire city, partition the
city into submarkets and develop a model for each submarket or,
alternatively, fit one model and include in it an indicator variable for
each submarket. The motivation for introducing submarkets is to improve
prediction accuracy. It is clear that not all submarket definitions will
improve accuracy: for example, define every house to be in its own
submarket, and then the only training data for a house is the history of
prices for that house.  It's hard to see how this model could be
accurate.

So the problem is to find a definition of submarkets that improves
accuracy. The text considered two definitions. For the first, submarkets
were defined as the ``sales groups'' used by the Auckland government real
estate tax appraisers.  ``The sales groups are geographic areas
considered by the appraisers to be relatively homogeneous.''

For the second definition of submarkets, principal components analysis
(PCA) was used.  Two sets of PCA-based clusters were defined. The first
was based on all the PCA components accounting for 80 percent of the
variance in price. The second was based on the first two principal
components. The extracted clusters were defined to be the submarkets;
they were not required to hold contiguous houses.

Five models were developed.

\begin{itemize}
\item Model 1 was for the entire market. The features set was from the
tax assessment augmented by indicator variables for each quarter.

\item Model 2 was like model 1, but included indicator variables for the
sales groups.

\item Model 3 was a separate model for each sales group.

\item Model 4 was a separate model for the first set of PCA-based
clusters.

\item Model 5 was a separate model for the second set of PCA-based
clusters.

\end{itemize}

Various subsets of the houses and features were tested. We focus here on
results for detached houses (corresponding to our interest in
single-family residences), for the text's restricted data set (using
only variables readily available from the tax assessor), and for
non-spatially adjusted data (the spatial adjustments add complications
without changing the conclusions of interest to us).

A resampling technique was used to compare models. The comparison metric
was a measure of accuracy: fraction of estimated prices that are within
10 and 20 percent of their true values. Root mean squared errors (RMSE)
were not reported.

Based on the fraction of estimates within 10 percent of true values, the
models using the sales groups outperformed the models using the clusters
found through PCA. Thus, reliance on experts trumped reliance on the
specific clustering algorithms used. Other automated means to find
clusters may outperform the human experts. Models with clusters always
outperformed model 1, the model for the entire market. Model 2, which
used indicator variables for the sales groups, slightly outperformed
model 3, which used a separate model for each sales group.

The strength of this study is the demonstration that submarkets are
important and the discovery that, at least in Auckland in 1999, the
human expert tax authorities outperformed two PCA-based clustering
algorithms.

A weakness, not of the study but of the literature, is a lack of
follow up in attempting to construct algorithmic clustering techniques
that outperform the human experts. Such an effort would require data, of
course.

\section{2003 Study of Tucson Prices in 1998} 

In \cite{fik-03-spatial}, authors Timothy J. Fik, David C. Ling, and
Gordon F.  Mulligan studied approaches for incorporating location into
house-price estimation models using data from 1998 for Tucson, Arizona.
The data were for 2,971 sales transactions as recorded in the nine
multiple listing services systems that cover Tucson.

Four models were compared. All are linear in form and predict the log
price using features in natural units.

\begin{itemize}

\item Model 1 was an ``aspatial'' model. It contains no location
features. Features used included the interior square footage, lot size,
and age of the house, as well as the squares of these features.

\item Model 2 used indicator variables for the multiple listing systems,
which partition Tucson.  In addition, the features from model 1 were
used as well as interactions with the indicator variables. Though nine
multiple listing systems were used, only three indicator variables for
them were included. The choice of these variables reflects the expert
opinions of real estate agents, who grouped some of the multiple listing
services.

\item Model 3 used GPS coordinates after transforming them into $x,y$
offsets from the most southwestern house. The feature set was built in
several steps. First, all the $x,y$ and house features from model 1 were
included. Second, the squares and cubes of all these features were added
to the feature set. Finally, the products of all plain, squared, and
cubed features were added.

\item Model 4 used all the features from model 2 and 3 combined, so it
had both the expertise of the real estate agents and the $x,y$
locations.

\end{itemize}

The text claimed that model 4 performed best as measured by the mean
absolute error and fraction of estimates with 10 percent of observed
prices. The error rates were identified by randomly selecting 500 of the
2971 transactions, training on remaining 2491 transactions and testing
on the 500.

Model 3 was claimed to perform almost as well as model 4, and did
not require the expert knowledge of the real estate agents, just the
property descriptions and GPS coordinates.

Model 3 was claimed to perform better than model 2 as measured by mean
absolute error and only slightly worse as measured by fraction of
estimates within 10 percent.

Strengths of the work include:

\begin{itemize}

\item Incorporation of GPS coordinates. This work seems to be among the
first to do so.

\item Showing that the GPS coordinates provide a useful feature in
predicting real estate prices.

\item Demonstrating that the expert knowledge of real estate agents is
not needed and providing a simple-to-understand method of incorporating
GPS coordinates. The text's method for incorporating GPS coordinates has
been followed by some other studies.

\end{itemize}

Limitations of the work include: 

\begin{itemize}

\item Inclusion of only a few non-location features of the house.

\item The approach for estimating the generalization error is relatively
weak: it uses a single holdout test sample instead of a cross
validation procedure that would use the data more thoroughly.

\end{itemize}

\section{2004 Study of Fairfax County Prices in 1967 through 1991}

In \cite{case-04} authors Bradford Case, John Clapp, Robin Dubin, and
Mauricio Rodriguez reported on a competition to build house price
prediction models for single-family residences in Fairfax County,
Virginia from 1967 through 1991.  The time period was long but covers
only 60,000 transactions.

The model-building efforts were organized as a competition. The data
were split into training and testing sets. One participant kept the
testing data and scored the predictions from others.

The data were from the tax assessor, a GPS coding firm, and the 1990
U.S.  decennial census. They included both housing characteristics (such
as land area, number of rooms, and so forth), GPS coordinates, and
census tract data. The census tract data were based on 1990 census
tracts.

A large number of models were built in several families.

\begin{itemize}

\item Model family 1 was based on an ordinary least squares (OLS) model
with housing features and with what was called a ``trend surface.'' The
trend surface was modeled by including latitude and longitude, the
squares of these features, and the product of these features. The trend
surface is very similar to the $x,y$ approach from
\cite{fik-03-spatial}, which was reviewed in the previous section.  A
variant in the model 1 family included indicator variables for census
tracts, building what is now called submarket models. Model 1 variants
also included indicator variables for the years.

\item Model family 2 was based on a kind of local regression in which
the log of the price is modeled as the sum of a linear function of
house features and a possibly non-linear function of the house's
location in space and time. One variant in this model worked in two
stages. In stage one, residuals for all transactions were determined. In
stage two, the residuals from the nearest fifteen neighbors of each
house sold within the last three years were included as features in the
linear portion of the model.

\item Model family 3 was a set of linear models in which the error terms
were allowed to be correlated and the covariance matrix of the error
terms was modeled explicitly. In the literature, these kinds of models
are called both geostatistical and kriging models. The model was (we
follow the text, page 176) $Y = \beta X + \mu$, where $\mu$ is drawn
from $N(0, \sigma^2 K)$, a change from the standard OLS model in which
$\mu$ is drawn from $N(0, \sigma^2 I)$, where $I$ is the identity
matrix. If $K$ were estimated by $\hat K$, then the estimate for $\beta$
would be $\hat \beta = (Y'{\hat K}^{-1} X) ^ {-1} X' {\hat K} ^ {-1} Y$.

The problem reduces to estimating $K$, which is a square matrix of size
$n$, the number of observations. This is done by assuming a form for
$K$. Kriging, a technique within geostatistics,  is based on the
assumption that $K$,  the correlation matrix for the error terms, is a
function solely of the distance between each observation $i$ and $j$.
The model builder Robin Dubin assumed this form for $K$: $K_{ij} = b_1
\exp(-d_{ij}/ b_2)$ when $ i \neq j$ and $K_{ij} = 1$ when $i = j$. Here
$d_{ij}$ was the physical distance separating house $i$ and $j$.  The
parameters $K_{ij}$, $b_1$, $b_2$, and $\sigma$ were estimated through
maximum likelihood.

One advantage of kriging is that it explicitly models the error and
thus allows the predicted error to be added back to the estimate
from the linear model.  A separate local model was built for each query
transaction.  Variants included inclusion or not of census tract data,
testing of several methods to estimate the covariance of the error
terms, and tuning of hyperparameters used to selected samples for
inclusion in the local models.

\item Model family 4 was a set of linear models with ``homogeneous
[submarkets] and nearest-neighbor residuals.'' The submarkets were found
through $K$-means clustering of features of census tracts. The census
tract features were the fitted parameters of separate linear models for
each census tract. The optimal number of submarkets was found to be 12.
This model family was a two-stage model, in which residuals from the
nearest five properties from the first stage were used as features in
the second stage. A separate model was estimated for each submarket.

\end{itemize}

Models were built by the builders and tested by the judge. Feedback was
given to the builders.  The model builders then tuned their models and
resubmitted.  The judge computed a handful of metrics, all variants on
mean and median errors (but not fraction within $x$ percent).

% My terminology | Paper
% Model 1 | Clapp OLS
% Model 2 | Clapp LRM local regression
% Model 3 | Dubin
% Model 4 | Case

<<case>>=
names  <- c('1a',  '1b',  '2a',  '2b', '3',   '4')
errors <- c(11520, 11768, 11892, 11644, 11079, 11500)
min.error <- min(errors)
excess.errors <- errors / min.error
excess.errors
@

We focus here on results as measured by the root median squared
prediction error, a metric that we use in our own work. Under this
metric, all models performed within 7.3 percent of each other. The best
model was a variant of model 3, a local kriging model.  The text claims
that it is ``computationally intensive.'' The software to implement it
was available at time of publication from Robin Dubin and was written in
the Gauss programming language.

One of the OLS variants had errors 4.0 percent higher than the best
model; the other OLS variant had errors 6.2 percent higher than the best
model.

The strengths of the study include:
\begin{itemize}
\item The use of a single data set by multiple model builders
\item The long time period for the data sets
\item Inclusion of many time periods in the testing sample.
\end{itemize}

A weakness is the use of a single test data set. More modern approaches
would use some variant on cross validation.

A weakness is the use of 1990 census tract values for transactions
before the date the 1990 census became available, probably in roughly
1993. More appropriate would have been to use data from the decennial
censuses in 1960, 1970, and 1980 as well as the 1990 census values. 


\section{2009 Study of Los Angeles Prices in 2004}

In \cite{chopra-09} author Summit Prakash Chopra developed factor graphs
for relational regression, an approach intended to capture ``the
underlying relation structure'' of the data. He develop the approach
and illustrated its value using two real estate price prediction
problems. Here we review the first of these price prediction problems
because its setting is most like other price prediction studies in this
literature review.

Five models were compared:

% see page 101 for the results
\begin{itemize}

\item Model 1 was a nearest neighbor model using 90 neighbors, a value
found through experimentation. The distance metric was the Euclidean
metric on the entire feature space with equally weighted coordinates.


\item Model 2 was linear regression using an L2 regularizer. The
features used were the tax assessor features.


\item Model 3 was a locally-weighted regression. Here ``local'' means
that a separate linear model was constructed for each query transaction.
The training set for the query was the $K$ nearest neighbors to the
query.  The distance metric was the Euclidean distance in the feature
space and the GPS coordinates. Distances were converted to weights via a
Gaussian kernel (called exponential in the text). $K$ was set to 70
through experimentation.

\item Model 4 was a neural network with two hidden layers.

\item Model 5 was a ``relational factor graph,'' a model developed for
the study. This model is designed to determine neighborhood
desirability for all houses collectively. 

\end{itemize}

The models were compared by training on the first 90 percent of
transactions in 2004 and then estimating the values for the transactions
in the last 10 percent of 2004. The size of the data set was about 42,000
transactions.

Here we report the study's results for the metric fraction of estimated
values within 10 percent of the true values. The accuracy of the models
is in the order listed above: model 1 was least accurate (47 percent of
transactions were within 10 percent of true values), model 5 was
most accurate (66 percent within 10 percent). The 66 percent figure is a
high figure compared to other studies, however, none of the studies are
repeatable as all of the data are proprietary. Moreover, the time period
for the predictions was about 5 weeks, much shorter than for most other
studies.

A strength of the study is the development of the relational factor
graph approach.

%A gap in the literature is to compare the value of the desirability of
%the neighborhoods as determined by the factor graph approach with
%easier-to-compute and easier-to-understand approaches. One such approach
%is to use the $Location$ feature from the Louisville 2003 - 2007 study
%review later.  Another such feature is the census tract median household
%income, which is used in the present work. How well models built with
%these features perform is work that has not yet been carried out.
%
%The great result obtained by the relational factor graph begs to be
%tested in other studies.

\section{2010 Study of Louisville Prices in 1999}

In \cite{bourassa-10-predicting}, authors Steven C. Bourassa, Evan
Cantoni, and Martin Hoesli compared hedonic methods for predicting
residential real estate price. The data were 13,000 single-family house
sales in the year 1999 in Louisville, Kentucky. Data came from the
Property Valuation Administrator for Jefferson County, which then
contained Louisville. (Louisville and Jefferson County subsequently
merged.) Census data at the census block level were used. The regressor
was the log of the price.  Predictors included the house's age and age
squared and the lot size and lot size squared. Indicator variables were
included for quarterly time periods.

These models were compared:

\begin{itemize}

\item An OLS model.

\item A two-stage OLS model. The first stage was used to calculate
residuals. The second stage used the average residual of the ten nearest
neighbors as an additional feature.

\item A geostatistical model, an approach that models the covariance
matrix of residuals from a first-stage model. The key assumption of
geostatistical models is that ``the covariance [of residuals] between
locations depends only on the distance between them
\cite[p.~142]{bourassa-10-predicting}.'' This model is similar to the
geostatistical model in the 2004 study of Fairfax County \cite{case-04}.

%That is, consider
%all locations $s$ in space $F$. The data are hypothesized to be
%generated by random process $Y(s)$ so that when $s \in F$, $E(Y(s)) = \mu$, in
%other words, $Y$ has mean $\mu$ within region $F$. Now consider any two
%points $s_1$ and $s_2$. We can draw samples from the random process $Y$
%and measure the covariance between the value of $Y$ at each $s$,
%$Cov(Y(s_1), Y(s_2)).$ The key assumption in geostatistical analysis is
%that the coveriance $Cov$ of the measured values does not depend on the
%locations themselves, but only on the distance between, a great
%simplification made for modeling purposes. Thus, one assumes that
%function $C$ exists such that
%$Cov(Y(s_1), Y(s_2)) = C(s_1 - s_2)$. (This presentation follows the
%BCH paper.)

\item A ``trend surface'' method. Five features were used: lot size,
interior space, age, latitude, and longitude. Then the squares and cubes
of these features were added, giving 15 features. Then all $15 \times 14$ pairwise
interactions of the 15 features were also added. A linear model was then
fitted to the $5 + 15 + 15 \times 14$ features. This model is similar to
Model 3 of the 2003 Tucson study \cite{fik-03-spatial}.

\end{itemize}

Most models were fitted once for the entire market and several times for
submarkets. The submarket models were sometimes defined by indicator
variables for the submarket and sometimes by models trained for entire
submarkets.  Submarkets were defined in several ways, including starting
with census blocks and building up neighborhoods by merging census
blocks with similar house values.

The comparison approach was to train models on 74 percent of the data
and determine the errors on the remaining 26 percent of the data. This
procedure was repeated 100 times for different random draws. 

The key error metrics used were the fraction of the test transactions
within 10 and 20 percent of the known true values.

The study claims that accuracy was improved by including submarkets and
that more narrowly defined submarkets were better than more broadly
defined submarkets. The study reached no conclusion on whether the
indicator variable or entire submarket approach was more accurate. The
most accurate model was a geostatistical model with indicator variables
for submarkets, a surprise to me given the simplified assumptions in the
geostatistical kriging model. Perhaps improvements in modeling the
errors are possible.

There are several limitations of the study. One is that the computer
codes were implemented in Splus, a commercial package, and the S+
Spatial Toolbox was used. Updating the work to open source tools would
encourage reproducibility.  Another limitation is that the data are
proprietary to the study team. I spoke with Bourassa and was told that
his license for the data did not allow sharing of the data with me.
Another limitation is that the data are for one year and the size of the
data set is relatively small (13,000 transactions).

%In the OLS models at
%least, indicator variables for calendar quarters were used, raising the
%possibility that the models know future prices (there is not sufficient
%discussion in the text to know whether this is actually true or false.)
%We take extradinary care in managing our training data to exclude any
%data from the future of a query transaction.

The main strengths of the work are the use of a single data set to
compare very different models. It is a shame that the modeling work
cannot be extended to also test other techniques independent of the
original study team.

\section{2011 Study of Louisville Prices in 2003 - 2007}

In \cite{zurada-11-comparison} authors Jozef Zurada, Alan S. Levitan,
and Jian Guan compared regression and ``artificial intelligence'' methods
for predicting real estate prices. The study used Louisville data for
2003 through 2007. 

Seven models were compared (descriptions are from the text):

\begin{itemize}

\item MRA: multiple regression analysis using features from the tax
assessor. 

\item NN: neural network.

\item M5P trees: a decision tree with a linear regression model built at
each leaf.

\item Additive Regression (aka, gradient boosting): an ensemble method
that repeatedly adds models that best maximize predictive performance.

\item SVM-SMO regression: support vector machines optimized not with 
quadratic programming but with sequential minimal optimization, which is
claimed to be faster.

\item RBFNN: a neural network variant with one hidden layer where the
hidden layer is a radial Gaussian activation function; hence a radial
basis function neural network.

\item MBR: memory-based reasoning, which was not defined in the text.
Possibly it was the average of the 10 nearest neighbors (based on the
headings in Exhibits 8, 9, and 10).

\end{itemize}


The data set came from the Louisville tax assessor. After processing, it
contained 16,366 sales in years 2003 - 2007 (hence before the real
estate crash) and 18 features (the text sometimes says 16 features).


Five ``scenarios'' were studied.

\begin{itemize}

\item Scenario 1 used the data from the tax collector. There was one
model for the entire market.

\item Scenario 2 used all the features from scenario 1 and a feature
called $Location$ meant to represent neighborhood desirability: the mean
price of properties within a tax assessor district, which is designed by
the tax assessor to contain 10 to 50 properties. What time periods were
used was not specified. There was one model for the entire market. (A
better name for $Location$ would be $NeighborhoodValue$.)

\item Scenario 3 used $K$-means to create 3 clusters using all the
features from scenario 2 and the sales price. A Euclidean distance was
used, most likely with equal weighting of the coordinates, as coordinate
weighting was not discussed. A separate model was built for each
cluster.

\item Scenario 4 used $K$-means clustering to create 5 clusters using just
the $Location$ feature from scenario 2 and sales price. A separate model
was built for each cluster.

\item Scenario 5 used $K$-means clustering to create 5 clusters using
$Location$, sale price, age, and interior square footage. A separate
model was built for each cluster.

\end{itemize}

Each of the seven model forms was compared on each of the five
scenario-cluster combinations.  Comparisons were based on 10-fold cross
validation. Cross validation was repeated: if each fold had more than
5000 observations, the cross validation was repeated 3 times with new
random draws; otherwise it was repeated 10 times.  It is claimed (but
not substantiated) that this procedure is ``sufficient to achieve
stabilization of cumulative average error'' (page 368).

Models are compared on a five metrics including RMSE and excluding
fraction within $x$ percent.



<<zurada>>=
names <- c(     'mra',  'nn', 'rbf', 'svm', 'mbr',  'ar', 'm5p')
# RMSE results by scenario s.1 .. s.5 and cluster c.a .. c.d
sc <- list(
 s.1           = c(39288, 38235, 38641, 39439, 36985, 35281, 35802)

,s.2           = c(31472, 30755, 32949, 31651, 33729, 28139, 28614)

,s.3.cluster.a = c(36235, 43010, 37317, 36022, 43950, 34719, 35996)
,s.3.cluster.b = c(28460, 34389, 30777, 28137, 33951, 27155, 27255)
,s.3.cluster.c = c(23152, 26589, 25386, 23080, 26125, 22002, 22035)

,s.4.cluster.a = c(40871, 44075, 39069, 41044, 46529, 38966, 38970)
,s.4.cluster.b = c(23033, 24897, 23173, 22967, 24979, 21709, 22280)
,s.4.cluster.c = c(22652, 24869, 22786, 22659, 24670, 21820, 21906)
,s.4.cluster.d = c(21176, 24564, 21403, 21026, 24646, 20854, 20937)
 
,s.5.cluster.a = c(46557, 48761, 47865, 46574, 56306, 45529, 45459)
,s.5.cluster.b = c(40419, 39809, 41352, 40575, 43793, 40343, 40362)
,s.5.cluster.c = c(44828, 46143, 46586, 44828, 53827, 44313, 44239)
,s.5.cluster.d = c(26464, 24965, 30269, 27107, 30257, 25699, 25708)
,s.5.cluster.e = c(22817, 22168, 26991, 22876, 25286, 22114, 22158)
)

mra.div.by.smallest <- NULL
DivBySmallest <- function(v) {
min.value <- min(v)
result <- v / min.value
print(result)
mra.div.by.smallest[[length(mra.div.by.smallest) + 1]] <<- result[[1]]
}
for (name in names(sc)) {
print(name)
DivBySmallest(sc[[name]])
}
mra.div.by.smallest
mean(mra.div.by.smallest)
@

We undertook our analysis of the text's results, relying exclusively
on the RMSE metric. We found:

\begin{itemize}

\item Adding the $Location$ feature ($NeighborhoodValue$) always
improved estimates.

\item Some clusters have lower estimated RMSE values while others have
higher estimated RMSE values. The clusters here are equivalent to
submarkets in other studies and this finding is typical.

\item Additive Regression performs best in 12 of the 14 scenario-cluster
combinations. When it doesn't perform best, it is at most 1.3 percent
worse than the best performing model. When Additive Regression was not
the best performing model, the best performing model was M5P in one case
and NN in other.

\item MRA is never the best performing model, but never the worse
performing model.

\item MRA's RMSE as a fraction of that of the best performing model ranges
between 1.015 (1.5 percent worse than the best model) and 1.118 (11.8
percent worse than the best model). The mean was 1.049, so that MRA
performs on average 4.9 percent worse than the best model.

\end{itemize}

The main strengths of the text are its multi-year nature and testing of
many different models and scenarios.

The main limitations are the proprietary data set, the failure to
define the MBR method, and, the biggest disappointment, an absence of
detail on the additive regression model. 

I asked for a copy of the data sets and was told by Alan Levitan, one of
the authors, that a strict confidentiality and nondisclosure agreement
was imposed by the Louisville tax assessor. He referred me to the tax
assessor's website, which offers to sell the data. (All the text's
authors were at the University of Louisville when the study was
published.)

I was not successful in obtaining a copy of the code used in the text.
Clearly the research community for real estate analytics is never going
to be able ``to stand on the shoulders of giants'' (the metaphor is
attributed to Bernard of Chartes by John of Salisbury in a book
published in 1159 \cite[p.~167]{salisbury-1159}). Though we may be able
to identify the giants (the text study is one of them), we can't use
their data nor leverage their software. 


\section{Contributions of This Work}

This work extends the literature in several ways.

\begin{itemize}

\item Open sources all of the software. The implementation is entirely
in the R programming language \cite{r-14}. All the source code is available in
the author's Github account \code{rlowrance}. The license is the GNU
General Public License Version 3. Most of the modeling code is written
in a generic way so that it could be incorporated into a reusable
library. The only data-source specific code is in the code that cleans
up the data sets. All the modeling code is designed to be adapted to
work with any data set.

\item Inclusion of transactions both before and after the 2007 real
estate crash. None of the other studies reviewed includes data after the
crash. It is possible that the crash invalidated some models.

\item Price levels for real estate are potentially changing all the
time. The most typical approach in the literature is to capture
period-specific price levels through indicator variables, sometimes
quarterly, sometimes annually. Our approach is to directly consider the
effect of increasing or decreasing the training period on model
accuracy, thus implicitly capturing price level effects. (Time period
indicators may cause the future to appear in models, as the time period
indicator coefficients may be been set using data after the query
transaction. None of the studies reviewed here mention this concern.)

\item Most prior studies focus on one model form, often the log-level
form in which the log of the price is estimated using features in
natural measurement units. This work studies whether this model form is
in fact best for predictive purposes.

\item Most prior studies fix a feature set and use it for prediction.
This work studies potential feature sets and uses a simple heuristic
(potentially new) that is then used to pick the best feature set.

%\item Most prior studies are focused on one year of data. We use multiple
%years of data.

\item Most prior studies are for relatively small data sets. Our data set
is for all of Los Angeles County, the most populated county in the
United States \cite{census-bureau-14-population-clock}.

\item Most prior studies measure goodness of fit of the tested model
using some variant of the metric ``fraction of estimated values within
10 percent of the actual value.'' We test this metric against other choices.

\item We avoid using future information in fitting models.  We avoid the
future through careful design of the data sets and by fitting a model to
each query transaction in a way that the all data on or after the date
of the query transaction are invisible to the fitting process. 

\end{itemize}


Some limitations of this work include:

\begin{itemize}

\item The data set is proprietary to the study. Thus even though the
source code is available, results cannot be replicated on these data.

\item The work reported on here does not use the GPS coordinates for the
properties. Other work has found that leveraging the GPS coordinates can
improve predictive accuracy.

\end{itemize}
