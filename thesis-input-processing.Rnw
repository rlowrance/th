% vim: textwidth=72

% STYLE GUIDE
% not dataset, but data set
%\input{thesis-prologue}

\documentclass[10pt]{amsart}
%\documentclass[10pt]{amsbook}
%\setlength{\textwidth}{5in}

\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
%\usepackage[all]{xy}   %xy-pic

% graphics
\usepackage{graphicx}    % needed for \includegraphics
% Tell \includegraphics where to search for files
% see Gratzer p 317
% The trailing / is required
\graphicspath{%
{/home/roy/Dropbox/nyu-real-estate/repp-repo.git/src/local-weighted-regression-2/src/}
}
\usepackage{epstopdf}   % allow eps files as graphics input
\usepackage{caption}    % allow line breaks \\ in captions

% don't indent first list of a paragraph
%\setlength{\parindent}{0pt}   
% increase spacing between paragraphs
\setlength{\parskip}{1ex plus 0.5ex minus 0.2 ex}
% don't align right margin
\raggedright

% proclamations
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\newtheorem{notation}{Notation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem]{Theorem}

%%%%%% commands local to this document
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\term}[1]{\emph{#1}}
\newcommand{\blanks}{\_\_\_}
\newcommand{\blank}{\textunderscore\textunderscore\textunderscore}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
\title{Input Procesing}
\author{Roy E. Lowrance}
\email{roy.lowrance@gmail.com}
\maketitle


We start with real estate data for Los Angeles Country: the taxroll for
2008 and 25 years of deeds ending late in 2009. These data came from
CoreLogic. We supplemented them with data from the U.S. Census Bureau
from the year 2000 and, from a geocoding service, the latitude and
longitude of many of the parcels. We join these file to create a
transactions file: a parcel and the price of the parcel on a date that
it traded. We create a subset of the transactions file containing only
parcels for single-family residences, only arms-length transactions, and
only transactions for which every field has a ``reasonable'' value.

This chapter provides the details on how this was done. It contains these
sections:
\begin{itemize}
\item A description of the input files
\item How the input files were joined into the transactions file.
\item Summary statistics about the transactions file.
\end{itemize}

\section{Input Files}

The taxroll is used by the tax assessor to prepare and send property tax
bills. In Los Angeles Country, the initial real estate bills are send
starting November 1. The taxroll used in this work is as of November 1,
2008, representing tax dues in late 2008 and in 2009.

The eight taxroll files we use are assembled by CoreLogic, which obtains the
original files, cleans them up, augments them with other data, and sells
them. There is one record (CoreLogic type 2580) for every parcel.

The fields in each record are in these groups:

\begin{itemize}

\item The parcel identifier, called the Assessor Parcel Number (APN).
This value is presented twice, once formatted with hyphens and once as a
plain number field.  The number field is not always numeric and does not
always have the correct number of digits, so the two fields are analyzed
to infer the ``best'' APN.

\item Information on the parcel itself: it's census tract, its latitude
and longitude (though these fields are not populated in our data, so
that we have used a separate geocoding file), location on maps, the
universal land use code (LUSEI), and so forth. The LUSEI field is used
to identify whether the parcel is for a single-family residence.

\item Information on the subdivision, primarily its location in refence books.

\item The address of the property including its 9-digit zipcode.

\item Information on the owner, which is not populated.

\item A series of fields describing the assessment for the parcel.
  California Propositions 7 and 13 force the assessment to not be an
  unbiased estimate of the market value of the property. Proposition 13
  constrains the assessment to not grow beyond the last sale price of
  the property by more than two percent annually, thus biasing
  assessments to be on average lower than property values in periods of
  high real estate inflation. Proposition 7 reduces the assessment to an
  estimated market value upon successful petition of the property owner,
  thus biasing the assessment to be on average higher than property
  values, because not all owners will petition to have their assessments
  reduced when the real estate market is falling.

\item Information on the most recent sale of the property. We don't use this
information and rely instead on the information in the deeds. The two
sources do not always agree.

\item Information on the mortgage. We don't use any mortgage information in
this work.

\item Information on the prior sale. We again rely on the deeds for this type
of information.

\item A description of the lot including its size in acres and square feet.

\item A description of the primary building, including the year built, number 
of rooms, number of bedrooms, number of bathrooms, whether it has a swimming
pool. Many of the values are missing, so we use a subset of the values that
are often present.

\item The legal description of the property. We don't use this.

\end{itemize}

A deed transfers ownership or legal rights to a property. When a
property is sold or encumbered, one files an appropriate deed with the
registrar of deeds. The deeds used in this work are for the 25 years
ending late in 2009.

The eight deeds files we use are assembled by CoreLogic, which obtains
the original files, cleans them up, augments them with other data, and
sells them. There is one record (CoreLogic type 1080) for every deed.

The fields in each record are in these groups:

\begin{itemize}

\item The parcel identifier, the APN. This is coded as in the taxroll file and
has similar issues.

\item A description of the owner. We don't use this.

\item The owner's mailing address. We don't use this.

\item Property information. We rely on the taxroll for this type of information
and hence do not use these fields.

\item Information on the sale, including the sale date, the price, how
  many APNs were in the transaction, the type of deed, and the primary
  category code (PRICATCODE) reporting whether the transaction was at
  arms-length. We use only arms-length transactions in this work. We use
  only grant deeds (these are deeds of sale) and trust deeds (sales
  supported by a mortgage) in this work.

\end{itemize}

The census file was downloaded from the U.S. Census. It is for the year 2000 and
contains records for census tracts in Los Angeles Country.

The geocoding file was produced by GeoLytics, Inc. It contains latitudes and
longitudes for many of the parcels in Los Angeles.

\section{Creating the Transactions File}

We join the input files to create a transactions file which we then
subset to form the main data file for most of the analysis. This section 
describes the steps.

The major steps are these:
\begin{itemize}
  \item Select just the arms-length deeds.
  \item Select just the parcels containing single-family-residences.
  \item Create additional features for the zip codes and census tracts.
  \item Create additional features for the census tracts.
  \item Join all the files together.
  \item Pick a subset that has ``reasonable'' values.
  \item Split the subset into individual features.
\end{itemize}

Details of each of these steps are in the subsections that follow.

\subsection{Select arms-length deeds}

The deeds file classifies every record as to whether it was an
arms-length sale or not.  How this is done is not known for sure, but
presumedly relies on the type of deed and the relationship if any
between the seller and buyer.

Our work uses on deeds classified as arms-length sales as recorded
in the PRICATCODE field. The deeds files from CoreLogic contain XXX
deds of which XXX are classified as arms-length.

\subsection{Select single-family residences}

The tax assessor classifies each parcel according to its primary use.
One of the uses is as a single-family residence.

Our work uses only parcels classified as single-family residences as
recorded in the LUSEI field. The taxroll files from CoreLogic contain
XXX taxroll records of which XXX are classified as single-family
residences.

\subsection{Create additional features for zip codes and census tracts}

A potentially-informative feature of a parcel is whether it is near 
industry, a park, shopping, or a school. Perhaps the first of these
characteristics detracts from attractiveness and the others increase
attractiveness.

These features are not directly reflected in the taxroll file and hence
must be deduced. We have latitudes and longitudes only for residences,
not for parcels containing industry, parks, retailers, or schools. Hence
we resort to determining whether 5-digit zip codes and census tracts
contain industry, parks, retailers, and schools.

Thus we have two additional input files that must be joined: one stating
whether every 5-digit zip code has any of the features of interest, the
other with the same information for every census tract.

\subsection{Create additional features for the census tracts}

The features we want to have for the census tracts are the average
commuting time (perhaps longer commutes lower property values), the
median household income (perhaps neighbors with higher incomes also
have higher property values), and the fraction of houses that are
owner-occupied (perhaps higher ownership is associated with higher
property values).

None of these features is provided directly in the Census Bureau files,
but is straight-foward to compute form the information provided.

\subsection{Join all the files together}

The transactions file is created by joining each of the input files.
\begin{itemize}
  \item The file of all arms-length deeds containing XXX records.
  \item The file of all single-family-residence parcels containing XXX
    records. These parcels are merged into the deeds using the best APN
    field from each file. The resulting merged file has XXX records.
  \item The file containing XXX records derived from the census tract 
    data. This file is merged into the deeds-parcels file using the
    census tract fields. A file with XXX records results.
  \item The file containing XXX records from the geocoding provider.
    This file has the APN as its primary key and is merged into the
    census-deeds-parcel file.
  \item The file containing XXX records recording features of the
    5-digit zip codes and XXX similar records from census tracts. These
    files are merged into the census-deeds-geocoding-parcels file.
\end{itemize}


The resulting transactions file contains XXX records.

\subsection{Pick a subset with reasonable values}

We would be finished with the data munging except that the transactions
file contains observations with very unusuable values. For example,
there are single-family residences with no rooms, with sales prices in
the hundreds of millions of dollars, with a huge number of parking
spaces, and other unusual features.

This project assumed that observations with unreasonable values were
not recorded properly and chose to discard those records. An alternative
would be to identify the missing or mis-coded values and to impute their
values.

These judgements were applied to reject records and form ``subset1,''
the subset of transactions actually used in the analysis.
\begin{itemize}
  \item Assessed value. Transaction arising from parcels with an
    assess value exceeding the maximum sales price were discarded. How
    the maximum sales price was determined is described just below.
    There are XXX such.
  \item Document type code. Transactions arising only from grant
    deeds and trust deeds were retained. Grant deeds are sales and trust
    deeds pledge the property as collateral for a mortgage. There are
    XXX that are neither grant nor trust deeds.
  \item Effective year built. Transactions arising from properties
    without an effective year built were discarded. The effective year
    built is the year of the last major remodeling or year the property
    is built. There are XXX such.
  \item Geocoding. Transactions for which either the latitude or
    longitude were missing were rejected. There were XXX such.
  \item One building. Transactions arising from parcels with more
    than one building were rejected because we have only descriptions
    for one building. There are XXX such.
  \item One parcel. Transactions arising from deeds that reported
    more than one sold parcel were rejected because there is no way to
    apportion the price to the parcels. There are XXX such.
  \item Recording date. Some deeds are missing recording dates. The
    recording date is needed by some of our models, so these
    observations were discarded.  There are XXX such. Note that there
    are not similar deletions fo missing sales dates. Instead, when a
    sales date is missing, it is imputed from the recording date. This
    imputation uses the average delay between sales and recording (XXX
    days) as the best estimate for the missing sales date.
  \item Sale amount. Some deeds report extremely large prices. The
    largest price is the transactions file is XXX. Research in the Wall
    Street Journal suggests that the highest transaction price in Los
    Angeles through the end of 2009 is XXX, so observations with prices
    higher than this were rejected. There were XXX such.
  \item Sale code. The price on the deed might not be for the full
    value of the parcel. We rejected transactions that did not say the
    sales price was for the full amount. There were XXX such.
  \item Total rooms. We rejected transactions for houses reported to
    have less than XXX rooms. There were XXX such.
  \item Transaction type code. We rejected transactions for parcels
    that were not either new construction nor resales. Other
    possibilities include time shares, construction loans, and
    refinancing. There were XXX such.
  \item  Units number. We have the description for only the primary
    unit on the parcel, so we rejected observations with more than one
    unit. There were XXX such.
  \item Year built. Transactions arising from properties with a
    missing year built were discarded. There are XXX such.
\end{itemize}

Some transactions were discarded because of extremely high values in one
or more features. Observations that exceeded the 99th percentile of
reported values were discarded. Features subject to this protocol were
these:
\begin{itemize}
  \item Land square footage. The square footage of the land. The
    largest value in the transactions file is XXX; the largest value in
    the subset of retained transactions is XXX. There are XXX
    observations with values exceeding the 99th percentile.
  \item Living square feet. The square footage of the livable part
    of the  house. The largest value in the transactions file is XXX;
    the largest value in the subset of retained transactions is XXX.
    There are XXX observations with values exceeding the 99th
    percentile.
  \item Universal building square feet. The square footage inside
    the house. The largest value in the transactions file is XXX; the
    largest value in the subset of the retained transactions is XXX.
    There are XXX observations with values exceeding the 99th
    percentile.
\end{itemize}

\subsection{Split the subset into individual features}

This part of the project uses R as the programming language. The output
of each of the processing steps is a data frame that is stored in R's
internal binary serialized format. The idea is to make it quick to read
in the data. If there is a need for access by programs that cannot
easilry read R's serialized format, CSV files can be easily created.

After I starting working with subset 1, I found that even reading the
binary serialized format takes several minutes, and that is too long to
wait. So I created one final processing step, which is to split the
subset 1 data frame into individual features and write the features as
1-column data frames in serialized files.  Often an experiment needs
only a hand full of features, and reading just the features needed and
assembling them into a data frame for analysis is quicker than reading
all the features and discarding most of them.

Since models will need transformed versions of the features, I
pre-computed those as well. For the continuous features will all
positive values, I created centered versions and centered versions of
the log of the values. For continuous features that can be zero, instead
of the log I used 1 plus the log.

\section{Summary statistics on the retained subset of transactions}

WRITE ME OR GET RID OF ME.

\end{document}
 
% vim:fdm=syntax
