% vim: textwidth=72

% STYLE GUIDE
% not dataset, but data set
%\input{thesis-prologue}

\documentclass[10pt]{amsbook}
%\setlength{\textwidth}{5in}

\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
%\usepackage[all]{xy}   %xy-pic

% graphics
\usepackage{graphicx}    % needed for \includegraphics
% Tell \includegraphics where to search for files
% see Gratzer p 317
% The trailing / is required
\graphicspath{%
{/home/roy/Dropbox/nyu-real-estate/repp-repo.git/src/local-weighted-regression-2/src/}
}
\usepackage{epstopdf}   % allow eps files as graphics input
\usepackage{caption}    % allow line breaks \\ in captions

% don't indent first list of a paragraph
%\setlength{\parindent}{0pt}   
% increase spacing between paragraphs
\setlength{\parskip}{1ex plus 0.5ex minus 0.2 ex}
% don't align right margin
\raggedright

% proclamations
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\newtheorem{notation}{Notation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem]{Theorem}

%%%%%% commands local to this document
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\term}[1]{\emph{#1}}
\newcommand{\blanks}{\_\_\_}
\newcommand{\blank}{\textunderscore\textunderscore\textunderscore}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}

\chapter{Input Processing}

We start with real estate data for Los Angeles Country: the taxroll for
2008 and 25 years of deeds ending late in 2009. These data came from
CoreLogic. We supplemented them with data from the U.S. Census Bureau
from the year 2000 and, from a geocoding service, the latitude and
longitude of many of the parcels. We join these file to create a
transactions file: a parcel and the price of the parcel on a date that
it traded. We create a subset of the transactions file containing only
parcels for single-family residences, only arms-length transactions, and
only transactions for which every field has a ``reasonable'' value.

This chapter provides the details on how this was done. It contains these
sections:
\begin{itemize}
\item A description of the input files
\item How the input files were joined into the transactions file.
\item Summary statistics about the transactions file.
\end{itemize}

\section{Input Files}

The taxroll file is actually 8 files containing the taxroll as of
November 1, 2008. There is a record (of type 2580) for every parcel. The
fields in each record are:

\begin{itemize}

\item The parcel identifier, called the Assessor Parcel Number (APN).
This value is presented twice, once formatted with hyphens and once as a
plain number field.  The number field is not always numeric and does not
always have the correct number of digits, so the two fields are analyzed
to infer the ``best'' APN.

\item Information on the parcel itself: it's census tract, its latitude
and longitude (though these fields are not populated in our data, so
that we have used a separate geocoding file), location on maps, the
universal land use code (LUSEI), and so forth. The LUSEI field is used
to identify whether the parcel is for a single-family residence.

\item Information on the subdivision, primarily its location in refence books.

\item The address of the property including its 9-digit zipcode.

\item Information on the owner, which is not populated.

\item A series of fields describing the assessment for the parcel. California
Propositions 7 and 13 force the assessment to not be an unbiased estimate of
the market value of the property. Proposition 13 constrains the assessment
to next exceed a specified rate of growth since the last property sale, thus
biasing assessments to be on average lower than property values. Proposition 7
reduces the assessment to an estimated market value upon successful petition
of the property owner, thus biasing the assessment to be higher than property
values, because not all owners will petition to have their assessments reduced
when the market is falling.

\item Information on the most recent sale of the property. We don't use this
information and rely instead on the information in the deeds. The two
sources do not always agree.

\item Information on the mortgage. We don't use any mortgage information in
this work.

\item Information on the prior sale. We again rely on the deeds for this type
of information.

\item A description of the lot including its size in acres and square feet.

\item A description of the primary building, including the year built, number 
of rooms, number of bedrooms, number of bathrooms, whether it has a swimming
pool. Many of the values are missing, so we use a subset of the values that
are often present.

\item The legal description of the property. We don't use this.

\end{itemize}

The deeds file is actually 8 files containing every deed created over about
25 years. A deed transfers ownership or legal rights to a property. When a 
property is sold, a deed is supposed to be created. In California (but not all
states) the deed states the selling price for the property. There is a deeds
record (of type 1080) for every deed. The fields in a deeds record are:

\begin{itemize}

\item The parcel identifier, the APN. This is coded as in the taxroll file and
has similar issues.

\item A description of the owner. We don't use this.

\item The owner's mailing address. We don't use this.

\item Property information. We rely on the taxroll for this type of information
and hence do not use these fields.

\item Information on the sale, including the sale date, the price, how many
APNs were in the transaction, the type of deed, and the PRICATCODE reporting whether the 
transaction was at arms-length. We use only arms-length transactions in this
work. We use only grant deeds (these are deeds of sale) and trust deeds (sales
supported by a mortgage) in this work.

\end{itemize}

The census file was downloaded from the U.S. Census. It is for the year 2000 and
contains records for census tracts in Los Angeles Country.

The geocoding file was produced by GeoLytics, Inc. It contains latitudes and
longitudes for many of the parcels in Los Angeles.



\section{Introduction}
This project starts with data sets from several providers, each with
its own layouts. To build and test models, one needs to join the data
sets into cohesive sets of observations. This chapter describes how
that was done and explains the rationale for decisions made along the
way. It provides enough detail so that someone else could recreate the
observations sets that were used in the project.

In what follows, I call the data sets from the providers the ``raw''
data. Section 1 below describes how these raw data sets are laid out in
the file system. It also describes the content of the data sets. 


CoreLogic provided the bulk of the raw data in the form of two collections of
files: a deeds collection and a parcels collection. A deed is a document that
transfers ownership of property or asserts a claim on the property. A parcel is
a piece of land that may or may not contain one or more improvements such as a
house.

My goal was to build estimation models that estimates arms-length
selling prices for single-family houses. ``Arms-length'' means that
the price on the deed was at market, not a price imposed through the
court system or a price that reflects a gift, as in a sale of a house
at a favorable price to a child. A single-family residence is an
improvement on a parcel that is zoned as for single-family use. A
single-family house is a residence that contains housing for one famil, as opposed
to a two or more family house or an apartment house. 

In addition to the raw data from CoreLogic, I have geocoding data and
census data. The geocoding data contains the latitude and longitude of
the property together with the property identifier called the Accessor
Parcel Number (APN). The ``Accessor'' is the tax assessor which helps
to administer the local property tax system. The APN is supposed on every
parcel record. The geocoding data was purchased by one of my predecessors
who worked on these data.

The census data comes from the US Census Bureau, which provides on
www.census.gov many datasets, all free to download. The downloaded
observations contains the census tract identifier, which uniquely
the record. The parcel file also contains the census tract number.

The four raw files were joined in a multi-step process designed to be
very simple and to be reusable in other research projects.
\begin{enumerate}

\item The deeds file is really eight different files. Several versions
  exist, and I used the exact same version used by Steve Laufer
  because eventually I want to feed my price estimations into his
  mortgage default prediction model. This step concatenates the eight
  files and drop all the deeds that are not coded as arms-length. In
  addition to dropping non arms-length observations, features thought
  to not be of use in the modeling are also dropped. Moreover, because
  the deeds file contains information about the parcels and we have a
  parcels files, all the parcel information (except the APN) is
  dropped, making the parcel file the definitive source for parcel
  features. The resulting file \code{deeds-al.csv} has about 4.7
  million observations. The code to do this is in file \code{createDeedsAl.R}.

\item The parcels file is also eight different files. I again used
  Steve Laufer's version. This step concatenates the eight files and
  drops all the parcels not designated as single-family
  residential. As for the deeds file, features of the parcel that will
  not be used in the modeling are dropped. And information on the most
  recent deeds associated with the parcel is also dropped. The
  resulting file \code{parcels-sfr.csv} has about 1.4 million
  observations. The code to do this is in file \code{createParcelsSFR.R}.

\item The census file is a single file downloaded from the Census
  Bureau. The models use three fields that are derived from the data
  in the file: the average commute time for those who do not work at
  home, the median household income, and the fraction of houses that
  are owned by their occupants. The resulting file \code{census.csv}
  file has about 2,000 observations. The code to do this is in file
  \code{createCensus.R}.

\item The 3 files just created and the geocoding file are
  joined. First the deeds and parcels files are joined on APNs, to
  create a transaction record. The transaction record contains the
  census tract number, and it is used to join the census records. The
  geocoding records are joined using the APN field in the transaction
  record. The code to do this is in file
  \code{createTransactions.R}. The resulting file
  \code{transactions-al-sfr.csv} has about 2.6 million
  transactions. The code to do this is in file
  \code{createTransactions.R}.

\item The transaction file contains all the records from its ancestors
  that could be matched and were for single-family residences sold at
  arms length. However, many of the observations in it do not make
  sense. For example, sometimes the supposed arms-length sales price
  is zero. The program \code{createSubset1.Rmd} chooses a reasonable
  subset to be used for further analysis and creates data file
  \code{subset1.csv} containing about 1.3 million observations. The
  selection criteria are described in detail in a section just below.

\item HOW THE IMPUTATION WAS DONE. Refer to section below. Create file
  \code{subset1-imputed.csv}.
\end{enumerate}

\section{Subsetting the Transactions}

% The data in this section are generated by running
% transactions-subset1.Rmd

The transactions file joins on APNs the arms-length deeds and
single-family residential parcels, augmenting the joined data set with
census and geocoding data. The data set is organized into a matrix
where the 2.6 million rows represent observations and the 63 columns
represent features.  This section describes how the transactions are
manipulated to produce a subset of observations. The subset will be
used in subsequent modeling work. It retains all the features
(columns) from the transactions file. The file name is
\code{transactions-subset1.csv}.

Some feature values in the transactions are not reasonable: they may
be missing or they may have an unreasonably small or large
value. Observations containing values judged to be unreasonable were
eliminated, and the remaining observations form the subset.  Below is
a table depicting ranges and missing values for features used to form
the subset. Following the table are the criteria used to create the
subset.

\input{../../data/v6/output/transactions-subset1-ranges.tex}


In the descriptions below, feature names in CAPITAL LETTERS are
features present in the raw files that were brought forward to the
transactions file. Feature names in or containing lowercase letters
were created as part of the subsetting process. The analysis was done
in R, which likes to replace spaces in features names with
periods. Thus A FEATURE and A.FEATURE are different names for the same
feature.

The observations judged suitable for modeling transaction prices meet
these criteria:
\begin{itemize}

\item The transaction date can be reasonably estimate. About 800,000
  observations are missing the SALE DATE. The RECORDING DATE is always
  present. Both SALE DATE and RECORDING DATE are supposed to be in the
  form YYYYMMDD but sometimes are not. In some cases the only form
  defect is that the date ends in 00 as in ``19910100'', so that the
  date is not given precisely. For such dates, the final 00 is
  replaced by 15, moving the date to the middle of the month,
  resulting in an adjusted date. When both the adjusted SALE DATE and
  the adjusted RECORDING DATE are known and are interpretable as a
  date, the mean delay from the sale to the recording of
  the sale is about 55 days. I assumed that the sales dates are
  missing at random and hence subtracted the mean difference from the
  recording date to estimate the sale date, when the sale date itself
  was missing. No observations were dropped in this process,
  preserving the roughly 800,000 million transactions missing a SALE
  DATE. The process created the feature ``transaction date.''

\item The SALE.AMOUNT (price) is reasonable. The range for SALE AMOUNT
  is $[0, 55 \, billion]$. A zero price is not reasonable, though such a
  price could result from the property being a gift or the price being
  miscoded. I intend to exclude transactions not at arms length and hence
  am happy to exclude gift transactions. Transactions with miscoded prices
  contain no information, so I was happy to exclude the few miscoded prices
  that I could identify. Examinig the very high prices, \$55 billion is 
  a lot of money, even for the ritzy parts
  of Los Angeles. The Wall Street Journal reported on January 7, 2013
  that the most expensive Los Angeles house every sold was for \$85
  million (a house in Beverly Hills, see
  http://http://blogs.wsj.com/developments/2013/01/07/russian-billionaire-couple-buys-malibu-estate/?KEYWORDS=most+expensive+home
  accessed February 10, 2013). I decided to drop all observations
  with a transaction price exceeding \$85,000,000 as miscoded, though it
  is possible that the Wall Street Journal has its facts wrong. The two
  criteria--not zero and not too large--exclude about 400,000
  observations and almost all of these are excluded because of a zero
  price.

\item The deed was a grant or trust deed as reported by the DOCUMENT
  TYPE CODE. About 300,000 observations have deeds that are neither
  grant nor trust deeds. A grant deed represents the transfer of the
  property. A trust deed is sometimes used when the property is
  purchased with a mortgage. (The Wikipedia page for ``Trust deed''
  describes the legal distinctions between trust deeds and mortgages.)
  Besides grant and trust deeds, there are many types of deeds. The
  other types of deeds found in the transactions file are:
  \begin{description}
  \item[quit claim deed] transfers the property without transfering
    the title, so that the receiver has no title guarantee. According
    to the Wikipedia article ``Quitclaim deed'' accessed on February
    10, 2013, these deeds are most often used to transfer property
    amoung family members or in divorce settlements. In either case,
    the SALE AMOUNT is suspicious.

  \item[foreclosure deed] transfers the property from borrower to the mortgage
    holder, according to to the Wikipedia article
    ``Foreclosure'' accessed on Feburary 10, 2013. These transactions
    are at arms length, but they are preceeded by a foreclosure
    process. During the foreclosure process, the owner faces a high
    probability of loosing the house, so has a limited incentive to
    maintain the property. As a result, properties sold out of
    foreclosure are often sold at prices below similar properties. For
    example, realtor.org reports that in December 2012, foreclosed and
    short sale properties (these are sold by the owner at a price that
    does not allow the mortgage to be fully paid) sold at an average
    17 percent discount (see
    http://economistsoutlook.blogs.realtor.org/2013/01/31/foreclosed-real-estate-selling-at-a-17-percent-discount/,
    accessed February 12 2013).  Excluding these deeds has the effect
    of conditioning the models built to exclude foreclosure sales. In
    the commercial market for real estate value estimation models,
    vendors sometimes provide models that include all type of deeds,
    models that excluded distressed properties (typically defined as
    foreclosure deeds and short sales), and models that include only
    distressed properties.

  \item[multi-county or open-end mortgage] which are coded into one
    group. I excluded multi-county deeds because we have information
    only on one parcel, not all of them, and in addition, there is no
    way to split the SALE AMOUNT among the parcels. An open-end
    mortgage, according to Investopedia.com at ``Open-End Mortgage''
    accessed on February 10, 2013, combines a regular mortgage with a
    home equity line of credit. In this case, the price reflects more
    than the value of the parcel because it includes an option to draw
    down more cash.

  \end{description}

\item The deed represented a resale of a parcel or the initial sale of
  a parcel as reported by the TRANSACTION TYPE CODE. About 30,000
  observations have deeds that are not for initial or subsequent
  sales. These deeds include refinance deeds, time share deeds,
  construction loan deeds, seller carryback deeds, deeds in which the SALE
  AMOUNT is reported as nominal, and deeds in which the TRANSACTION
  TYPE CODE is missing.

\item The full sale price is recorded on the deed as reported in the
  SALE CODE field. About 700,000 observations do not record the full
  price. These other deeds confirm a previously estimated price,
  report a partial sales price, have a price that is not in the public
  record, have a unknown price, a verified price (eliminated because
  there are only a few of these observations), or have a missing SALE
  CODE (about 600,000 observations).

\item Exactly 1 parcel was sold. The field MULTI APN FLAG CODE
  indicates deeds for multiple and split parcels. MULTI APN COUNT
  records the number of parcels for the deed. (APN stands for accessor
  parcel number. It is the unique ID for the parcel.) Both fields were
  interpretted in order to classify a deed as being for exactly 1
  parcel. MULTI APN FLAG code, when missing, indicates that the parcel
  was not a detail record for another deed, a multi-parcel sale, nor
  a split parcel sale; I assumed it was for 1 parcel when it was
  missing. It is missing in about 2,500,000 parcels. MULTI APN COUNT
  contains a 0 value about 2,600,000 times, never contains a 1 value,
  and contains values in $[2,4]$ less than 10 times. I classified as
  1-parcel those deeds where the MULTI APN FLAG CODE was missing and where
  the MULTI APN COUNT was not more than 1.  This decisions excluded
  about 70,000 observations.

\item The parcel has one building. The number of buildings is in
  feature NUMBER OF BUILDINGS with range $[0,5]$. Excluding parcels
  without at least one building eliminates empty lots. Excluding
  parcels with 2 or more buildings eliminates parcels with a main
  house and a detached garage, tool shed, gazebo, guest cottage, or
  second huge house. However, for those extra buildings, we don't have
  a description. More than one building is reported for about 1,000
  observations. This decision excluded about 5,000 observations.

\item Reasonable assessed value. The assess value is broken down into
  the assessed value of the land and the assess value of the
  improvements. The relavant feature names and their ranges are TOTAL
  VALUE CALCULATED $[0, 97 \, million]$, LAND VALUE CALCULATED $[0, 54
  \, million]$, and IMPROVEMENT VALUE CALCULATED $[0,96 \,
  million]$. The term ``CALCULATED'' refers to how the raw data file
  was assembled.  The zero values are just barely possible, but were
  considered invalid and hence observations containing zero values
  were dropped. We know that the maximum total value is \$85 million 
  (if the Wall Street Journal is correct),
  so we dropped observations in which any value component exceed this
  amount even though its possible for a property to be assessed for
  more than its market value. These conditions excluded about 4,000
  observations.

\item Reasonable amount of land. LAND SQUARE FOOTAGE contains the
  amount of land and has range $[0, \, 400 million]$. I dropped
  observations with no land and land exceeding the 99th percentile
  (about 88,000 square feet). This decision excluded about 30,000
  observations. For this feature and the next few, the intention 
  was to exclude properties that are huge. The rationale for
  excluding them is in part that there are few of them and we are happy
  to bias our models to more normal properties.

\item Reasonable building square feet. UNIVERSAL BUILDING SQUARE FEET
  contains the square footage in the building and has range $[0,
  355 \, thousand]$. The term ``UNIVERSAL'' refers to the method of
  calculation. I dropped observations with no square feet and with
  square feet exceed the 99th percentile (about 5500 square feet). This
  decision excluded about 30,000 observations.

\item Reasonable living square feet. LIVING SQUARE FEET is the livable
  part of the buildings, often defined as the space that is heated,
  has interior construction suitable for living, and is accessible
  from the outside or other living area (from
  http://homebuying.about.com/od/realestatecareers/ss/square\_footage.htm,
  accessed February 13, 2013 for additional criteria). The range is
  $[0; 58 \, thousand]$. I dropped observations with no square feet
  and with square feet exceeding the 99th percentile (about 5400
  square feet). This decision excluded about 30,000 obsservations.

\item Known YEAR BUILT. This feature is used during the imputation of
  missing values (see below). It has range $[0, 2010]$. A zero value
  is most likely a missing. About 5,000 observations has zero values
  and  were excluded. The oldest house was built in 1801.

\item Known effective year built. EFFECTIVE YEAR BUILT is the year of
  last remodeling of the house. The tax assessor knows this by
  tracking building permits. The range is $[0, 2010]$. The zeros are a
  problem, so I excluded about 6,000 observations. The oldest
  effective year built is 1875.

\item At least one room. TOTAL ROOMS records the number of rooms. It
  has range $[0, 84]$. The zero values are a problem (otherwise, there
  is no house) and 84 rooms which is a lot of rooms. I excluded observations
  with no rooms and more than the 99th percentile of rooms (9),
  thereby excluding about 600,000 observations. Within the retain
  observations (those with 1 - 9 rooms), there are 600 houses with no
  bedrooms and 400 houses with no bathrooms. These are either data
  mistakes or tiny or primitive houses. I kept them in the subset.

\item Reasonable geocoding. The geocoding variables are G LATITUDE and
  G LONGITUDE. G LATITUDE has range $[0, 38.5]$, where the 0 values
  are missing data.

  G LONGITUDE has range $[-121,0]$, where the 0 values are
  missing data. Removing observations with either missing G.LONGITUDE
  or G.LATTIUDE  excludes about 400,000
  observations. (The latitude and longitude are carried to more
  precision in the files than are used in the summary.)

\end{itemize}

The effect of each criteria in isolation is given in the table
below. It shows the number of records that would be excluded by each
criterion if it operated alone. All together, about 2.6 million
exclusions are applied, roughly the size of the transactions
file. Luckily many of the exclusions apply to the same observations so
that about 1.3 million observations are in the subset.

\input{../../data/v6/output/transactions-subset1-excluded}


\section{Analysis of Qualitative Features in the Subset}

The qualitative features in the subset include ZONING, VIEW,
AIR CONDITIONING CODE, and other features. The raw files contain
strings containing values for the fields. Often the values are
missing. Thus one needs to decide what do to with observations missing
one or more qualitative features.

The table below depicts relevant qualitative features in the subset,
showing for each the number of unique values taken on (counting NAs if
present as a value), the number of missing values (NAs), and the
fraction of values that are missing. 

\input{../../data/v6/output/transactions-subset1-analyze-qualitative-features.tex}

Some features are not in the table because they were not considered to
be relevant. Omitted from the list are qualititative featues that are
redundant with already-choosen features. For example, there are
several features that relate to coordinates on maps, zipcodes,
refinements to census tracts, and to names of municipalities and
townships, but we already have GPS. Other qualitative
features not considered relate to the buyer
(ABSEENTEE.INDICATOR.CODE.deeds), the financial transaction
(FORECLOSURE.CODE, TITLE.COMPANY.CODE), and file layout and history
(APN.SEQUENCE.NUMBER), features redundant with other features (for
example GARAGE.CODE and PARKING.TYPE.CODE are mostly redundant), and
other fields judged to be non-informative for pricing purposes in typical
transactions. (Note that contrary to this assumption, some buyers focus
on distressed properties which may be identifiable through the FORECLOSE
CODE.)

In handling the missing values, Chopra (\cite[p.~87]{chopra-09})
discovered a set of mutually always-occuring qualitative features was
selected. Observations not containing all of these features were
eliminated.

Here a different approach is followed. Instead of finding a subset of
qualitative features that occurs together, I examined each qualitative
feature. Some features were retained exactly as provided. Some
features were dropped, as I decided they contained no information or
redundant information. Some features were recoded, often by assigning
a value to the missing values. Some features were put on a list of
to-be-imputed features. Some features had multiple treatments.


Here are the decisions I made on the examined categorical
feature. The order of presentation is the order that the features
appear in the subset file.
\begin{description}

\item[RESALE NEW CONSTRUCTION CODE retained] This code tells whether
  the house is brand new. It's always present. It's retained as a
  feature with no transformation.

\item[ZONING imputed] There are about 2500 zoning codes and about 250 missing
  codes. The missing values were imputed. How missing values were imputed
  is described below.

\item[VIEW dropped] The VIEW is the view from the building. There are
  many missing value. However, the coded values would seem to have an
  impact on the value of the property. The coded values present and
  their meanings are VCI (``city''), VCN (``canyon''), VGO (``good''),
  VLA (``lake''), VOC (``ocean''), VWR (``water''),. There is a code
  000 (``None'') but it never appears. At first, I decided to treat
  the missing codes as if they were 000 codes. However, then I
  analyzed the LOCATION INFLUENCE CODE and noticed that VIEW can be
  dropped.

\item[LOCATION INFLUENCE CODE recoded] This code records positive or
  negative aspects of the location. The coded values present and their
  meanings are I01 (``type unknown''), IBF (``bay front''), ICA
  (``canal''), ICR (``corner''), ICU (``cul-de-sac''), IGC (``golf
  course''), ILP (``lake/pond''), IRI (``river''), and IWL
  (``woodland''). There is code 000 (``none'') that is never used. I
  decided to treat the missing codes as if they were I01
  codes. Moreover, because LOCATION INFLUENCE CODE captures many of
  the VIEW values, I decided to drop the VIEW feature.

\item[AIR CONDITION CODE recoded] This code records the type of air
  conditioning. It is missing 64 percent of the time. However, the
  code for no air conditioning is defined abut never appears in the
  input file. I assumed that missing air conditioning codes should
  have been coded as 000 (``none''). The rationale is that the monthly
  high temperatures in Los Angeles (source:
  gocalifornia.about.com/logangeles/bl\_la\_temp.htm, accessed February
  3, 2013) is at most 84 degrees, not so hot as to require air
  conditioning.

\item[CONDITION CODE imputed] The code represents the physical condition of
  the main improvement. Choices used in the transactions are average,
  excellent, fair, good, and poor. This features is missing in 51
  percent of the transactions. On the assumption that house conditions
  tend to run in neighborhoods, the missing values for this variable
  were imputed.

\item[CONSTRUCTION TYPE CODE imputed] The code represents the type of
  construction. It is missing 74 percent of the time. Many houses are
  built in subdivision using similar construction techniques. On that
  assumption, the missing values were imputed.

\item[EXTERIOR WALLS CODE imputed] The code represents the type or
  finish on the exterior walls. It is mising only 2 percent of the
  time. Because much construction in subdivisions, it should be
  similar to nearby houses. The missing values were imputed.

\item[FIREPLACE TYPE CODE dropped] The code is the type of
  fireplace. In the transaction subset it is either missing or code
  001 (``type unknown''). It was dropped because nothing can be
  imputed for just one known value.

\item[FOUNDATION CODE recoded imputed] The code is the type of
  foundation. In the subset, it is missing only 2 percent of the
  time. One code present is 001 (``type unknown''). The missing values
  were imputed after recoding the 001 values as NAs.

\item[FLOOR CODE dropped] This code represents the type of floor. In the
  subset, only codes A00 and NA are present. There is not enough
  information to impute the missing values, so the feature is dropped.

\item[FRAME CODE dropped] The code represents the type of frame on the
  house. It is never present in the subset, so it is dropped.

\item[GARAGE CODE recoded] The code presents the type of garage. Two of the
  code values stand for no garage, but these values are never used. I
  decided to recode the missing values as 000 (``no garage'').

\item[HEATING CODE imputed] The code represents the type of heating
  system. The 2 percent of missing values are imputed.

\item[PARKING TYPE CODE recoded imputed] The code represents the type
  of parking. Only 3 percent are missing. However, none of the values
  used in the subset are 000 (``none''), suggesting that the missing
  values are for parcels with no parking spaces. A cross tabulation
  shows that in the subset, most often when the parking type is NA,
  the number of parking spaces is 0. Hence I decided to recoding
  missing parking type codes as 000 when the number of spaces is
  0. Other NA values were imputed.

\item[POOL FLAG recoded] The code represents whether there is a
  pool. The field is defined to be ``Y'' if and only if there is a
  pool on the property. However, there are about 12,000 ``N'' value in
  the file. I kept the ``N'' values and recoded the NA values as
  ``N'', thus recoding the variable to meet its definition in the code
  book supplied by Core Logic. 

\item[POOL CODE recoded] The code represents the type of pool. It is
  missing exactly whenever the POOL FLAG is missing which is in 79
  percent of the observations. I assumed that the missing POOL CODE
  values stood for no pool and hence recoded them as 000 (which means
  ``none''). 

\item[QUALITY CODE imputed] The code represents the type of construction. The
  53 percent of values that are missing are imputed.

\item[ROOF COVER CODE imputed] The code represents the material used in the
  roof. The 1 percent of values that are missing are imputed. 

\item[ROOF TYPE CODE imputed] The code represents the shape of the roof. The
  5 percent of values that are missing are imputed.

\item[STYLE CODE imputed] The code represents the building syle (e.g.,
  Colonial). The 1 percent of values that are missing are imputed.

\item[SEWER CODE dropped] The code represents the type of sewer
  system. It is missing about 3 percent of the time. By far
  the most common type of sewer is 001 (``unknown''). This feature was
  dropped on the assumption that the tax assessor really doesn't know it.

\item[WATER CODE imputed] The code represents the type of water
  service. It is missing about 60 percent of the time. Almost all of
  the supplied values are WPU (``public''). This feature was imputed.

\end{description}


\section{Imputing Missing Feature Codes}

The number of missing codes in the fields to be imputed are given in
the table below.

%\input{../../data/v6/output/transactions-subset1-imputed-features}

Here are the definitions of these fields:

\begin{description}
\item[ZONING] The county zoning code
\item[CONDITION CODE] Physical condition of the main improvement;
  e.g., Good, Fair, Under Construction
\item[CONSTRUCTION TYPE CODE] Primary method of construction; e.g.,
   Steel/Glass, Concrete Block, Log
\item[EXTERIOR WALLS CODE] The type and/or finish of the exterior
  walls; e.g., Vinyl Siding, Brick Veneer, Frame/Stone.
\item[FOUNDATION CODE] The type of foundation; e.g., Continuous
  Footing, Pier, Mud Sill
\item[HEATING CODE] Type of method of heating; e.g., Hot Water, Heat
  Pump, Baseboard, Radient
\item[PARKING TYPE CODE recoded] Type of parking found on the parcel;
  e.g., Basement Garage, Carport, Stucco Garage
\item[QUALITY CODE] Type of construction quality of building; e.g.,
  Excellent, Economical
\item[ROOF COVER CODE] Type of roof covering; e.g., Clay Tile,
  Aluminum, Shake
\item[ROOF TYPE CODE] Shape of the roof; e.g., Gambrel, Gable, Flat.
\item[STYLE CODE] Type of building style, e.g., Colonial, Cape Code, Bungalow
\item[WATER CODE] Type of water service on the parcel; e.g., Public,
  Well, Cistern
\end{description}

The approach to imputing missing CODE values is based on a notional
generative model. Builders often construct more than one house at the
same time, in an effort to improve their costs. When they do, the
houses often have similar designs with relatively minor
variations. Thus the CODE features listed in the table will tend to be
similar. If this generative model is accurate, imputing the CODE
values for a query house based on known code values for houses built
nearby in space and time is appropriate.

For the ZONING feature, the observation is similar: the intent of zoning is to
group parcels with similar allowed purpose. Zoning is often set as housing
tracts are built. Thus imputing a missing zoning value based on parcels nearby
in space and time makes sense.

In addition, the imputation assumes that the missing values are
missing completely at random. Thus there is no information in the fact
that for a fixed observation, one or more features is missing.

The generative model suggests using a local logistic regression to assign a
probability distribution to each missing value in a query. The code with the
highest probability will be assigned as the estimate for the missing value. An
alternative, which would be an extension of this work, would be to add
transactions to the dataset in the proportion of the probabilities for the
imputed missing values.

In the local linear regression, I build a model for every missing
feature $f$ for every query house $q$. A house with 3 missing features
will be the query house in 3 local linear regression.

TODO: rewrite starting here. Used an Epanenchnikov quadratic kernel.

**RESTART

This section needs to describe
\begin{itemize}
  \item The hyperparameters
  \item That it takes 28 days of CPU to determine the accuracy of one
    set of hyperparameters
  \item That hadoop was choosen as the distributed platform to use
  \item The design of the hadoop hyperparameter selection program
  \item Results of running that program
\end{itemize}

The implementation creates a data file \code{parcels-imputed-FIELD.csv}
for each FIELD that is imputed. The main program that creates this file
is \code{parcels-imputed-FIELD.lua}.

The model weights all the houses that have a known value for feature
$f$ according to their distance from the query
transaction. The weighting will be done via a Gaussian kernel, as such
a kernel assigns a weight to every house, avoiding potential problems with
zero weights that might arise if an Epanechnikov or tri-cube kernel
were used. The problem is that by having a hard distance cutoff beyond
which a zero weight is assigned, houses in low density parts of the
distribution would have no neighbors with positive weight. Yann LeCun
pointed out that approach using $k$ nearest neighbors would suffer
from the same problem and that a properly-defined local linear
rergression would not have this problem.

The Guassian kernel has a free parameter $\sigma$, the standard
deviation, that is found via cross validation. The Kernel function is
\[ K^{gaussian}(q, x; \sigma) = \phi (|q - x|; \sigma) \]
where $q$ is the query point, $x$ is a house with a known value for
the missing feature, $\sigma$ is the standard deviation of the
Gaussian distribution $\phi$ which is
\[ \phi(t; \sigma) = \exp(-t^2 / 2 \sigma^2) / \sqrt{2 \pi} \sigma .\]

In the above equation, $t = |q -x|$ is the distance between houses. This
distance must be measured in space and time. I considered two choices in
defining this metric. The first choice, which I ultimately rejected, is to
define $x$, $y$, and $t$ coordinates, where $x$ and $y$ are the latitude and
longitude and $t$ was the time measured in years. The program around this
metric involves determining the curvature of the Earth at the average latitude
of the two points, using the curvature to determine the number of meters per
degree change of latitude and longtiude and hence calculating $x$ and $y$ in
meters. Time would be handled by converting the difference in years between two
observations to equivalent meters using a conversion constant. This approach
would have introduced the conversion factor as another free parameter. The
description is in the footnote\footnotemark.

The second choice and the one I implemeted, simply standardized the
latitude, longitude, and year differences, putting them on roughly the
same scale. This idea came from \cite[p. 174]{hastie-09}.


\footnotetext{
The spatio-temporal distance $d$ between observations $x_1$ and $x_2$
will depending on
\begin{itemize}
\item the latitudinal distance in meters betweeen the two points,
\item the longitudinal distance in meters between the two points,
\item the number of years between the initial construction of the
  properties. (We don't have a more refined construction date.)
\end{itemize}

The latitudinal and longitudinal distances depends on the geometry of
the Earth. Earth is flattened because of its spinning. I used the WGS
84 model, the same model used for GPS systems (see Wikipedia at
``World Geodetic System'', accessed Feburary 18, 2013). According to
Wikipedia at ``Latitude'', accessed February 18, 2013, the length of
one degree of latitude at latitude $\phi$ under WGS 84 is

\[ lat_1(\phi) = \frac{\pi a (1 - e^2)}{180 (1 - e^2 \sin^2
  \phi)^{1.5}} \] 

and the length of one degree of longitude at latitude $\phi$ is

\[ long_1(\phi) = \frac{\pi a \cos \phi}{180 (1 - e^2 \sin^2 \phi)^{0.5}} \]

where
$a = 6,378,137$ meters is the radius of the Earth at the equator
and $e^2 = 0.006,694,379,990.14$ is the square of the eccentricty of
the Earth's assumed WGS 84 ellipsoidal shape.

I defined the distance $d$ from point $a=[latitude_a, longitude_a,
time_a]$ to point $b =[latitude_b, longitude_b, time_b]$ as

\[ d = \sqrt{d_{latitude}^2(a,b) + d_{longitude}^2(a,b) + \alpha
  d_{time}^2(a,b)} \]

where the latitude and longitude distances are taken at the midpoint
of the latitudes of $a$ and $b$ so that

\[ \phi = (latitude_a + latitude_b) / 2 \]

\[ d_{latitude}(a,b) = (latitude_a - latitude_b) lat_1(\phi) \]

\[ d_{longitude}(a,b) = (longitude_a - longitude_b) long_1(\phi). \]

The time difference is

\[ d_{time}(a,b) = time_a - time_b .\]

Latitude and longitude are measured in degrees, $lat_1: degrees
\mapsto meters$, so $d_{latitude}$ and $d_{longitude}$ are in
meters. $d_{time}$ is in years, so $\alpha$ is in meters per year. $d$
is measured in meters.
}



CONTINUE TO EXPLAIN THE IMPUTATION. WRITE ME.

\section{Transforming the Subset}

The section starts with and retains all 1.3 million transactions. It
considers each possible feature and records the decision to drop it,
retain it, or transform it.

The section is organized our type of feature.

\subsection{Identification Features}

When something goes wrong in subsequent steps and that something is
caused by the data, its important to be able to work out where the
original data came from. Thus there is an ID for each deed and parcel
record that is carried forward from the raw data to the transactions
file. Some other fields are created and retained.

These fields result:
\begin{description}
\item[deed.file.number] The raw deeds are in 8 files that have a
  common base name and a numerical suffix in $[1,8]$. This is the
  suffix.
\item[deed.record.number] The data record number in the deeds file,
  starting with 1. The concatenation of deeds.file.number and
  deeds.record.number uniquely identifies the deed that was used to
  create the transaction.
\item[parcel.file.number] Similar to deeds.file.number.
\item[parcel.record.number] Similar to deeds.record.number.
\item[CENSUS.TRACT] The census tract identifier from the parcels
  file.
\item[transaction.record.number] A unique sequential number for the
  observation. Assigned before the subset was created.
\item[runif] A number in $[0,1]$ drawn from the uniform
  distribution. The idea is to use these number to create training,
  test, and validation sets.
\item[APN.UNFORMATTED] The APN number common to the deeds and parcels files. The
  geocoding record used can be identified by the APN value.
\end{description}

The naming convention is that all upper case names are feature names
that were found in the raw files. Lower case names are feature names
that were created. Feature names that were transformed start with the
name from the source file and have a suffix that identifies the
transformation.

\subsection{Basic Features}

These are the basic features:
\begin{description}
\item[date] The number of days before (negative value) or after
  (postive value) the epoch 1970-01-01. The date was created from the
  SALE DATE and RECORDING DATE in the deeds file. The range of dates
  is from 1900-03-13 to 2009-11-13. Most of the dates are in
  $[1984,2009]$.
\item[day.std] The standardized day number for the
  transaction. Standardization is performed here and for other
  features by substracting the mean and dividing by the standard
  deviation. The idea is to get the features to roughly the same
  scale.
\item[SALE.AMOUNT] The price of the transaction.
\item[SALE.AMOUNT.log] The natural logarithm of the price. Most models
  will predict the log of the price, not the price. 
\end{description}

The log of the price is better to predict than the price for at least
two reasons. First, the mean price of the houses in the subset is
about \$300,000 (this might seem low until one recalls that the range
of dates is mostly the last 25 years or so). Of the 1.6 million
transactions in the subset, about 44,000 have a price of at least
\$1,000,000 and about 250 have a price of at least \$10,000,000. In
models trained with an $L_2$ loss function, the relatively few most
expensive houses would dominate the loss, resulting in a model that
was biased toward the larger houses.

The second reason for predicing the log price is that there are some
``size'' features such as the size of the lot where one would
hypothesize that doubling the feature would double the price, all
other things being equal. For example, consider two identical parcels
adjacent to each other, with the only difference being the size of the
lot. The larger one is hypothesized to have twice the price of the
smaller one. This type of relationship is modelled by taking the log
of both the price and feature.

\subsection{Size Features}

Size features are continuous features that are hypothesized to increase
the value of the house proportional to their size. Some of the
features can have a zero value. Hence they are transformed the the log
domain by computing $log(1 + x)$. In addition. the features can have
very different scales. So the log-domain values are standardized.

These features occur in all transactions. The features are:
\begin{description}
\item[NUMBER.OF.BUILDINGS.log1p.std] The number of buildings. One
  could argue that this is not a proper size feature since holding the
  lot size constant, simply adding more buildings would eventually
  decrease the value. Offsetting this argument in part is the fact
  that only about 300 transactions have more than 1 building.
\item[LAND.VALUE.CALCULATED.lop1p.std] When the tax assessor calculates
  the value, the local law may require that the land value also be
  calculated. The term ``CALCULATED''
  refers to how the raw data files were assembled.
\item[IMPROVEMENT.VALUE.CALCULATED.log1p.std] This is the tax
  assessors estimate of the value of the construction on the land. The
  construction is called an ``improvement'', which is not intended to
  be an aesthetic judgement.
\item[LAND.SQUARE.FOOTAGE.log1p.std] The amount of land. There is a
  supposedly-equivalent measurement in the raw data for the acreage,
  but that is rounded to three decimal points, so I chose this feature
  instead.
\item[UNVIVERSAL.BUILDING.SQUARE.FEET.log1p.std] This is the amount of
  space in the building. ``Universal'' refers to the rule set for
  calculating space. There is another measurement BUILDING SQUARE FEET
  that is calculated using local rules, so it is not used.
\item[LIVING.SQUARE.FEET.log1p.std] The amount of space that is
  heated, hence excludes garage space.
\item[TOTAL.ROOMS.log1p.std] The number of rooms in the house.
\item[BEDROOMS.log1p.std] The number of bedrooms. The number of
  bedrooms is classified as a size feature, though one could argue
  that continuing to add bedroom to a house and holding all other
  features constant at first would increase the value of the house and
  then decrease its value. About 200 houses have zero bedrooms.
\item[TOTAL.BATHS.CALCULATED.log1p.std] The number of bathrooms,
  ``CALCULATED'' refers to how the value was derived in the raw
  files. The raw files report the number of bathrooms of various
  sizes, but these potential features were not used in the work. About
  200 houses have zero bathrooms.
\item[FIREPLACE.NUMBER.log1p.std] The number of fireplaces. Zero is an expected
  value.
\item[PARKING.SPACES.log1p.std] The number of parking spaces. Zero is an
  expected value.
\item[median.household.income.log1p.std] The median income derived from the
  census bureau data.
\end{description}

\subsection{Non-size, Continuous Features}

These features are numeric and judged to not be a size feature. These
features are present in every observation. The log isn't take, because
the feature is assumed to contribute to the log price in a linear
fashion. The feature values are standardized in an attempt to get them
all to a similar scale.
\begin{description}
\item[EFFECTIVE.YEAR.BUILT.std] The last year in which the improvement
  acquired its current construction.
\item[avg.commute.std] The average commute in minutes for the census tract.
\item[fraction.own.occupied] The fraction of residences in the census
  tract that are occupied by their owners.
\item[G.LATTIUDE.std] The latitude of the parcel.
\item[G.LONGITUDE.std] The longitude of the parcel.
\item[fraction.improvement.value.std] This is the improvement value
  divided by the sum of the improvement value and the land value. All
  other things being equal, a larger value reflects a more valuable
  improvement. It is an indication of the market attractiveness of the
  house.
\end{description}




DO NEXT TYPE OF FEATURE

The 1.6 million transactions all became part of the observation
set. However, not all variables in the transactions were retained in
the observations. The goal was to retain variables that were either
always coded or often coded. When a value was missing, one needs to
decide whether to drop the entire transaction, to treat the missing
value as if it were coded as some other value, or to impute the
missing.

\input{thesis-epilogue}


 
% vim:fdm=syntax
