\chapter{Finding the Best Linear Model}
% vim: textwidth=72
% vim: foldmethod=manual

<<BestLinearModel>>=
working <- control$working
path.cv.criterion <- paste0(working, 'e-cv-chart_chart7.txt')
path.cv.form.ndays <- paste0(working, 'e-cv-chart_chart8.txt')
path.lcv2 <- paste0(working, 'e-features-lcv2-chart_1.txt')
path.9.txt <- paste0(working, 'e-cv-chart_chart9.txt')
path.9.gg1 <- paste0(working, 'e-cv-chart_chart9_1.pdf')
path.9.gg2 <- paste0(working, 'e-cv-chart_chart9_2.pdf')
path.pca.1 <- paste0(working, 'e-features-pca-chart_1.txt')
path.pca.2.01 <- paste0(working, 'e-features-pca-chart_2_01.txt')
path.pca.2.02 <- paste0(working, 'e-features-pca-chart_2_02.txt')
path.pca.2.03 <- paste0(working, 'e-features-pca-chart_2_03.txt')
path.10.txt <- paste0(working, 'e-cv-chart_chart10.txt')
path.10.gg1 <- paste0(working, 'e-cv-chart_chart10_1.pdf')
path.10.gg2 <- paste0(working, 'e-cv-chart_chart10_2.pdf')
path.11.gg1 <- paste0(working, 'e-cv-chart_chart11_1.pdf')
path.11.gg2 <- paste0(working, 'e-cv-chart_chart11_2.pdf')
path.12.gg1 <- paste0(working, 'e-cv-chart_chart12_1.pdf')
path.12.gg2 <- paste0(working, 'e-cv-chart_chart12_2.pdf')
path.12.txt <- paste0(working, 'e-cv-chart_chart12_3.txt')
@ 

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n. \]

From this simple idea, a huge variety of models can be generated. Which
linear models are most accurate for predicting the prices of houses?

We answer this question by examining data from 2003 and later using the
decennial census from the year 2000 but not the tax assessment for 2008.
Our interest is in estimating the generalization error for unseen
transactions. For this, we use 10-fold cross validation on the training
portion of the subset1 data.

Property descriptions come from the tax roll for the year 2008. The
tax roll contains property descriptions as of December 31, 2007. We
need to exclude from out data any houses built or modified after that
date. An analysis shows that there are no such houses.

The plan of attack is EXPLAIN THE SECTIONS.


Explain the scope = entire market models. There is one model for every
house to be estimated.

Explain local models

\section{Figure of Merit}

The literature often compares models using the fraction of estimates
within 10 percent of the known values in training data. The motivation
for this figure of merit is in part that Freddie Mac is said
(\cite{fik-03-spatial} footnote on page 642) to use that metric to judge
the effectiveness of automated valuation models. In any case, using
fraction within 10 percent (along with fractions with 20 and 30 percent)
is popular in the real estate pricing literature.

It is a curious choice of metric for at least two reasons. The first
reason is that linear models are often trained to minimize the mean
squared error (MSE). So fraction within 10 percent isn't the criteria
the model was trained to deliver. The second reason is that selecting
fraction within $x$ percent is implicitly a decision to favor models
that are close when accurate and potentially very far from the true
prices when not accurate. 

Here's a contrived example that illustrates that point: suppose there
are two properties both of which sold for \$100,000. Suppose we have two
models to consider. Model A produces estimates of  \$100,000 and \$200,000:
it is either perfect or very wrong. Model B produces estimates of
\$85,000 and \$115,000: it's never super accurate but never very wrong.
Model A has 50 percent of its estimates within 10 percent of the true
value, Model B has 0 percent of its estimates within 10 percent of the
true value. So under the fraction-with-$x$-percent metric, we prefer
model A over model B. 

Now imagine using model A. When it delivers its estimate, we have no
idea whether the estimate is good or bad.  To me, this situation
is untenable if there is an acceptable alternative (and there is: use
MSE). To make the untenable situation acceptable, one would need to
produce both an estimate and a confidence in the estimate.  Now the
model has to be trained to deliver two values, and we have conceivable
more than doubled the difficulty of the problem of selecting the best
model: there are now two figures of merit and how to make tradeoffs
between them is not obvious.

The issue of choice of figure of merit would be moot if using either
metric resulting in selecting the same model as best. To investigate
this, we designed an experiment. In the experiment, we consider
the estimated generalization error from a variety of model. The models
were generated by sweeping these design choices:

\begin{itemize}
\item Response variable: either the price or log(price)
\item The form of the predictors: either level (natural units) or log of
natural units
\item The number of days in the training period, from 30 up to 360 in
steps of 30 days.
\end{itemize}

All of these models used all features always present except for those
derived from the tax assessor data. All models used data from 2003 on.
To keep running times reasonable, a random one percent sample of possible
test transactions (queries) was drawn from each cross-validation fold.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.criterion}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Measurement Approaches Compared }
\label{figure:merit1}
\end{figure}

The results are depicted in Figure \ref{figure:merit1}.

To read the table, consider the first three rows. All of these are for
the price-level form of the linear model: we predict price using the
natural units for each feature. The three rows show the value of a model
selection metric for 30, 60, and more days. An asterisk before a number
indicates that the number is the minimizer for the row. The metrics are
\code{meanRMSE}, the mean of the root mean squared error for each fold;
\code{medRMSE}, the median of the root median squared error for each
fold; and \code{fctWI10}, the mean fraction of estimates within 10
percent of the actual values for each fold.

There are several observations to make:

\begin{itemize}

\item It's never true that the metric \code{fctWI10} would select as best the
same model that would be choosen by either of the other metrics. Often
the chosen models would have very different training periods. For
example, for the price-level model, both \code{meanRMSE} and
\code{medRMSE} suggest training on 60 days of data and \code{fctWI10}
suggests training on 330 days of data. A similar conclusion holds for
the logprice-level model and the logprice-log model. The metric leads to
very different training periods.

\item It's often true that the metric \code{meanRMSE} and
\code{medRMSE} select a similar number of training days. The number of
training days is either identical or differs by at most 60 days. These
metrics have similar choices for models selected.

\item It's always true that the metric \code{meanRMSE} greatly exceeds
\code{medRMSE}. This is because there is right-skew in the prices: the
mean exceeds the median, so that the mean error can greatly exceed the
median error.

\end{itemize}

Going forward, we focus on the \code{medRMSE} metric, judging that it
more intutitively sizes average errors than does \code{medianRMSE} and
that is is more reliable for model selection than \code{fctWI10}.


\section{Entire-market Models}

This section explores linear models that are specified once for the
entire marketplace: there is one model form for every query transaction
and this model is location aware only through features of the census
tract and zip code of the property. I call theses kinds of models
``entire-market models.''

With the class of entire-market models, we explore design choices. In
the first subsection that follows, we examine the form of the equation
in the model and the training period used to fit the model. We then
examine what features give the best predictive performance and examine a
reduced-feature model. We then show that regularizing the linear model
with an L2 regularizer improves performance.

\subsection{Model form and ndays}

In the literature, a popular model for is the log-linear form: one
predicts $log(price)$ using features in their natural units. Other
choices for model form include log-log, in which the features are in the
log domain, level-level in which price is predicted using features in
the natural units, and level-log in which price is predicted using
features in the log domain.

At least three rationales can be invoked to choose a model. First is
that the form of the model should conform to one's prior about the data
generation process. If you suppose that doubling the lot size might
double the price, you can admit this possibility by specifying
$\log(price) = \beta \ log(size)$. If you suppose that adding a bedroom
adds a certain amount of value, you would want to have $price = \beta \
bedrooms$. If you suppose both are true, you are stuck.

A second rationale for choosing the form is to transform the prices to a
form more compatible with the statistical assumptions of the model.
Since real estate prices tend to be right-skewed, a log transformation
would make the transformed distribution more normal.

A third rationale is to transform the price to the log domain in order
to avoid applying equal importance with respect to prediction accuracy
to expensive and less expensive houses. The idea is to fit the model to
minimize average error not price-weighted error. 

% OMIT, SINCE WE DIDN"T TEST THESE log-mixed form
%Sometimes these rationale's are combined. For example, in
%\cite{chopra-09}, prices were transformed to the log domain in order to
%not overweight the importance of accuracy for larger houses, and size
%feature (such as lot size and interior space) were transformed into the
%log domain to allow for the possibility that doubling them would double
%the price.

Here we focus on the third rationale and simply ask: why model form
yields the lowest estimated generalization errors as measured by the
median rMedianSE across folds.

Another issue is to determine the best training period. The linear
models are setting the marginal cost of each feature. If prices are
moving slowly, then training on a longer periods of time might be
beneficial.  If prices are moving rapidly, then training only on recent
data may be necessary.

Our experiment fixed the time period (2003 on) and the feature set
(all features always in transactions excluding the features derived from
the tax assessment). We vary the model form and number of training days.
Because a local model is built for every query point, we take a one
percent random sample of the possible query points in each fold in order
to reduce the computational burden.

Figure \ref{figure:EGEFormNdays} shows the estimated generalization
errors from 10-fold cross validation. Comparing the first two rows, we
see that when price is the response variable, more accuracy is obtained
by working with features in natural units than in the log domain.
Comparing all the rows, we see that estimating $\log(price)$ instead of
$price$ always leads to reduced estimated generalization errors.
Comparing rows three and four shows that the for most training periods,
using a log-level model outperforms a log-log model. Finally, the lowest
estimated generalization error is for the log-level model when trained
for 60 days.

We use this model form and training period as a benchmark going forward.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.form.ndays}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Across Model Forms and Training
Period}
\label{figure:EGEFormNdays}
\end{figure}

\subsection{Feature selection}

The benchmark model has two features. Which of these are the best
features to use? Here we consider two approached. In the first, we
use a simple heuristic to determine the best features to use. In the
second, we use PCA for the same purpose. We then compare results and
have one surprising findings.

\subsubsection{LCV Heuristic}

We have 24 features that are candidates for inclusion in the model. A
straight-forward way to determine the best feature combinations is to
consider all possible subsets and estimate the generalization error for
each. This would be way too time consuming.

Our idea is to instead use L1 regularization to consider determine the
rank-ordering of the 24 features in terms of their importance. We then
use cross validation to estimate the generalization error from a model
with the first, the first two, \dots, all 24 rank-ordered features. We
call the procedure LCV, for L1 regularization followed by cross
validation.

The implementation is by using the Elastic Net from Zoe and Hastie
(\cite{zou-05}). It solves a linear model by concurrently using an L1
and L2 regularizer. As a by-product, the \code{elasticnet} R package
(\cite{zou-12}) produces a rank-ordering of the features from Zoe and
Hastie (\cite{zou-05}). It fits a linear model by concurrently using
L1 and L2 regularizer. 

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.9.txt}}
\caption{Median rMedianSE Table}
\label{figure:MedianTable}
\end{figure}

The result of the rank-ordering from the Elastic Net procedure is in
Figure \ref{figure:MedianTable}. It shows that the most important
feature is the living area of the house. Real estate agents claim the
most important features of the house its its location, but that isn't
true in these data. Indeed, a features of the location (median household
income and average commute time) are second and third in importance. 

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph1}
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg1}}
\end{figure}

The same data are shown in \ref{figure:MedianGraph1}, which shows the
estimated generalization error from including the first $n$ features,
together with a 95 percent confidence interval. One sees that the lowest
estimated error is when using the first 20 features, from living area to
census tract has park. The estimated errors then tend to increase and
the placement of the confidence intervals does not compel one to
consider going beyond the first 20 features. 

If one seeks parsimony, one could consider using the first eight
features (through zip5 has industry), after which there is an increase
in estimated generalization error before a subsequent decline. Another
possible feature set is the first 20 feature (through census tract has
park), as that feature set has the lowest estimated error. Another
choice would be 19 features (through factor is new construction) as
though it would have a slightly higher median estimated generalization
error than for the 20-feature choice, the 95% confidence interval is
smaller.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.lcv2}}
\caption{Most Important Features}
\label{figure:LCV}
\end{figure}

Figure \ref{figure:LCV} shows the 24 rank-ordered features split into
house features and location features. Some observations on the house
features:

\begin{itemize}

\item living area: this is the interior living space. Construction is
required to change it, so its not surprising to see it highly-ranked.
That is is more highly ranked than the land square footage is a surprise
to me.

\item fireplace number: the number of fireplaces, including the
possibility of zero. A surprise, perhaps fireplaces are proxies for
overall amenities in the house.

\item year built: the year the house was built. Other studies have
concluded that the age and the squared age of the house are important
features. Our models
are local, so that we need a house age for every query transaction.
Instead of pre-building a training set for each possible query, we
transform year built into age and age squared just before fitting the
model.

\item factor has pool: whether there is a swimming pool. Not a
surprise, as the houses are all in Los Angeles, where the weather is
often pleasant.

\end{itemize} 

Some observations on the location features:

\begin{itemize}

\item median household income and fraction owner occupied: the median
income in the census tract from the year 2000 census and the fraction of
houses in the census tract that are occupied by owners. Both of these
features indicate the wealth of the neighborhood.

\item average commute time: in other studies, longer commutes are
associated with lower prices.

\item zip 5 and census tract have industry: being near industry is not
good for prices.

\item census tract and zip have park: Los Angeles residents are not
there for the parks. Perhaps this is because its a driving-around city,
not a walking-around city.

\end{itemize}

%
%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph2}
%\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
%\end{figure}
%

\subsubsection{PCA}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.1}}
\caption{Principal Components\\Cumulative Variance}
\label{figure:PCA1}
\end{figure}

The second approach to determining the importance of features is to use
Principal Components Analysis. Figure \ref{figure:PCA1} shows the the
principle components associated with the 24 features. We see that the
first three principal components account for almost 100 percent of the
variance.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.01}}
\caption{Principal Components\\Component 1}
\label{figure:PCA201}
\end{figure}

In Figure \ref{figure:PCA201} we see the weights of the 24 features in
the first principal component. The median household income has the
highest absolute weight. Other weights are much lower in absolute value,
The only ones of much importance are the land square footage and the
living area. It seems that the first principal component may be about
the wealth of the neighborhood: more income is correlated with larger
plots is correlated with bigger houses.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.02}}
\caption{Principal Components\\Component 2}
\label{figure:PCA202}
\end{figure}

In Figure \ref{figure:PCA202} we see the weights of the 24 features in
the second principal component. The land square footage has the highest
absolute weight. Other weights are much lower in value with household
income and living area being the largest in absolute size. This
principal component is possibly about the size of the land and has the
same correlated highly-weighted features as the first principal
component.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.03}}
\caption{Principal Components\\Component 3}
\label{figure:PCA203}
\end{figure}

In Figure \ref{figure:PCA203} we see the weights of the 24 features
for third principal component. It is mostly composed of the basement
square feet, perhaps a proxy for other size features in the house:
bigger basement probably implies bigger house.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.10.txt}}
\caption{Median rMedianSE Table}
\label{figure:MedianTable10}
\end{figure}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph102}
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg1}}
\end{figure}

%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph102}
%\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg2}}
%\end{figure}

In Figure \ref{figure:MedianTable10} we see the estimated generalization
error from using the median household income, median household income
and land square foot, and so on up to the 4 features identified as
important in the PCA work. The same data are in Figure
\ref{figure:MedianGraph102} which shows the 95 percent confidence
intervals graphically. One might choose to use only the first 3 features
based on this graphic.

\subsubsection{Reduced Feature Models} %%%%%%%%%%%%%%%%%%%%%%%%

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE
}
\label{figure:MedianGraph111}
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg1}}
\end{figure}

To find the best reduced-feature model, we test all the feature sets
suggested by the LCV and PCA analyses. These results are in Figure
\ref{figure:MedianGraph111}, which brings together results previously
shown. A surprising conclusion is that the PCA-derived feature sets all
underperform the LCV-derived feature sets: extremely small feature sets
are challenged in this setting. The best feature sets is the
one with the first 20 features in Figure \ref{figure:LCV}

%
%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph112}
%\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg2}}
%\end{figure}

\subsection{Regularization}    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regularizing models is one way to avoid overfitting and thereby improve
prediction accuracy. So far the models are all linear and are not
regularized.

To assess the extent to which regularization leads to more accuracy, we
take the model using the 20 best features and run multiple versions using
several choices for the weight of an L2 regularizer. A simple grid
search was used. 

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.12.txt}}
\caption{Estimated Generalization Error from L2 Regularization}
\label{figure:Regularize3}
\end{figure}

% including a pdf \begin{figure}[h]
\begin{figure}[ht]
\caption{Estimated Generalization Error from L2 Regularization }
\label{figure:Regularization1}
\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg1}}
\end{figure}

Figure \ref{figure:Regularize3} contains the result in a table. Figure
\ref{figure:Regularization1} has the same results graphically. We see
the typical pattern, small values for the regularizer importance improve
estimated accuracy and larger values decrease accuracy. Setting the
weight to $\lambda = 4$ seems like a good choice: it has the lowest
median across the cross-validation folds and the 95 percent confidence
intervals are not signally warnings.


%% including a pdf
%\begin{figure}[h]
%\caption{
%Estimated Generalization Error from L2 Regularization
%\\
%Not Showing Origin
%}
%\label{figure:Regularization2}
%\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg2}}
%\end{figure}
%

\section{Submarket Models}

\subsection{Using Indicator Variables}
\subsection{Zip vs. Census vs. City}

\section{The best linear model is \dots}

\section{Coda: A Non-Linear Model, Random Forests}
