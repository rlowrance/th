\chapter{Finding the Best Linear Model}
% vim: textwidth=72
% vim: foldmethod=manual

<<BestLinearModel>>=
working <- control$working
path.7.txt <- paste0(working, 'e-cv-chart_chart7.txt')
path.8.txt <- paste0(working, 'e-cv-chart_chart8.txt')
path.lcv2 <- paste0(working, 'e-features-lcv2-chart_1.txt')
path.9.txt <- paste0(working, 'e-cv-chart_chart9.txt')
path.9.gg1 <- paste0(working, 'e-cv-chart_chart9_1.pdf')
path.9.gg2 <- paste0(working, 'e-cv-chart_chart9_2.pdf')
path.pca.1 <- paste0(working, 'e-features-pca-chart_1.txt')
path.pca.2.01 <- paste0(working, 'e-features-pca-chart_2_01.txt')
path.pca.2.02 <- paste0(working, 'e-features-pca-chart_2_02.txt')
path.pca.2.03 <- paste0(working, 'e-features-pca-chart_2_03.txt')
path.10.txt <- paste0(working, 'e-cv-chart_chart10.txt')
path.10.gg1 <- paste0(working, 'e-cv-chart_chart10_1.pdf')
path.10.gg2 <- paste0(working, 'e-cv-chart_chart10_2.pdf')
path.11.gg1 <- paste0(working, 'e-cv-chart_chart11_1.pdf')
path.11.gg2 <- paste0(working, 'e-cv-chart_chart11_2.pdf')
path.12.gg1 <- paste0(working, 'e-cv-chart_chart12_1.pdf')
path.12.gg2 <- paste0(working, 'e-cv-chart_chart12_2.pdf')
path.12.txt <- paste0(working, 'e-cv-chart_chart12_3.txt')
path.13.indicator  <- paste0(working, 'e-cv-chart_chart13_indicators.txt')
path.13.submarkets <- paste0(working, 'e-cv-chart_chart13_submarkets_summary.txt')
path.13.examples   <- paste0(working
                             ,'e-cv-chart_chart13_submarkets_examples_property_city.txt')
path.14.rf.1 <- paste0(working, 'e-cv-chart_chart14_1.txt')
path.14.rf.2 <- paste0(working, 'e-cv-chart_chart14_2.txt')
@ 

Linear models predict the price as a linear function of features:

\[ price = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n. \]

From this simple idea, a huge variety of models can be generated. Which
linear models are most accurate for predicting the prices of houses?

We answer this question by examining data from 2003 and later using the
decennial census from the year 2000 but not the tax assessment for 2008.
Our interest is in estimating the generalization error for unseen
transactions. For this, we use 10-fold cross validation.

Property descriptions come from the tax roll for the year 2008. The
tax roll contains property descriptions as of December 31, 2007. We
need to exclude from our data any houses built or modified after that
date. An analysis shows that there are no such houses.

The plan of attack is reflected in the sections that follow.

\begin{itemize}

\item Section 1 explains our choice of metric to compare models. We
show that using different metrics leads to judging that different
models are better. We decide to focus on median errors.

\item Section 2 studies design choices for linear models that cover the
entire market.  We start by considering model forms and the number of
training days that lead to the most accurate models. (The term ``model
form'' captures whether transformations into the log domain are used in
the left hand and right hand sides of the model. More on this later.)
Having fixed form and number of training days, we then consider which
features are best and compare heuristics for selecting the best
features.  Finally, fixing form, number of  training days, and feature
sets, we show that regularization improves estimation accuracy.

\item Section 3 considers submarket models. Submarket models partition
the possibly queries and use the partitions in the fitting and
prediction processes. We consider both indicator-based submarket models,
in which the submarkets are represented by indicator variables in a
single model for all queries, and separate submarket models, in which a
separate model is build for each submarket. We consider multiple
definitions for submarkets. We find that submarket models outperform
entire-market models.

\item Section 4 summarizes our results for linear models and provides
some guidelines for extending the work into a commercial setting.

\item Section 5 contrasts the results from the linear models with a
non-linear model: the random forests model. We show that a
random forests model outperforms our best linear model.

\end{itemize}

\section{Figure of Merit}

The literature often compares models using the fraction of estimates
within 10 percent of the known values. The motivation for this figure of
merit may in part be that Freddie Mac is said \cite[footnote on
p.~642]{fik-03-spatial} to use that metric to judge the effectiveness of
automated valuation models. In any case, using fraction within 10
percent (along with fraction within 20 and 30 percent) is popular in the
real estate pricing literature.

It is a curious choice of metric for at least two reasons. The first
reason is that linear models are often trained to minimize the mean
squared error. So fraction within 10 percent isn't the criteria
the model was trained to deliver. The second reason is that selecting
fraction within $x$ percent is implicitly a decision to favor models
that are close when accurate and to not minimize the average error.

Here's a contrived example which illustrates that point. There
are two properties both of which sold for \$100,000. Suppose we have two
models to consider. Model A produces estimates of  \$100,000 and
\$200,000.  It is either perfect or very wrong. Model B produces
estimates of \$85,000 and \$115,000. It is never super accurate but
never very wrong.  Model A has 50 percent of its estimates within 10
percent of the true value, Model B has 0 percent of its estimates within
10 percent of the true value. So under the fraction-with-$x$-percent
metric, we prefer model A over model B. 

Now imagine using model A. When it delivers its estimate, we have no
idea whether the estimate is good or bad.  To me, this situation
is untenable if there is an acceptable alternative (and there is: use
some variant on mean squared error). To make the untenable situation
acceptable, one would need to produce both an estimate and a confidence
in the estimate.  Now the model has to be trained to deliver two values,
and we have conceivably more than doubled the difficulty of the problem
of selecting the best model: there are now two figures of merit and how
to make tradeoffs between them is not obvious.

The issue of choice of figure of merit would be moot if using either
metric resulted in selecting the same model as best. To investigate
whether the issue is moot, we designed an experiment. In the experiment,
we consider the estimated generalization error from a variety of models.
The models were generated by sweeping these design choices:

\begin{itemize}

\item Response variable: either the $price$ or $\log(price)$

\item The form of the predictors: either level (natural units) or log of
natural units for size features

\item The number of days in the training period, from 30 up to 360 in
steps of 30 days

\item The figure of merit used to select the model: one of the mean of
the root mean squared errors from the folds (\code{meanRMSE} in the
figure), the median of the root median squared errors from the folds
(\code{medRMSE}), or the fraction within 10 percent of actual values
(\code{fctWI10}).

\end{itemize}

All of these models used all features always present except for those
derived from the tax assessor data. All models used data from 2003 on.
To keep running times reasonable, a random one percent sample of possible
test transactions (queries) was drawn from each cross-validation fold.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.7.txt}}
\caption{
Estimated Generalization Errors\\
For Model Selection Metrics\\
By Response Variable, Predictors, and Training Periods\\
For Sales in 2003 and Later
}
\label{figure:merit1Vertical}
\end{figure}

The results are depicted in Figure \ref{figure:merit1Vertical}.

The table has two panels. In panel A, the response variable is price. In
panel B the response variable is $\log(price)$. Each panel has a row for
each training period considered. The length of the training period is in
column \code{ndays}. Adjacent to the training period are two groups of
columns. The first group is for the level form of the predictors: we use
the predictors in their natural units. The second group is for the log
form of the predictors: we transform the size features into the log
domain. Each group of three column has the estimated error using one of
the metrics. Our interest is to determine which model is best. For the
first two metrics, \code{meanRMSE} and \code{medRMSE} we seek the lowest
error. For the third, \code{fctWI10}, we seek the highest accuracy.

You will notice very high value in Panel B, column 2. These are the
estimated errors using the \code{meanRMSE} metric for the log-level form
of the linear model. What is happening is that a few houses are very
badly predicted. These houses tend to have unusually large values for
some features, causing the $\log(price)$ estimate to be very much higher
than the actual transaction price.

\begin{figure}[ht]
\begin{tabular}{ | l | r | r | r| }
 \hline
             & \multicolumn{3}{c |}{best ndays} \\
 \hline
 form        & \code{meanRMSE} & \code{medRMSE} & \code{fctWI10}\\
 \hline
 level-level & 180 & 90 &  90 \\
 level-log   & 120 & 90 & 120 \\
 log-level   & 195 & 30 &  30 \\
 log-log     & 120 & 30 &  30 \\
 \hline
\end{tabular}
\caption{
Preferred Number of Training Days\\
By Model Selection Metric\\
For Model Forms\\
Summarizing Figure \ref{figure:merit1Vertical}
}
\label{figure:Analysis}
\end{figure}

We seek to answer the question: which model is selected as best by each
metric? Figure \ref{figure:Analysis} contains these results. 

There are several observations to make:

\begin{itemize}

\item In no cases, do all three metrics pick the same number of training
days.

\item The metrics \code{medRMSE} and \code{fctWI10} closely agree on the
best number of training days. They agree on all model forms except for
the log-log form, and there they are only 30 days apart.

\item The metric \code{meanRMSE} usually picks more training days as
optimal than do the other metrics.

\item It's always true that the expected loss for the metric
\code{meanRMSE} greatly exceeds that for \code{medRMSE}. This is because
there is right-skew in the prices: the mean exceeds the median, so that
the mean error can exceed the median error.

\end{itemize}

We conclude that the choice of metric does influence the choice of the
best training period to use.

Going forward, we focus on the \code{medRMSE} metric, judging that it
more intuitively sizes average errors than does \code{meanRMSE} (because
of the right-skew in the prices) and reflecting our predisposition to
minimize average errors rather than the tightness of close estimates.


\section{Entire-market Models}

This section explores linear models that are specified once for the
entire marketplace: there is one model for every query transaction and
this model is location aware only through features of the census tract
and zip code of the property. I call these kinds of models
``entire-market models.''

Within the class of entire-market models, we explore design choices. In
the first subsection that follows, we examine the form of the equation
in the model and the training period used to fit the model. We find that
training using 60 days of data before the query transaction is best. The
best form of model to use is log-level, so that one would predict the
log of price using the features in natural units. We then examine,
within this choice of training period and form,  which features give the
best predictive performance and examine a reduced-feature model. We show
that a 20-feature model performs best. Finally, using the 20 feature
model trained on 60 days of data in log-level form,  we show that
regularizing the linear model with an L2 regularizer improves
performance.

\subsection{Model form and number of training days}

In the literature, a popular model for is the log-level form: one
predicts $\log(price)$ using features in their natural units. Other
choices for model form include log-log, in which the log of price is
predicted using size features that are in the log domain and non-size
features in their natural units, level-level in which price is predicted
using features in their natural units, and level-log in which price is
predicted using size features in the log domain and non-size features in
their natural units.

At least three rationales can be invoked to choose a model form. First is
that the form of the model should conform to one's prior assumptions
about the data generation process. If you suppose that doubling the lot
size might double the price, you can admit this possibility by
specifying $\log(price) = \beta \ log(size)$, a log-log form. If you
suppose that adding a bedroom adds a certain amount of value, you would
want to have $price = \beta \ bedrooms$, a level-level form. 

A second rationale for choosing the form is to transform the prices to a
form more compatible with the statistical assumptions of the model.
Since real estate prices tend to be right-skewed, a log transformation
would probably make the transformed distribution more normal.

A third rationale is to transform the price to the log domain in order
to avoid applying equal importance with respect to prediction accuracy
to expensive and less expensive houses. By prediction $\log(price)$
instead of $price$, the idea  is to fit the model to minimize errors on
the more typical, less expensive houses, not to minimize the
price-weighted error, which could be regarded as overly influenced by a
few very large houses.

% OMIT, SINCE WE DIDN"T TEST THESE log-mixed form
%Sometimes these rationale's are combined. For example, in
%\cite{chopra-09}, prices were transformed to the log domain in order to
%not overweight the importance of accuracy for larger houses, and size
%feature (such as lot size and interior space) were transformed into the
%log domain to allow for the possibility that doubling them would double
%the price.

Here we focus on the third rationale and simply ask: Which model form
yields the lowest estimated generalization errors as measured by the
median of the root median squared error across folds? 

Another issue is to determine the best training period. The linear
models are setting the marginal price of each feature. If prices are
moving slowly, then training on a longer period of time might be
beneficial.  If prices are moving rapidly, then training only on recent
data might be necessary.

Our experiment fixed the time period (2003 on) and the feature set
(all features always in transactions excluding the features derived from
the tax assessment). We vary the model form and number of training days.
Because a local model is built for every query point, we take a one
percent random sample of the possible query points in each fold in order
to reduce the computational burden.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.8.txt}}
\caption{
Estimated Generalization Errors\\
By Response Variable, Predictors, and Training Period\\
Using Census Features and Not Tax Assessor Features\\
For Sales in 2003 and Later
}
\label{figure:EGEFormNdays01}
\end{figure}


<<FormNdays, include=FALSE>>=
lowest <- 58862
highest <- 77560
p <- 1 - (lowest / highest)
p
Percent <- function(p) {
sprintf('%2.0f', p * 100)
}
@

Figure \ref{figure:EGEFormNdays01} shows the estimated generalization
errors from 10-fold cross validation. The lowest error is for $ndays =
30$ and is in the fourth column, which holds errors for the log-level
model. Thus the lowest error occurs when training for 30 days and
predicting $\log(price)$ using the features in their natural units. The
log-level model (in column four) always has the lowest error. One way to
think about the results is to consider a very naive model builder who
would build a level-level model and use a year of training data. The
model builder would find an error that is the last entry in the second
column: \$\Sexpr{Commas(highest)}.  The best error is
\$\Sexpr{Commas(lowest)}, which is \Sexpr{Percent(p)} percent lower.


In our search for the best linear model, going forward we fix the number
of training days to 30 and use the log-level form for the model.


\subsection{Feature selection}

The models in the previous sections all use the 24 features that always
appear in our transactions and were not derived from the tax assessment.
Which of these are the best features to use? Here we consider two
approaches. In the first, we use a simple heuristic to determine the
best features to use. In the second, we use Principal Components
Analysis (PCA) for the same purpose.  We then compare results.

\subsubsection{LCV Heuristic}

We have 24 features that are candidates for inclusion in the model. A
straight-forward way to determine the best feature set is to consider
all possible subsets and estimate the generalization error for each.
Doing so would be way too time consuming.

Our idea is to instead use L1 regularization to determine the
rank ordering of the 24 features in terms of their importance. We then
use cross validation to estimate the generalization error from a model
with the first, the first two, \dots, all 24 rank ordered features. We
call the procedure LCV, for L1 regularization followed by cross
validation.

The implementation is by using the Elastic Net from Zoe and Hastie
\cite{zou-05}. It fits a linear model while concurrently using an L1
and L2 regularizer. As a by-product, an R implementation, the
\code{elasticnet} R package \cite{zou-12}, produces a rank ordering of
the features in terms of their importance. In our use of this package,
we set the L2 regularizer to zero and use just the rank ordering of
features under the L1 regularizer.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.9.txt}}
\caption{
Table of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For 24 Sets of Features\\
For Model Form log-level\\
For 30 Day Training Period\\
For Sales in 2003 and Later
}
\label{figure:MedianTable}
\end{figure}

The result of the rank ordering from the Elastic Net procedure is in
Figure \ref{figure:MedianTable}. It shows that the most important
feature is the living area of the house. Real estate agents often claim
the most important feature of the house is its location, but that isn't
true in these data. Instead, features of the location (median household
income and average commute time) are second and third in importance. 

% including a pdf
\begin{figure}[h]
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
\caption{
Graph of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For 24 Sets of Features\\
For Model Form log-level\\
For 30 Day Training Period\\
For Sales in 2003 and Later
}
\label{figure:MedianGraph1}
\end{figure}

The same data are shown graphically in Figure \ref{figure:MedianGraph1},
which displays the estimated generalization error from including the
first $n$ features, together with a 95 percent confidence interval. One
sees that the lowest estimated error is when using the first 15
features, from living-area to zip5-has-school. The estimated errors then
tend to increase and the placement of the confidence intervals does not
compel one to consider going beyond the first 15 features. 

If one seeks parsimony, one could consider using the first six
features (through fraction-owner-occupied), after which there is an increase
in estimated generalization error before a subsequent decline to the
minimum at 15 features.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.lcv2}}
\caption{
24 Features\\
By Importance Rank\\
Classified as House Features or Location Features
}
\label{figure:LCV}
\end{figure}

Figure \ref{figure:LCV} shows the 24 rank ordered features split into
house features and location features. Some observations on the house
features:

\begin{itemize}

\item living-area: this is the interior living space. Construction is
required to change it, so it is not surprising to see it highly-ranked.
That it is more highly ranked than the land square footage is a surprise
to me, because land square footage seems harder to change than interior
living space.

\item fireplace-number: the number of fireplaces, including the
possibility of zero. A surprise---perhaps fireplaces are proxies for
overall amenities in the house.

\item year-built: the year the house was built. Other studies have
concluded that the age and the squared age of the house are important
features. Our models are local, so that we need a house's age for every
query transaction.  Instead of pre-building a training set for each
possible query, we transform year built into age and age squared just
before fitting the model. We perform a similar transformation for the
effective year built, which is the last year of significant updates to
the house.

\item factor-has-pool: whether there is a swimming pool. Not a
surprise, as the houses are all in Los Angeles, where the weather is
often pleasant and a swimming pool might be a joy.

\end{itemize} 

Some observations on the location features:

\begin{itemize}

\item median-household-income and fraction-owner-occupied: the median
income in the census tract from the year 2000 census and the fraction of
houses in the census tract that are occupied by owners. Both of these
features indicate the wealth of the neighborhood.

\item average-commute-time: in other studies, longer commutes are
associated with lower prices.

\item zip5-has-industry and census-tract-has-industry: being near
industry is not good for house prices.

\item census-tract-has-park and zip5-has-park: Los Angeles residents are
not there for the parks. Perhaps this is because it is a driving-around
city, not a walking-around city.

\end{itemize}

%
%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph2}
%\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
%\end{figure}
%

\subsubsection{PCA}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.1}}
\caption{Cumulative Variance of the 24 Principal Components}
\label{figure:PCA1}
\end{figure}

The second approach we used to determining the importance of features is 
Principal Components Analysis. Figure \ref{figure:PCA1} shows the
principal components associated with the training set predictors. We see
that the first three principal components account for almost 100 percent
of the variance.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.01}}
\caption{Feature Weights for the First Principal Component}
\label{figure:PCA201}
\end{figure}

In Figure \ref{figure:PCA201} we see the weights of the 24 features in
the first principal component. The median household income has the
highest absolute weight. Other weights are much lower in absolute value.
The only ones of much importance are the land square footage and the
living area. It seems that the first principal component may be about
the wealth of the neighborhood.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.02}}
\caption{Feature Weights for the Second Principal Component}
\label{figure:PCA202}
\end{figure}

In Figure \ref{figure:PCA202} we see the weights of the 24 features in
the second principal component. The land square footage has the highest
absolute weight. Other weights are much lower in value with household
income and living area being the largest in absolute size. This
principal component is possibly about the size of the land.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.03}}
\caption{Feature Weights for the Third Principal Component}
\label{figure:PCA203}
\end{figure}

In Figure \ref{figure:PCA203} we see the weights of the 24 features
for third principal component. It is mostly composed of the basement
square feet, perhaps a proxy for other size features in the house:
bigger basement probably implies bigger house. This principal component
is possibly about the size of the house.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.10.txt}}
\caption{
Table of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Feature Sets Selected by the PCA Analsysis
}
\label{figure:MedianTable10}
\end{figure}

% including a pdf
\begin{figure}[h]
\caption{
Graph of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Feature Sets Selected by the PCA Analsysis
}
\label{figure:MedianGraph102}
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg1}}
\end{figure}


In Figure \ref{figure:MedianTable10} we see the estimated generalization
error from using the median household income, land square footage, and
so on up to the 4 features identified as important in the PCA work. The
same data are in Figure \ref{figure:MedianGraph102} which shows the 95
percent confidence intervals graphically. If you prefer sparsity, you
might use the first 3 features. If you prefer the lowest error, you
might choose the first 4 features.


\subsubsection{Reduced Feature Models} %%%%%%%%%%%%%%%%%%%%%%%%

% including a pdf
\begin{figure}[h]
\caption{
Graph of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Feature Sets Selected by the LCV and PCA Analyses
}
\label{figure:MedianGraph111}
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg1}}
\end{figure}

<<Reduce, include=FALSE>>=
error.best20 <- 57880
error.pca.4 <- 71832
1 - (error.best20 / error.pca.4)
@

To find the best reduced-feature model, we test all the feature sets
suggested by the LCV and PCA analyses. These results are in Figure
\ref{figure:MedianGraph111}, which brings together results previously
shown. The PCA-derived feature sets all underperform the LCV-derived
feature sets: extremely small feature sets are challenged in this
setting. The best feature set is the one with the first 20 features from
the LCV heuristic. This feature set has an estimated error of \$57,880,
which is 19 percent lower than the estimated error of \$71,832 that was
found by PCA.

\subsection{Regularization}    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regularizing models is one way to avoid overfitting and thereby improve
prediction accuracy. So far the models are not regularized.

To assess the extent to which regularization leads to more accuracy, we
take the model using the 20 best features and run multiple versions using
several choices for the weight of the L2 regularizer. A simple line
search was used. 

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.12.txt}}
\caption{
Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Selected L2 Regularizers\\
}
\label{figure:Regularize3}
\end{figure}

% including a pdf \begin{figure}[h]
%\begin{figure}[ht]
%\caption{Estimated Generalization Error from L2 Regularization }
%\label{figure:Regularization1}
%\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg1}}
%\end{figure}

Figure \ref{figure:Regularize3} contains the result in a table. We see
the typical pattern: increasing the regularizer weight first decreases
errors and as the weight continues to increase, errors are increased.
Setting the weight to $\lambda = 4$ seems like a good choice: it has the
lowest median across the cross-validation folds and the 95 percent
confidence intervals are not signaling warnings.

The impact of the regularizer on the estimated generalization error is
very small: the median is reduced about \$100 from an unregularized
error of \$57,880, a reduction of less than two-tenths of a percent. The
small impact of the regularizer suggests that there is not much
overfitting in the unregularized model. The reason for that is possibly
that the training sets are not very large. In our approach, we build a
local model for each query and that model had only 60 days of training
data.

\section{Submarket Models}

We considered three very simple definitions of submarkets. In the first,
the submarkets are defined by the census tract. Each census tract is its
own submarket. In the second, submarkets are defined by the city names.
Each city is its own submarket. In the third, submarkets are defined by
zip code. Each five-digit zip code is its own submarket.

We considered two approaches to incorporating submarkets into the model.
The first uses indicator variables, the second builds a separate model
for each submarket.

\subsection{Using Indicator Variables}

The indicator models all have indicators for whether the observation is
in a specific submarket. For example, for zip-code based submarkets, the
model is

% ref: gratzer p.220
\begin{align}
\log(price) & =  \beta_0
            + \beta_1 ZipIs11111 + \beta_2 ZipIs22222 + \ldots \\
            & + \beta_k LotSize + \beta_{k+1} LivingArea + \ldots . 
\end{align}

%% ref: Gratzer page 212 (splitting long formulas)
%\begin{multline}
%\log(price) = \\
%\beta_0\\
%+ \beta_1 ZipIs11111 + \beta_2 ZipIs22222 + \ldots \\
%+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots \\
%.
%\end{multline}

The indicators are capturing the possibly different average prices in
each submarket. The average price per feature is the same across
submarkets.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.indicator}}
\caption{
Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Submarket Indicators
}
\label{figure:Indicator}
\end{figure}

<<Indicators, include=FALSE>>=
no.indicators <- 57781
zip5.indicators <- 39557
1 - (zip5.indicators / no.indicators)
@

Figure \ref{figure:Indicator} shows the estimated generalization errors
from adding indicators variables to the regularized linear model. We see
that adding indicators for zip codes, census tract, and cities all help
and that the best result is obtained with the five-digit zip code
indicators, which has an error rate 32 percent lower than the
entire-market model.

\subsection{Separate Models for Each Submarket}

The separate-models approach builds a separate model for each submarket.
Each model has the same form:

\[
\log(price) =
\beta_0 
+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots
.
\]

A query transaction is presented, its submarket is determined, for
example, it is in zip code 11111, and then a model is trained using only
transactions in the prior 60 days in the zip code. This fitted model is
used to estimate the value for the query transaction. 

This fitting process results in many models that cannot be fit, most
often because there are too few transactions in the training set. 


\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.submarkets}}
\caption{
Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Submarket Models
}
\label{figure:Submarkets13}
\end{figure}

<<Submarkets, include=FALSE>>=
no.indicators <- 57781
property.city <- 44417
1 - (property.city / no.indicators)
@


Figure \ref{figure:Submarkets13} shows the results of this experiment.
The submarket models that used city names had the lowest estimated
generalization error, which was 23 percent lower than the whole-market
error. The fitting process, however, succeeded in all folds in
only 69 percent of the city names. (We say the coverage was 69 percent.)
Coverage was highest for the zip code-based submarkets, but estimated
performance was worse than for city-based submarkets. The census tracts
are the worst of both worlds: the highest expected error and the lowest
coverage.

Coverage numbers could be increased most likely by either retraining the
models on more than 60 days of data or forming submarkets groups by
splicing together some of the submarkets. The latter approach has been
tried in the literature. The former appears to have not been tried.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.examples}}
\caption{
Estimated Generalization Errors\\
For Selected Property City Submarket Models
}
\label{figure:Examples13}
\end{figure}

The low estimated errors in Figure \ref{figure:Submarkets13} are perhaps
deceptive: they are the median values across all the submarkets. Individual
models for specific submarkets may have much lower or higher errors than
the median for all markets.

% median prices for the 10 lowest cities
% lancaster 222K
% palmdale  240
% norwalk   375
% arleta    405
% compton   290
% lapuente  355
% southgate 355
% pomona    335
% valinda   360
% panorama city 394

% median prices for the 10 largest cities
% rolling hills estates 1.2 million
% santa monica          1.3
% palos verdes penninsula ?
% beverly hills           1.4
% toluca lake             1.0
% hermosa beach           1.0
% pacific palisades       1.6
% marina delrey           1.0
% hidden hills            2.1
% rolling hills           1.9
% malibu                  1.8

Let's focus on the city-defined submarkets, which have the lowest median
errors. There are 191 separate cities. The median errors for each city
vary widely. Figure \ref{figure:Examples13} shows the medians for
various cities. We see that the most accurately-estimated houses are
those in Lancaster, where the estimated generalization error is about
\$15,000. A common attribute of the cities with the ten lowest errors is
that they all have relatively low-priced houses. Median house prices for
them from 2003 through 2009 range from \$220,000 to \$405,000.

Turning to the cities with the ten highest estimated errors, we see
cities that are expensive: Santa Monica, Beverly Hills, Malibu. The
median prices in this group of cities ranges from \$1.0 million to \$2.1
million (for Hidden Hills).

The large estimated errors for the more expensive cities may be
generated in part from the higher prices, so that a 10 percent error in
an estimate is a larger amount than for lower-priced cities.  Another
source of the large errors might be that the more expensive properties
sell based on features that are not in our data.


\section{The best linear model is \dots}

Based on the time period from 2003 through the first part of 2009, the
best linear model would predict $\log(price)$ from the features in
natural units, use the 20 features we identified, and incorporate an L2
regularizer.  It would be fitted on 60 days of data and created for each
submarket as defined by the city name. These design choices are in
effect hyperparameters for the linear models.

Commercial model builders should consider a different focus in selecting
the hyperparameters than we have used here.  Rather than select them
once for the entire time period and for all submarkets, they should
select hyperparameters for each submarket based on recent
transactions instead of the longer time period studied here: we are
interested in models that work on average, most commercial price
estimators are interested in models that work on tomorrow's
transactions.

\section{Coda: A Non-Linear Model, Random Forests}

Every study that we reviewed that compares linear to non-linear models
claims that non-linear models generally outperform linear models when
predicting house prices. We verify that claim in our setting by
developing a number of random forest models by sweeping these design
choices:

\begin{itemize}

\item The feature set used: we testing both all 24 features and the 20
features that were best for the linear model. 

\item The number of trees in the random forest (\code{ntree}): more
trees should lead to better results, probably with a tailing off in
performance at some point. We tested 1, 100, 300, and 1000 trees in each
random forest.

\item The number of features to try when adding a new leaf
to the tree (\code{mtry}). The implementation of random forests first
tests that number of random-selected features to add and then adds the
one giving the best greedy performance. We test 1, 2, 3, and 4 features.

\end{itemize}


\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.14.rf.1}}
\caption{
Estimated Generalization Errors\\
For Random Forests\\
For Selected Hyperparameters \code{ntree} and \code{mtry}\\
For All Features Except Assessment Features and the Best 15 Features from the Linear Models\\
Trained for 30 Days
}
\label{figure:RF1}
\end{figure}

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.14.rf.2}}
\caption{
Estimated Generalization Errors\\
For Random Forests\\
For Selected Hyperparameters \code{ntree} and \code{mtry}\\
For All Features Except Assessment Features\\
Trained for 60 Days
}
\label{figure:RF2}
\end{figure}

TODO: REWRITE FROM HERE ON.

Figure \ref{figure:RF1} shows the results of this experiment.
In Panel A and B, increasing \code{mtry} always decreases the error, and
increasing \code{ntree} usually decreases the error.  We conclude that
there is some tuning to be done for the hyperparameters.

Comparing Panel A and Panel B, we see that the error is always
lower when all the predictors are used. Apparently the random forests
have found a way to use the 4 predictors that degraded the performance
of the linear model.

<<random>>=
best.lin <- 39557
rf       <- 35782
ratio    <- best.lin / rf
ratio
@

Finally the best estimated error is in Panel B for $ntree = 1000$ and
$mtry = 4$. This error, \$35,782, is lower than the error from the best
entire-market model (the model that used the zip-code indicators), which
had an error of \$39,557. The linear model has errors that are about 10
percent higher than the random forests model.


<<Improvements, include=FALSE>>=
linear.best20 <- 57781
rf.best20     <- 48335
linear.zip.indicators <- 39557
rf.all <- 35782
1 - (rf.best20 / linear.best20)
1 - (linear.zip.indicators / rf.best20)
1 - (rf.all / linear.zip.indicators)
1 - (rf.all / linear.best20)
1 - (rf.all / rf.best20)
@
\begin{figure}[ht]
\begin{tabular}{| l | l | l | r | r | }
\hline
&Model              & features                          & error &
decrease \\
\hline
1 & Linear regularized & best 20, no zip-code indicators   & 57781 & NA \\
2 & Random forests     & best 20, no zip-code indicators   & 48335 & 16\%\\
3 & Linear regularized & best 20, with zip-code indicators & 39557 & 18\%\\
4 & Random forests     & all, no zip-code indicators       & 35782 & 10\% \\
\hline
\end{tabular}
\caption{
Estimated Generalization Error\\
For Selected Entire-Market Models\\
For Selected Feature Sets
}
\label{figure:CompareModels}
\end{figure}

We find that the random forests model with all features is the best
performing. Figure \ref{figure:CompareModels} contains the results of a
change-by-change analysis. It starts in row 1 with the best linear
whole-market model. This model used the 20 features found to be best, was
regularized, and was trained on 60 days of data.  Line 2 contains the
results from an equivalent random forests model. It uses the same 20
features and was also trained on 60 days of data. It has a 16 percent
lower error. The random forests model is making better use of the data. 

Continuing to line 3, we find the same linear model as in
line 1, but this time the features include the five-digit zip code
indicators. This model has an 18 percent lower error than the random
forests model in line 2. Line 4 contains the results from a whole-market
random forests model that uses all 24 features and was trained on 60
days of data. It improves on the linear submarket model by 10 percent.

Finally, the addition of the four features that increased the error rate
in the linear model improved the error rate in the random forests model
from \$48,335 to \$35,782, a reduction of 26 percent. That's a large
gain for very little work from the model builder.

Possibly both the linear and random forests models can be improved.
Either may be able to use productively additional relevant feature,
including the GPS coordinates, perhaps directly and perhaps as a trend
surface. More hyperparameters choices for the random forests model could
be explored.  A submarket model using random forests could be built.
Additionally, rather than using the 60-day training period found best
for the linear models, the best training period for random forests could
be determined. Finally, the best model form for random forests is not
necessarily the log-level form tested.


Figure \ref{figure:RF2} SHOWS.

