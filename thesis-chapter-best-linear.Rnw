\chapter{Finding the Best Linear Model}
% vim: textwidth=72
% vim: foldmethod=manual

<<BestLinearModel>>=
working <- control$working
path.cv.criterion <- paste0(working, 'e-cv-chart_chart7.txt')
path.cv.form.ndays <- paste0(working, 'e-cv-chart_chart8.txt')
path.lcv2 <- paste0(working, 'e-features-lcv2-chart_1.txt')
path.9.txt <- paste0(working, 'e-cv-chart_chart9.txt')
path.9.gg1 <- paste0(working, 'e-cv-chart_chart9_1.pdf')
path.9.gg2 <- paste0(working, 'e-cv-chart_chart9_2.pdf')
path.pca.1 <- paste0(working, 'e-features-pca-chart_1.txt')
path.pca.2.01 <- paste0(working, 'e-features-pca-chart_2_01.txt')
path.pca.2.02 <- paste0(working, 'e-features-pca-chart_2_02.txt')
path.pca.2.03 <- paste0(working, 'e-features-pca-chart_2_03.txt')
path.10.txt <- paste0(working, 'e-cv-chart_chart10.txt')
path.10.gg1 <- paste0(working, 'e-cv-chart_chart10_1.pdf')
path.10.gg2 <- paste0(working, 'e-cv-chart_chart10_2.pdf')
path.11.gg1 <- paste0(working, 'e-cv-chart_chart11_1.pdf')
path.11.gg2 <- paste0(working, 'e-cv-chart_chart11_2.pdf')
path.12.gg1 <- paste0(working, 'e-cv-chart_chart12_1.pdf')
path.12.gg2 <- paste0(working, 'e-cv-chart_chart12_2.pdf')
path.12.txt <- paste0(working, 'e-cv-chart_chart12_3.txt')
path.13.indicator  <- paste0(working, 'e-cv-chart_chart13_indicators.txt')
path.13.submarkets <- paste0(working, 'e-cv-chart_chart13_submarkets_summary.txt')
path.13.examples   <- paste0(working
                             ,'e-cv-chart_chart13_submarkets_examples_property_city.txt')
path.14.rf <- paste0(working, 'e-cv-chart_chart14b.txt')
@ 

Linear models predict the price as a linear function of features:

\[ price = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n. \]

From this simple idea, a huge variety of models can be generated. Which
linear models are most accurate for predicting the prices of houses?

We answer this question by examining data from 2003 and later using the
decennial census from the year 2000 but not the tax assessment for 2008.
Our interest is in estimating the generalization error for unseen
transactions. For this, we use 10-fold cross validation.

Property descriptions come from the tax roll for the year 2008. The
tax roll contains property descriptions as of December 31, 2007. We
need to exclude from our data any houses built or modified after that
date. An analysis shows that there are no such houses.

The plan of attack is reflected in the sections that follow.

\begin{itemize}

\item Section 1 explains our choice of metric to compare models. We
show that using different metrics leads to judging that different
models are better. We decide to focus on median errors.

\item Section 2 studies design choices for linear models that cover the
entire market.  We start by considering model forms and the number of
training days that lead to the most accurate models. (The term ``model
form'' captures whether transformations into the log domain are used in
the left hand and right hand sides of the model. More on this later.)
Having fixed form and number of training days, we then consider which
features are best and compare heuristics for selecting the best
features.  Finally, fixing form, number of  training days, and feature
sets, we show that regularization improves estimation accuracy.

\item Section 3 considers submarket models. Submarket models partition
the possibly queries and use the partitions in the fitting and
prediction processes. We consider both indicator-based submarket models,
in which the submarkets are represented by indicator variables in a
single model for all queries, and separate submarket models, in which a
separate model is build for each submarket. We consider multiple
definitions for submarkets. We find that submarket models outperform
entire-market models.

\item Section 4 summarizes our results for linear models and provides
some guidelines for extending the work into a commercial setting.

\item Section 5 contrasts the results from the linear models with a
non-linear model: the random forests model. We show that a
random forests model outperforms our best linear model.

\end{itemize}

\section{Figure of Merit}

The literature often compares models using the fraction of estimates
within 10 percent of the known values. The motivation for this figure of
merit may in part be that Freddie Mac is said \cite[footnote on
p.~642]{fik-03-spatial} to use that metric to judge the effectiveness of
automated valuation models. In any case, using fraction within 10
percent (along with fraction within 20 and 30 percent) is popular in the
real estate pricing literature.

It is a curious choice of metric for at least two reasons. The first
reason is that linear models are often trained to minimize the mean
squared error (MSE). So fraction within 10 percent isn't the criteria
the model was trained to deliver. The second reason is that selecting
fraction within $x$ percent is implicitly a decision to favor models
that are close when accurate and to not minimize the average error.

Here's a contrived example which illustrates that point. There
are two properties both of which sold for \$100,000. Suppose we have two
models to consider. Model A produces estimates of  \$100,000 and
\$200,000.  It is either perfect or very wrong. Model B produces
estimates of \$85,000 and \$115,000. It is never super accurate but
never very wrong.  Model A has 50 percent of its estimates within 10
percent of the true value, Model B has 0 percent of its estimates within
10 percent of the true value. So under the fraction-with-$x$-percent
metric, we prefer model A over model B. 

Now imagine using model A. When it delivers its estimate, we have no
idea whether the estimate is good or bad.  To me, this situation
is untenable if there is an acceptable alternative (and there is: use
MSE). To make the untenable situation acceptable, one would need to
produce both an estimate and a confidence in the estimate.  Now the
model has to be trained to deliver two values, and we have conceivably
more than doubled the difficulty of the problem of selecting the best
model: there are now two figures of merit and how to make tradeoffs
between them is not obvious.

The issue of choice of figure of merit would be moot if using either
metric resulted in selecting the same model as best. To investigate
whether the issue is moot, we designed an experiment. In the experiment,
we consider the estimated generalization error from a variety of models.
The models were generated by sweeping these design choices:

\begin{itemize}

\item Response variable: either the $price$ or $\log(price)$

\item The form of the predictors: either level (natural units) or log of
natural units

\item The number of days in the training period, from 30 up to 360 in
steps of 30 days

\item The figure of merit used to select the model: one of the mean of
the root mean squared errors from the folds (\code{meanRMSE} in the
figure), the median of the root median squared errors from the folds
(\code{medRMSE}), or the fraction within 10 percent of actual values
(\code{fctWI10}).

\end{itemize}

All of these models used all features always present except for those
derived from the tax assessor data. All models used data from 2003 on.
To keep running times reasonable, a random one percent sample of possible
test transactions (queries) was drawn from each cross-validation fold.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.criterion}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Measurement Approaches Compared }
\label{figure:merit1}
\end{figure}

The results are depicted in Figure \ref{figure:merit1}.

To read the table, consider the first three rows. All of these are for
the level-level form of the linear model: we predict price using the
natural units for each feature. The three rows show the value of a model
selection metric for 30, 60, and more days using the three metrics
studied. The columns with headings 30 up to 360 are the number of days
in the training period for each query.  An asterisk precedes the value
that optimizes a row: it marks the best number of days.

There are several observations to make:

\begin{itemize}

\item Only for the log-log form do the three metrics pick the same
number of training days as best.

\item When the metrics pick different training periods, they usually pick
similar training periods, but sometimes pick very different training
periods. The biggest difference in training periods is for the level-log
form.

\item It's often true that the metric \code{meanRMSE} and
\code{medRMSE} select a similar number of training days. The number of
training days is either identical or differs by, at most, 60 days. These
metrics have similar choices for models selected.

\item It's always true that the metric \code{meanRMSE} greatly exceeds
\code{medRMSE}. This is because there is right-skew in the prices: the
mean exceeds the median, so that the mean error can greatly exceed the
median error.

\end{itemize}

We conclude that choice of metric does influence choice of the training
period, particularly if one will use a level-log model. For the
log-level model, which is most popular in the literature, the choice of
metric to select a model is not very determining: one should pick the
metric based on some other criteria.

Going forward, we focus on the \code{medRMSE} metric, judging that it
more intuitively sizes average errors than does \code{meanRMSE} (because
of the right-skew in the prices) and reflecting our predisposition to
minimize average errors rather than the tightness of estimates.


\section{Entire-market Models}

This section explores linear models that are specified once for the
entire marketplace: there is one model for every query transaction and
this model is location aware only through features of the census tract
and zip code of the property. I call theses kinds of models
``entire-market models.''

Within the class of entire-market models, we explore design choices. In
the first subsection that follows, we examine the form of the equation
in the model and the training period used to fit the model. We find that
training using 60 days of data before the query transaction is best. The
best form of model to use is log-level, so that one would predict the
log of price using the features in natural units. We then examine,
within this choice of training period and form,  what features give the
best predictive performance and examine a reduced-feature model. We show
that a 20-feature model performs best. Finally, using the 20 feature
model trained on 60 days of data in log-level form,  we show that
regularizing the linear model with an L2 regularizer improves
performance.

\subsection{Model form and number of training days}

In the literature, a popular model for is the log-level form: one
predicts $\log(price)$ using features in their natural units. Other
choices for model form include log-log, in which the log of price is
predicted using features that are in the log domain, level-level in
which price is predicted using features in the natural units, and
level-log in which price is predicted using features in the log domain.

At least three rationales can be invoked to choose a model. First is
that the form of the model should conform to one's prior assumptions
about the data generation process. If you suppose that doubling the lot
size might double the price, you can admit this possibility by
specifying $\log(price) = \beta \ log(size)$, a log-log form. If you
suppose that adding a bedroom adds a certain amount of value, you would
want to have $price = \beta \ bedrooms$, a level-level form. 

A second rationale for choosing the form is to transform the prices to a
form more compatible with the statistical assumptions of the model.
Since real estate prices tend to be right-skewed, a log transformation
would probably make the transformed distribution more normal.

A third rationale is to transform the price to the log domain in order
to avoid applying equal importance with respect to prediction accuracy
to expensive and less expensive houses. By prediction $\log(price)$
instead of $price$, the idea  is to fit the model to minimize errors on
the more typical less expensive houses, not to minimize the
price-weighted error, which could be regarded as overly influenced by a
few very large houses.

% OMIT, SINCE WE DIDN"T TEST THESE log-mixed form
%Sometimes these rationale's are combined. For example, in
%\cite{chopra-09}, prices were transformed to the log domain in order to
%not overweight the importance of accuracy for larger houses, and size
%feature (such as lot size and interior space) were transformed into the
%log domain to allow for the possibility that doubling them would double
%the price.

Here we focus on the third rationale and simply ask: Which model form
yields the lowest estimated generalization errors as measured by the
median of the root median squared error across folds? 

Another issue is to determine the best training period. The linear
models are setting the marginal price of each feature. If prices are
moving slowly, then training on a longer period of time might be
beneficial.  If prices are moving rapidly, then training only on recent
data might be necessary.

Our experiment fixed the time period (2003 on) and the feature set
(all features always in transactions excluding the features derived from
the tax assessment). We vary the model form and number of training days.
Because a local model is built for every query point, we take a one
percent random sample of the possible query points in each fold in order
to reduce the computational burden.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.form.ndays}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Across Model Forms and Training
Period}
\label{figure:EGEFormNdays}
\end{figure}

Figure \ref{figure:EGEFormNdays} shows the estimated generalization
errors from 10-fold cross validation. Comparing the first two rows, we
see that when price is the response variable, a lower error is obtained
by working with features in natural units than in the log domain.
Comparing all the rows, we see that estimating $\log(price)$ instead of
$price$ always leads to reduced estimated generalization errors.
Comparing rows three and four shows that for most training periods,
using a log-level model outperforms a log-log model. Finally, the lowest
estimated generalization error is for the log-level model when trained
for 60 days.

In our search for the best linear model, going forward we fix the number
of training days to 60 and use the log-level form for the model.


\subsection{Feature selection}

The benchmark model has 24 features. Which of these are the best
features to use? Here we consider two approaches. In the first, we
use a simple heuristic to determine the best features to use. In the
second, we use Principal Components Analysis (PCA) for the same purpose.
We then compare results and have one surprising finding.

\subsubsection{LCV Heuristic}

We have 24 features that are candidates for inclusion in the model. A
straight-forward way to determine the best feature set is to consider
all possible subsets and estimate the generalization error for each.
Doing so would be way too time consuming.

Our idea is to instead use L1 regularization to determine the
rank-ordering of the 24 features in terms of their importance. We then
use cross validation to estimate the generalization error from a model
with the first, the first two, \dots, all 24 rank-ordered features. We
call the procedure LCV, for L1 regularization followed by cross
validation.

The implementation is by using the Elastic Net from Zoe and Hastie
\cite{zou-05}. It fits a linear model while concurrently using an L1
and L2 regularizer. As a by-product, an R implementation, the
\code{elasticnet} R package \cite{zou-12}, produces a rank-ordering of
the features in terms of their importance. In our use of this package,
we set the L2 regularizer to zero and use just the rank-ordering of
features under the L1 regularizer.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.9.txt}}
\caption{Median Errors For Features Sets: Table}
\label{figure:MedianTable}
\end{figure}

The result of the rank-ordering from the Elastic Net procedure is in
Figure \ref{figure:MedianTable}. It shows that the most important
feature is the living area of the house. Real estate agents often claim
the most important feature of the house is its location, but that isn't
true in these data. Instead, features of the location (median household
income and average commute time) are second and third in importance. 

% including a pdf
\begin{figure}[h]
\caption{Median Errors for Feature Sets: Graphic}
\label{figure:MedianGraph1}
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg1}}
\end{figure}

The same data are shown graphically in Figure \ref{figure:MedianGraph1},
which displays the estimated generalization error from including the
first $n$ features, together with a 95 percent confidence interval. One
sees that the lowest estimated error is when using the first 20
features, from living-area to census-tract-has-park. The estimated
errors then tend to increase and the placement of the confidence
intervals does not compel one to consider going beyond the first 20
features. 

If one seeks parsimony, one could consider using the first eight
features (through zip5-has-industry), after which there is an increase
in estimated generalization error before a subsequent decline. Another
possible feature set is the first 20 features (through
census-tract-has-park), as that feature set has the lowest estimated
error. Another choice would be 19 features (through
factor-is-new-construction) as though it would have a slightly higher
median estimated generalization error than for the 20-feature choice,
the 95 percent confidence interval is smaller.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.lcv2}}
\caption{Most Important Features}
\label{figure:LCV}
\end{figure}

Figure \ref{figure:LCV} shows the 24 rank-ordered features split into
house features and location features. Some observations on the house
features:

\begin{itemize}

\item living-area: this is the interior living space. Construction is
required to change it, so it is not surprising to see it highly-ranked.
That is is more highly ranked than the land square footage is a surprise
to me, because land square footage seems harder to change than interior
living space.

\item fireplace-number: the number of fireplaces, including the
possibility of zero. A surprise---perhaps fireplaces are proxies for
overall amenities in the house.

\item year-built: the year the house was built. Other studies have
concluded that the age and the squared age of the house are important
features. Our models are local, so that we need a house's age for every
query transaction.  Instead of pre-building a training set for each
possible query, we transform year built into age and age squared just
before fitting the model. We perform a similar transformation for the
effective year built, which is the last year of significant updates to
the house.

\item factor-has-pool: whether there is a swimming pool. Not a
surprise, as the houses are all in Los Angeles, where the weather is
often pleasant and a swimming pool might be a joy.

\end{itemize} 

Some observations on the location features:

\begin{itemize}

\item median-household-income and fraction-owner-occupied: the median
income in the census tract from the year 2000 census and the fraction of
houses in the census tract that are occupied by owners. Both of these
features indicate the wealth of the neighborhood.

\item average-commute-time: in other studies, longer commutes are
associated with lower prices.

\item zip5-has-industry and census-tract-has-industry: being near
industry is not good for house prices.

\item census-tract-has-park and zip5-has-park: Los Angeles residents are
not there for the parks. Perhaps this is because its a driving-around
city, not a walking-around city.

\end{itemize}

%
%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph2}
%\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
%\end{figure}
%

\subsubsection{PCA}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.1}}
\caption{Principal Components Cumulative Variance}
\label{figure:PCA1}
\end{figure}

The second approach we used to determining the importance of features is 
Principal Components Analysis. Figure \ref{figure:PCA1} shows the
principal components associated with the training set predictors. We see
that the first three principal components account for almost 100 percent
of the variance.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.01}}
\caption{Principal Component 1}
\label{figure:PCA201}
\end{figure}

In Figure \ref{figure:PCA201} we see the weights of the 24 features in
the first principal component. The median household income has the
highest absolute weight. Other weights are much lower in absolute value.
The only ones of much importance are the land square footage and the
living area. It seems that the first principal component may be about
the wealth of the neighborhood.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.02}}
\caption{Principal Component 2}
\label{figure:PCA202}
\end{figure}

In Figure \ref{figure:PCA202} we see the weights of the 24 features in
the second principal component. The land square footage has the highest
absolute weight. Other weights are much lower in value with household
income and living area being the largest in absolute size. This
principal component is possibly about the size of the land.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.03}}
\caption{Principal Component 3}
\label{figure:PCA203}
\end{figure}

In Figure \ref{figure:PCA203} we see the weights of the 24 features
for third principal component. It is mostly composed of the basement
square feet, perhaps a proxy for other size features in the house:
bigger basement probably implies bigger house. This principal component
is possibly about the size of the house.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.10.txt}}
\caption{Median Errors Across Folds: Table}
\label{figure:MedianTable10}
\end{figure}

% including a pdf
\begin{figure}[h]
\caption{Median Errors Across Folds: Graphic}
\label{figure:MedianGraph102}
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg1}}
\end{figure}


In Figure \ref{figure:MedianTable10} we see the estimated generalization
error from using the median household income, land square footage, and
so on up to the 4 features identified as important in the PCA work. The
same data are in Figure \ref{figure:MedianGraph102} which shows the 95
percent confidence intervals graphically. If you prefer sparsity, you
might use the first 3 features. If you prefer the lowest error, you
might choose the first 4 features.


\subsubsection{Reduced Feature Models} %%%%%%%%%%%%%%%%%%%%%%%%

% including a pdf
\begin{figure}[h]
\caption{Median Errors Across Folds}
\label{figure:MedianGraph111}
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg1}}
\end{figure}

To find the best reduced-feature model, we test all the feature sets
suggested by the LCV and PCA analyses. These results are in Figure
\ref{figure:MedianGraph111}, which brings together results previously
shown. A surprising conclusion is that the PCA-derived feature sets all
underperform the LCV-derived feature sets: extremely small feature sets
are challenged in this setting. The best feature set is the one with the
first 20 features.

\subsection{Regularization}    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regularizing models is one way to avoid overfitting and thereby improve
prediction accuracy. So far the models are not regularized.

To assess the extent to which regularization leads to more accuracy, we
take the model using the 20 best features and run multiple versions using
several choices for the weight of an L2 regularizer. A simple line
search was used. 

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.12.txt}}
\caption{Estimated Generalization Error from L2 Regularization}
\label{figure:Regularize3}
\end{figure}

% including a pdf \begin{figure}[h]
%\begin{figure}[ht]
%\caption{Estimated Generalization Error from L2 Regularization }
%\label{figure:Regularization1}
%\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg1}}
%\end{figure}

Figure \ref{figure:Regularize3} contains the result in a table. We see
the typical pattern: increasing the regularizer weight first decreases
errors and as the weight continues to increase, errors are increased.
Setting the weight to $\lambda = 4$ seems like a good choice: it has the
lowest median across the cross-validation folds and the 95 percent
confidence intervals are not signaling warnings.

The impact of the regularizer on the estimated generalization error is
very small: the median is reduced about \$100 from an unregularized
error of \$57,880, a reduction of less than two-tenths of a percent. The
small impact of the regularizer suggests that there is not much
overfitting in the unregularized model. The reason for that is possibly
that the training sets are not very large. In our approach, we build a
local model for each query and that model had only 60 days of training
data.

\section{Submarket Models}

We considered three very simple definitions of submarkets. In the first,
the submarkets are defined by the census tract. Each census tract is its
own submarket. In the second, submarkets are defined by the city names.
Each city is its own submarket. In the third, submarkets are defined by
zip code. Each 5-digit zip code is its own submarket.

We considered two approaches to incorporating submarkets into the model.
The first uses indicator variables, the second builds a separate model
for each submarket.

\subsection{Using Indicator Variables}

The indicator models all have indicators for whether the observation is
in a specific submarket. For example, for zip-code based submarkets, the
model is

% ref: gratzer p.220
\begin{align}
\log(price) & =  \beta_0
            + \beta_1 ZipIs11111 + \beta_2 ZipIs22222 + \ldots \\
            & + \beta_k LotSize + \beta_{k+1} LivingArea + \ldots . 
\end{align}

%% ref: Gratzer page 212 (splitting long formulas)
%\begin{multline}
%\log(price) = \\
%\beta_0\\
%+ \beta_1 ZipIs11111 + \beta_2 ZipIs22222 + \ldots \\
%+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots \\
%.
%\end{multline}

The indicators are capturing the possibly different average prices in
each submarket. The average price per feature is the same across
submarkets.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.indicator}}
\caption{Estimated Generalization Error Using Submarket Indicators}
\label{figure:Indicator}
\end{figure}

Figure \ref{figure:Indicator} shows the estimated generalization errors
from adding indicators variables to the regularized linear model. We see
that adding indicators for zip codes, census tract, and cities all help
and that the best result is obtained with the five-digit zip code
indicators.

\subsection{Separate Models for Each Submarket}

The separate-models approach builds a separate model for each submarket.
Each model has the same form:

\[
\log(price) =
\beta_0 
+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots
.
\]

A query transaction is presented, its submarket is determined, for
example, it is in zip code 11111, and then a model is trained using only
transactions in the prior 60 days in the zip code. This fitted model is
used to estimate the value for the query transaction. 

This fitting process results in many models that cannot be fit, most
often because there are too few transactions in the training set. 


\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.submarkets}}
\caption{Estimated Generalization Error Using Submarket Models}
\label{figure:Submarkets13}
\end{figure}


Figure \ref{figure:Submarkets13} shows the results of this experiment.
The submarket models that used city names had the lowest estimated
generalization error, but the fitting process succeeded in all folds in
only 69 percent of the city names. (We say the coverage was 69 percent.)
Coverage was highest for the zip code-based submarkets, but estimated
performance was worse than for city-based submarkets. The census tracts
are the worst of both worlds: the highest expected error and the lowest
coverage.

Coverage numbers could be increased most likely by either retraining the
models on more than 60 days of data or forming submarkets groups by
splicing together some of the submarkets. The latter approach has been
tried in the literature. The former appears to have not been tried.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.examples}}
\caption{Estimated Generalization Error Using Submarket Models}
\label{figure:Examples13}
\end{figure}

The low estimated errors in Figure \ref{figure:Submarkets13} are perhaps
deceptive: they are the median values across all the submarkets. Individual
models for specific submarkets may have much lower or higher errors than
the median for all markets.

% median prices for the 10 lowest cities
% lancaster 222K
% palmdale  240
% norwalk   375
% arleta    405
% compton   290
% lapuente  355
% southgate 355
% pomona    335
% valinda   360
% panorama city 394

% median prices for the 10 largest cities
% rolling hills estates 1.2 million
% santa monica          1.3
% palos verdes penninsula ?
% beverly hills           1.4
% toluca lake             1.0
% hermosa beach           1.0
% pacific palisades       1.6
% marina delrey           1.0
% hidden hills            2.1
% rolling hills           1.9
% malibu                  1.8

Let's focus on the city-defined submarkets, which have the lowest median
errors. There are 191 separate cities. The median errors for each city
vary widely. Figure \ref{figure:Examples13} shows the the medians for
various cities. We see that the most accurately-estimated houses are
those in Lancaster, where the estimated generalization error is about
\$15,000. A common attribute of the cities with the ten lowest errors is
that they all have relatively low-priced houses. Median house prices for
them from 2003 through 2009 range from \$220,000 to \$405,000.

Turning to the cities with the ten highest estimated errors, we see
cities that are expensive: Santa Monica, Beverly Hills, Malibu. The
median prices in this cities ranges from \$1.0 million to \$2.1 million
(for Hidden Hills).

The large estimated errors for the more expensive cities may be generated in part from
the higher prices, so that a 10 percent error in an estimate is a larger
number than for lower-priced cities.  Another source of the large errors
might be that the more expensive properties sell based on features that
are not in our data.


\section{The best linear model is \dots}

Based on time period from 2003 through the first part of 2009, the best
linear model would predict $\log(price)$ from the features in natural
units, use the 20 features we identified, be regularized, be fitted on
60 days of data, and created for each submarket as defined by the city
name. These design choices are in effect hyperparameters for the linear
models.

Commercial model builders should consider being more aggressive in
selecting the hyperparameters than we have been here. Rather than select
them once for the entire time period and for all submarkets, they should
select hyperparameters for each submarket based on very recent
transactions instead of the longer time period studied here: we are
interested in models that work on average, most commercial price
estimators are interested in models that work on tomorrow's
transactions.

\section{Coda: A Non-Linear Model, Random Forests}

Every study that we reviewed that compares linear to non-linear models
claims that non-linear models generally outperform linear models when
predicting house prices. We verify that claim in our setting by
developing a number of random forest models by sweeping these design
choices:

\begin{itemize}
\item The feature set used: we testing both all features and the 20
features that were best for the linear model. 
\item The number of trees in the random forest: more trees should lead to
better results, probably with a tailing off in performance at some
point. We tested 1, 100, 300, and 1000 trees in each random forest.
\item The number of features to try when adding a new leaf
to the tree. The implementation of random forests first tests that
number of random-selected features to add and then adds the one giving
the best greedy performance. We test 1, 2, 3, and 4 features.
\end{itemize}


\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.14.rf}}
\caption{Estimated Generalization Error Using Random Forests}
\label{figure:Submarkets}
\end{figure}

Figure \ref{figure:Submarkets} shows there results of this experiment.
In Panel A and B, increasing \code{mtry} always decreases the error, and
increasing \code{ntree} usually decreases the error.  We conclude that
there is some tuning to be done for the hyperparameters.

Comparing Panel A and Panel B, we see that the error is always
lower when all the predictors are used. Apparently the random forests
have found a way to use the 4 predictors that degraded the performance
of the linear model.

<<random>>=
best.lin <- 39557
rf       <- 35782
ratio    <- best.lin / rf
ratio
@

Finally the best estimated error is in Panel B for $ntree = 1000$ and
$mtry = 4$. This error, \$35,782, is lower than the error from the best
entire-market model (the model that used the zip-code indicators), which
had an error of \$39,557. The linear model is about 10 percent worse
than the random forests model.

%Both can be improved. The linear model can be improved through including
%the GPS coordinates and probably through many other means. How do design
%these improvements is not obvious. The random forests model can be
%improved simply by increasing the two hyperparameters, by adding
%features.

