\chapter{Finding the Best Linear Model}
% vim: textwidth=72
% vim: foldmethod=manual

<<BestLinearModel>>=
working <- control$working
path.cv.criterion <- paste0(working, 'e-cv-chart_chart7.txt')
path.cv.form.ndays <- paste0(working, 'e-cv-chart_chart8.txt')
path.lcv2 <- paste0(working, 'e-features-lcv2-chart_1.txt')
path.9.txt <- paste0(working, 'e-cv-chart_chart9.txt')
path.9.gg1 <- paste0(working, 'e-cv-chart_chart9_1.pdf')
path.9.gg2 <- paste0(working, 'e-cv-chart_chart9_2.pdf')
path.pca.1 <- paste0(working, 'e-features-pca-chart_1.txt')
path.pca.2.01 <- paste0(working, 'e-features-pca-chart_2_01.txt')
path.pca.2.02 <- paste0(working, 'e-features-pca-chart_2_02.txt')
path.pca.2.03 <- paste0(working, 'e-features-pca-chart_2_03.txt')
path.10.txt <- paste0(working, 'e-cv-chart_chart10.txt')
path.10.gg1 <- paste0(working, 'e-cv-chart_chart10_1.pdf')
path.10.gg2 <- paste0(working, 'e-cv-chart_chart10_2.pdf')
path.11.gg1 <- paste0(working, 'e-cv-chart_chart11_1.pdf')
path.11.gg2 <- paste0(working, 'e-cv-chart_chart11_2.pdf')
path.12.gg1 <- paste0(working, 'e-cv-chart_chart12_1.pdf')
path.12.gg2 <- paste0(working, 'e-cv-chart_chart12_2.pdf')
path.12.txt <- paste0(working, 'e-cv-chart_chart12_3.txt')
@ 

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n. \]

From this simple idea, a huge variety of models can be generated. Which
linear models are most accurate for predicting the prices of houses?

We answer this question by examining data from 2003 and later using the
decennial census from the year 2000 but not the tax assessment for 2008.
Our interest is in estimating the generalization error for unseen
transactions. For this, we use 10-fold cross validation on the training
portion of the subset1 data.

Property descriptions come from the taxroll for the year 2008. The
taxroll contains property descriptions as of December 31, 2007. We
need to exclude from out data any houses built or modified after that
date. An analysis shows that there are no such houses.

The plan of attack is EXPLAIN THE SECTIONS.


Explain the scope = entire market models. There is one model for every
house to be estimated.

Explain local models

\section{Figure of Merit}

The literature often compares models using the fraction of estimates
within 10 percent of the known values in training data. The motivation
for this figure of merit is in part that Freddie Mac is said
(\cite{fik-03-spatial} footnote on page 642) to use metric to judge the
effectiveness of automated valuation models. In any case, using fraction
within 10 percent (along with fractions with 20 and 30 percent) is
popular in the real estate pricing literature.

It is a curious choice of metric for at least two reasons. The first
reason is that linear models are often trained to minimize the mean
squared error (MSE). So fraction within 10 percent isn't the criteria
the model was trained to deliver. The second reason is that selecting
fraction within $x$ percent is implicitly a decision to favor models
that are close when accurate and potentially very far from the true
prices when not accurate. 

Here's a contrived example that illustrates that point: suppose there
are two properties both of which sold for \$100,000. Suppose we have two
models to consider. Model A produces estimates of  \$100,000 and \$200,000:
it is either perfect or very wrong. Model B produces estimates of
\$85,000 and \$115,000: it's never super accurate but never very wrong.
Model A has 50 percent of its estimates within 10 percent of the true
value, Model B has 0 percent of its estimates within 10 percent of the
true value. So under the fraction-with-$x$-percent metric, we prefer
model A over model B. 

Now imagine using model A. When it delivers its estimate, we have no
idea whether the estimate is good or bad.  To me, this situation
is untenable if there is an acceptable alternative (and there is: use
MSE). To make the untenable situation acceptable, one would need to
produce both an estimate and a confidence in the estimate.  Now the
model has to be trained to deliver two values, and we have conceivable
more than doubled the difficulty of the problem of selecting the best
model: there are now two figures of merit and how to make radeoffs
between them is not obvious.

The issue of choice of figure of merit would be moot if using either
metric resulting in selecting the same model as best. To investigate
this, we designed an experiment. In the experiment, we consider estimate
the generalization error from a variety of model. The models were
generated by sweeping these design choices:

\begin{itemize}
\item Response variable: either the price or log(price)
\item The form of the predictors: either level (natural units) or log of
natural units
\item The number of days in the training period, from 30 up to 360.
\end{itemize}

All of these models used all features always present except for those
derived from the tax assessor data. All models used data from 2003 on.
To keep running times resonable, a random one percent sample of possible
test transactions (queries) was drawn from each cross-validation fold.

The results are depicted in Figure \ref{figure:merit1}.

To read the table, consider the first two rows and the column with 150
for the number of days. The first line (price, level, medRMSE) has the
median of the root median squared errors from the 10 folds. Just below
it (price, level, fctWI10) is reported the fraction of values that are
within 10 percent of true values using the model trained to minimize
medRMSE.

Reading across the first pair of rows, we see that the model that
minimizes that medRMSE would be trained on 60 days of data and that the
model that minimized the fraction of observations within 10 percent
(fctWI10) would be trained 360 days. These training periods are very
different.

Likewise, comparing other pairs of rows shows that the model selected as
best by the two figures of merits always differ by at least 120 days of
training data.

Clearly there is a choice to be made for which figure of merit to use in
actual models. In the experiments that follow, we use the medRMSE figure
of merit to choose among models.

One other observation: considering just the medRMSE values, the lowest
estimated generalization error occurs when estimating $log(price)$ using
features in their natural units and 60 days of training data. This is
our base model going foward.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.criterion}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Measurement Approaches Compared }
\label{figure:merit1}
\end{figure}

\section{Entire-market Models}

\subsection{Model form and ndays}

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.form.ndays}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Across Model Forms and Training
Period}
\label{figure:EGEFormNdays}
\end{figure}

Figure \ref{figure:EGEFormNDays}

\subsection{Feature selection}

\subsubsection{LCV Heuristic}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.lcv2}}
\caption{Most Important Features}
\label{figure:LCV}
\end{figure}

Figure \ref{figure:LCV}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.9.txt}}
\caption{Median rMedianSE Table}
\label{figure:MedianTable}
\end{figure}

Figure \ref{figure:MedianTable}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph1}
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg1}}
\end{figure}

Figure \ref{figure:MedianGraph1}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Not Showing Origin
}
\label{figure:MedianGraph2}
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
\end{figure}

Figure \ref{figure:MedianGraph2}

\subsubsection{PCA}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.1}}
\caption{Principal Components\\Cumulative Variance}
\label{figure:PCA1}
\end{figure}

Figure \ref{figure:PCA1}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.01}}
\caption{Principal Components\\Component 1}
\label{figure:PCA201}
\end{figure}

Figure \ref{figure:PCA201}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.02}}
\caption{Principal Components\\Component 2}
\label{figure:PCA202}
\end{figure}

Figure \ref{figure:PCA202}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.03}}
\caption{Principal Components\\Component 3}
\label{figure:PCA203}
\end{figure}

Figure \ref{figure:PCA203}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.10.txt}}
\caption{Median rMedianSE Table}
\label{figure:MedianTable10}
\end{figure}

Figure \ref{figure:MedianTable10}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph102}
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg1}}
\end{figure}

Figure \ref{figure:MedianGraph101}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Not Showing Origin
}
\label{figure:MedianGraph102}
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg2}}
\end{figure}

Figure \ref{figure:MedianGraph102}

\subsubsection{Reduced Feature Models} %%%%%%%%%%%%%%%%%%%%%%%%


% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph111}
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg1}}
\end{figure}

Figure \ref{figure:MedianGraph111}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Not Showing Origin
}
\label{figure:MedianGraph112}
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg2}}
\end{figure}

Figure \ref{figure:MedianGraph112}

\subsection{Regularization}    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.12.txt}}
\caption{Estimated Generalization Error from L2 Regularization}
\label{figure:Regularize3}
\end{figure}

Figure \ref{figure:Regularize3}

% including a pdf
\begin{figure}[h]
\caption{
Estimated Generalization Error from L2 Regularization
\\
Showing Origin
}
\label{figure:Regularization1}
\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg1}}
\end{figure}

Figure \ref{figure:Regularization1}

% including a pdf
\begin{figure}[h]
\caption{
Estimated Generalization Error from L2 Regularization
\\
Not Showing Origin
}
\label{figure:Regularization2}
\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg2}}
\end{figure}

Figure \ref{figure:Regularization2}

\section{Submarket Models}

\subsection{Zip vs. Census vs. City}

\subsection{Regularization}

\section{The best linear model is \dots}
