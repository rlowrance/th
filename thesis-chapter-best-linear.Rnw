\chapter{Finding the Best Linear Model}
% vim: textwidth=72
% vim: foldmethod=manual

<<BestLinearModel>>=
working <- control$working
path.7.txt <- paste0(working, 'e-cv-chart_chart7_3.txt')
path.8.txt <- paste0(working, 'e-cv-chart_chart8.txt')
path.lcv2 <- paste0(working, 'e-features-lcv2-chart_1.txt')
path.9.txt <- paste0(working, 'e-cv-chart_chart9.txt')
path.9.gg1 <- paste0(working, 'e-cv-chart_chart9_1.pdf')
path.9.gg2 <- paste0(working, 'e-cv-chart_chart9_2.pdf')
path.pca.1 <- paste0(working, 'e-features-pca-chart_1.txt')
path.pca.2.01 <- paste0(working, 'e-features-pca-chart_2_01.txt')
path.pca.2.02 <- paste0(working, 'e-features-pca-chart_2_02.txt')
path.pca.2.03 <- paste0(working, 'e-features-pca-chart_2_03.txt')
path.10.txt <- paste0(working, 'e-cv-chart_chart10.txt')
path.10.gg1 <- paste0(working, 'e-cv-chart_chart10_1.pdf')
path.10.gg2 <- paste0(working, 'e-cv-chart_chart10_2.pdf')
path.11.gg1 <- paste0(working, 'e-cv-chart_chart11_1.pdf')
path.11.gg2 <- paste0(working, 'e-cv-chart_chart11_2.pdf')
path.12.gg1 <- paste0(working, 'e-cv-chart_chart12_1.pdf')
path.12.gg2 <- paste0(working, 'e-cv-chart_chart12_2.pdf')
path.12.txt <- paste0(working, 'e-cv-chart_chart12.txt')
path.13.indicator  <- paste0(working, 'e-cv-chart_chart13_indicators_medRMSE.txt')
path.13.submarkets <- paste0(working, 'e-cv-chart_chart13_submarkets_summary_medRMSE.txt')
path.13.examples.RMSE   <- paste0(working
                             ,'e-cv-chart_chart13_submarkets_examples_property_city_medRMSE.txt')
path.13.examples.MARE   <- paste0(working
                             ,'e-cv-chart_chart13_submarkets_examples_property_city_medMARE.txt')
path.14.rf.1.30 <- paste0(working, 'e-cv-chart_chart14_1_30.txt')
path.14.rf.1.60 <- paste0(working, 'e-cv-chart_chart14_1_60.txt')
path.14.rf.5.30 <- paste0(working, 'e-cv-chart_chart14_5_30.txt')
path.14.rf.5.60 <- paste0(working, 'e-cv-chart_chart14_5_60.txt')
@ 

Linear models predict the price as a linear function of features:

\[ price = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n. \]

From this simple idea, a huge variety of models can be generated. Which
linear models were most accurate for predicting the prices of houses?

We answered this question by examining data from 2003 and later using the
decennial census from the year 2000 but not the tax assessment for 2008.
Our interest was in estimating the generalization error for unseen
transactions. For this, we used 10-fold cross validation.

Property descriptions came from the tax roll for the year 2008. The
tax roll contained property descriptions as of October 31, 2007. We
needed to exclude from our data any houses built or modified after that
date. An analysis showed that there are no such houses.

All of the linear models in this chapter are local linear models, by
which we mean that a separate model was fitted for each query
transaction. The overall effect is to convert the linear models into an
aggregate non-linear model.

The plan of attack is reflected in the sections that follow.

\begin{itemize}

\item Section 1 explains our choice of metric to compare models. We
show that using different metrics leads to judging that different
models were better. We decided to focus on median errors.

\item Section 2 studies design choices for local linear models that cover the
entire market.  We started by considering model forms and the number of
training days that led to the most accurate models. (The term ``model
form'' captures whether transformations into the log domain are used in
the left hand and right hand sides of the model. More on this later.)
Having fixed form and number of training days, we then considered which
features were best and compared heuristics for selecting the best
features.  Finally, fixing form, number of  training days, and feature
sets, we showed that regularization improves estimation accuracy.

\item Section 3 considers submarket models. Submarket models partition
the possibly queries and use the partitions in the fitting and
prediction processes. We considered both indicator-based submarket models,
in which the submarkets are represented by indicator variables in a
single model for all queries, and separate submarket models, in which a
separate model is build for each submarket. We considered multiple
definitions for submarkets. We found that submarket models outperform
entire-market models.

\item Section 4 summarizes our results for local linear models and provides
some guidelines for extending the work into a commercial setting.

\item Section 5 compares the results from the local linear models with a
random forests model. We show that the local linear model outperformed
the random forests model.

\end{itemize}

\section{Figure of Merit}

The literature often compares models using the fraction of estimates
within 10 percent of the known values. The motivation for this figure of
merit may in part be that Freddie Mac is said \cite[footnote on
p.~642]{fik-03-spatial} to use that metric to judge the effectiveness of
automated valuation models. In any case, using fraction within 10
percent (along with fraction within 20 and 30 percent) is popular in the
real estate pricing literature.

It is a curious choice of metric for at least two reasons. The first
reason is that linear models are often trained to minimize the mean
squared error. So fraction within 10 percent isn't the criteria
the model was trained to deliver. The second reason is that selecting
fraction within $x$ percent is implicitly a decision to favor models
that are close when accurate and to not minimize the average error.

Here's a contrived example which illustrates that point. There
are two properties both of which sold for \$100,000. Suppose we have two
models to consider. Model A produces estimates of  \$100,000 and
\$200,000.  It is either perfect or very wrong. Model B produces
estimates of \$85,000 and \$115,000. It is never super accurate but
never very wrong.  Model A has 50 percent of its estimates within 10
percent of the true value, Model B has 0 percent of its estimates within
10 percent of the true value. So under the fraction-with-$x$-percent
metric, we prefer model A over model B. 

Now imagine using model A. When it delivers its estimate, we have no
idea whether the estimate is good or bad.  To me, this situation
is untenable if there is an acceptable alternative (and there is: use
some variant on mean squared error). To make the untenable situation
acceptable, one would need to produce both an estimate and a confidence
in the estimate.  Now the model has to be trained to deliver two values,
and we have conceivably more than doubled the difficulty of the problem
of selecting the best model: there are now two figures of merit and how
to make tradeoffs between them is not obvious.

The issue of choice of figure of merit would be moot if using either
metric resulted in selecting the same model as best. To investigate
whether the issue is moot, we designed an experiment. In the experiment,
we considered the estimated generalization error from a variety of models.
The models were generated by sweeping these design choices:

\begin{itemize}

\item Response variable: either the $price$ or $\log(price)$

\item The form of the predictors: either level (natural units) or log of
natural units for size features

\item The number of days in the training period, from 30 up to 360 in
steps of 30 days

\item The figure of merit used to select the model: one of the mean of
the root mean squared errors from the folds (\code{meanRMSE} in the
figure), the median of the root median squared errors from the folds
(\code{medRMSE}), or the fraction within 10 percent of actual values
(\code{fctWI10}).

\end{itemize}

All of these models used all features always present except for those
derived from the tax assessor data. All models used data from 2003 on.

\begin{figure}[]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.7.txt}}
\caption{
Estimated Generalization Errors\\
For Model Selection Metrics\\
By Response Variable, Predictors, and Training Periods\\
For Sales in 2003 and Later
}
%\label{figure:merit1Vertical}
\label{figure:path.7.txt}
\end{figure}

The results are depicted in Figure \ref{figure:path.7.txt}.

The table has two panels. In panel A, the response variable is price. In
panel B the response variable is $\log(price)$. Each panel has a row for
each training period considered. The length of the training period is in
column \code{ndays}. Adjacent to the training period are two groups of
columns. The first group is for the level form of the predictors: we
used the predictors in their natural units. The second group is for the
log form of the predictors: we transformed the size features into the
log domain. Each group of three column has the estimated error using one
of the metrics. Our interest was to determine which model was best. For
the first two metrics, \code{meanRMSE} and \code{medRMSE} we sought the
lowest error. For the third, \code{fctWI10}, we sought the highest
accuracy.

You will notice very high value in Panel B, column 2. These are the
estimated errors using the \code{meanRMSE} metric for the log-level form
of the linear model. What is happening is that a few houses were very
badly predicted. These houses tended to have unusually large values for
some features, causing the $\log(price)$ estimate to be very much higher
than the actual transaction price.

\clearpage

\begin{figure}[ht]
\begin{tabular}{ | l | r | r | r| }
 \hline
             & \multicolumn{3}{c |}{best ndays} \\
 \hline
 form        & \code{meanRMSE} & \code{medRMSE} & \code{fctWI10}\\
 \hline
 level-level & 180 & 90 &  90 \\
 level-log   &  90 & 90 & 120 \\
 log-level   & 195 & 30 &  30 \\
 log-log     & 120 & 30 &  30 \\
 \hline
\end{tabular}
\caption{
Preferred Number of Training Days\\
By Model Selection Metric\\
For Model Forms\\
Summarizing Figure \ref{figure:path.7.txt}
}
\label{figure:Analysis}
\end{figure}

We wanted to answer the question: which model was selected as best by each
metric? Figure \ref{figure:Analysis} contains these results. 

There are several observations to make:

\begin{itemize}

\item In no cases, did all three metrics pick the same number of training
days.

\item The metrics \code{medRMSE} and \code{fctWI10} closely agreed on the
best number of training days. They agreed on all model forms except for
the level-log form, and there they are only 30 days apart.

\item The metric \code{meanRMSE} usually picked more training days as
optimal than did the other metrics.

\item It was always true that the expected loss for the metric
\code{meanRMSE} greatly exceeded that for \code{medRMSE}. This is because
there is right-skew in the prices: the mean exceeds the median, so that
the mean error can exceed the median error.

\end{itemize}

We concluded that the choice of metric did influence the choice of the
best training period to use.

Going forward, we focused on the \code{medRMSE} metric, judging that it
more intuitively sizes average errors than did \code{meanRMSE} (because
of the right-skew in the prices) and reflecting our predisposition to
minimize average errors rather than the tightness of close estimates.


\section{Entire-market Models}

This section explores linear models that are specified once for the
entire marketplace: there is one model for every query transaction and
this model is location aware only through features of the census tract
and zip code of the property. I call these kinds of models
``entire-market models.''

Within the class of entire-market models, we explored design choices. In
the first subsection that follows, we examine the form of the equation
in the model and the training period used to fit the model. We found that
training using 30 days of data before the query transaction was best.
The best form of model to use was log-level, so that one would predict
the log of price using the features in natural units. In the subsequent
subsection, we then examine, within this choice of training period and
form, which features gave the best predictive performance and examined
a reduced-feature model. We found that a 15-feature model performed
best. In the final subsection, we examine adding an L2 regularizer to
the 15-feature model. We found that adding the regulizer improved
performance slightly.

\subsection{Model form and number of training days}

In the literature, a popular model for is the log-level form: one
predicts $\log(price)$ using features in their natural units. Other
choices for model form include log-log, in which the log of price is
predicted using size features that are in the log domain and non-size
features in their natural units, level-level in which price is predicted
using features in their natural units, and level-log in which price is
predicted using size features in the log domain and non-size features in
their natural units.

At least four rationales can be invoked to choose a model form. First is
that the form of the model should conform to one's prior assumptions
about the data generation process. If you suppose that doubling the lot
size might double the price, you can admit this possibility by
specifying $\log(price) = \beta \ log(size)$, a log-log form. If you
suppose that adding a bedroom adds a certain amount of value, you would
want to have $price = \beta \ bedrooms$, a level-level form. 

A second rationale for choosing the form is to transform the prices to a
form more compatible with the statistical assumptions of the model.
Since real estate prices tend to be right-skewed, a log transformation
would probably make the transformed distribution more normal.

A third rationale is to transform the price to the log domain in order
to avoid applying equal importance with respect to prediction errors
to expensive and less expensive houses. By prediction $\log(price)$
instead of $price$, the idea  is to fit the model to minimize errors on
the more typical, less expensive houses, not to minimize the
price-weighted error, which could be regarded as overly influenced by a
few very large houses.

We focused on a fourth rationale and simply asked: Which model form
yielded the lowest estimated generalization errors as measured by the
median of the root median squared error across folds? 

Another issue is to determine the best training period. The linear
models are setting the marginal price of each feature. If prices are
moving slowly, then training on a longer period of time might be
beneficial.  If prices are moving rapidly, then training only on recent
data might be necessary.

<<NumLocalModels, include=FALSE>>=
num.years <- 6
num.months <- 30
days.per.year <- 365
days.per.month <- 30
days <- num.years * days.per.year + num.months * days.per.month
days
@

Our experiment fixed the time period (2003 on) and the feature set
(all features always in transactions excluding the features derived from
the tax assessment). We varied the model form and number of training days
and built a local model for every query transaction in every training
fold. The local model was built by using the query date to discard data that
was not visible until or after the training date. The same model was
re-used for all query transactions on the date of the query transaction.
As a result, we built one model for almost every day in the transaction
set. The transaction set contains data for 6 years and 3 months, hence
we built about $6 \times 365 + 3 \times 30 \approx \Sexpr{days}$ models.

\begin{figure}[ht]
%\tiny
%\scriptsize
\small
%\normalsize
\verbatiminput{\Sexpr{path.8.txt}}
\caption{
Estimated Generalization Errors\\
By Response Variable, Predictors, and Training Period\\
Using Census Features and Not Tax Assessor Features\\
For Sales in 2003 and Later
}
\label{figure:path.8.txt}
\end{figure}


<<FormNdays, include=FALSE>>=
lowest <- 58862
highest <- 77560
p <- 1 - (lowest / highest)
p
@

Figure \ref{figure:path.8.txt} shows the estimated generalization
errors from 10-fold cross validation. The lowest error is for $ndays =
30$ and is in the fourth column, which holds errors for the log-level
model. Thus the lowest error occured when training for 30 days and
predicting $\log(price)$ using the features in their natural units. The
log-level model (in column four) always has the lowest error for every
choice for the number of training days. One way to think about the
results is to consider a very naive model builder who would build a
level-level model and use a year of training data. The model builder
would find an error that is the last entry in the second column:
\$\Sexpr{Commas(highest)}.  The best error is \$\Sexpr{Commas(lowest)},
which is \Sexpr{Percent(p)} percent lower.

In our search for the best linear model, we fixed the number of training
days to 30 and used the log-level form for the model.


\subsection{Feature selection}

The models in the previous sections all used the 24 features that always
appeared in our transactions and were not derived from the tax assessment.
Which of these are the best features to use? Here we consider two
approaches. In the first, we use a simple heuristic to determine the
best features to use. In the second, we use Principal Components
Analysis (PCA) for the same purpose.  We then compare results.

\subsubsection{LCV Heuristic}

We had 24 features that were candidates for inclusion in the model. A
straight-forward way to determine the best feature set would have been
to consider all possible subsets and estimate the generalization error
for each.  Doing so would have been way too time consuming, because
there were $2 ^ {24}$ possible feature sets to evaluate.

Our idea was to instead use L1 regularization to determine the
rank ordering of the 24 features in terms of their importance. We then
used cross validation to estimate the generalization error from a model
with the first, the first two, \dots, all 24 rank ordered features. We
call the procedure LCV, for L1 regularization followed by cross
validation.

The implementation was by using the Elastic Net from Zoe and Hastie
\cite{zou-05}. It fits a linear model while concurrently using an L1
and L2 regularizer. As a by-product, an R implementation, the
\code{elasticnet} R package \cite{zou-12}, produces a rank ordering of
the features in terms of their importance. In our use of this package,
we set the L2 regularizer to zero and used just the rank ordering of
features under the L1 regularizer.

\begin{figure}[ht]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.9.txt}}
\caption{
Table of Estimated Generalization Errors\\
And 95 Percent Confidence Intervals\\
For 24 Sets of Features\\
For Model Form log-level\\
For 30 Day Training Period\\
For Sales in 2003 and Later
}
\label{figure:path.9.txt}
\end{figure}

The result of the rank ordering from the Elastic Net procedure is in
Figure \ref{figure:path.9.txt}. It shows that the most important
feature is the living area of the house. Real estate agents often claim
the most important feature of the house is its location, but that wasn't
true in these data. Instead, features of the location (median household
income and average commute time) were second and third in importance. 

\clearpage

\begin{figure}[h]
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
\caption{
Graph of Estimated Generalization Errors\\
And 95 Percent Confidence Intervals\\
For 24 Sets of Features\\
For Model Form log-level\\
For 30 Day Training Period\\
For Sales in 2003 and Later
}
\label{figure:path.9.gg2}
\end{figure}

The same data are shown graphically in Figure \ref{figure:path.9.gg2},
which displays the estimated generalization error from including the
first $n$ features, together with a 95 percent confidence interval. The
large dots are the median of the root median squared errors from the
folds. The small dots to either side of the large dots are the lower and
upper bounds of the confidence intervals. One sees that the lowest
estimated error was when using the first 15 features, from living-area to
zip5-has-school. The estimated errors then tend to increase and the
placement of the confidence intervals does not compel one to consider
going beyond the first 15 features. 

If one seeks parsimony, one could consider using the first six
features (through fraction-owner-occupied), after which there was an increase
in estimated generalization error before a subsequent decline to the
minimum at 15 features.

\begin{figure}[ht]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.lcv2}}
\caption{
24 Features\\
By Importance Rank\\
Classified as House Features or Location Features
}
\label{figure:path.lcv2}
\end{figure}

Figure \ref{figure:path.lcv2} shows the 24 rank-ordered features split into
house features and location features. Some observations on the house
features:

\begin{itemize}

\item living-area: this is the interior living space. Construction is
required to change it, so I was not surprised to see it highly-ranked.
That it is more highly ranked than the land square footage was a surprise
to me, because land square footage seems harder to change than interior
living space.

\item fireplace-number: the number of fireplaces, including the
possibility of zero. A surprise---perhaps fireplaces are proxies for
overall amenities in the house.

\item year-built: the year the house was built. Other studies have
concluded that the age and the squared age of the house are important
features. Our models are local, so that we need a house's age for every
query transaction.  Instead of pre-building a training set for each
possible query, we transformed year built into age and age squared just
before fitting the model. We performed a similar transformation for the
effective year built, which is the last year of significant updates to
the house.

\item factor-has-pool: whether there is a swimming pool. Not a
surprise, as the houses are all in Los Angeles, where the weather is
often pleasant and a swimming pool might be a joy.

\end{itemize} 

Some observations on the location features:

\begin{itemize}

\item median-household-income and fraction-owner-occupied: the median
income in the census tract from the year 2000 census and the fraction of
houses in the census tract that are occupied by owners. Both of these
features indicate the wealth of the neighborhood.

\item average-commute-time: in other studies, longer commutes are
associated with lower prices.

\item zip5-has-industry and census-tract-has-industry: being near
industry was not good for house prices.

\item census-tract-has-park and zip5-has-park: Los Angeles residents are
not there for the parks. Perhaps this is because it is a driving-around
city, not a walking-around city.

\end{itemize}


\subsubsection{PCA}

\begin{figure}[ht!]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.1}}
\caption{Cumulative Variance of the 24 Principal Components}
\label{figure:path.pca.1}
\end{figure}

The second approach we used to determine the importance of features was
Principal Components Analysis. Figure \ref{figure:path.pca.1} shows the
principal components associated with the training set predictors. The
first three principal components accounted for almost 100 percent of the
variance.

\begin{figure}[ht!]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.01}}
\caption{Feature Weights for the First Principal Component}
\label{figure:path.pca.2.01}
\end{figure}

In Figure \ref{figure:path.pca.2.01} we see the weights of the 24 features in
the first principal component. The median household income had the
highest absolute weight. Other weights were much lower in absolute value.
The only ones of much importance were the land square footage and the
living area. It seems that the first principal component may have been about
the wealth of the neighborhood.

% non-rotated txt chart
\begin{figure}[ht!]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.02}}
\caption{Feature Weights for the Second Principal Component}
\label{figure:path.pca.2.02}
\end{figure}

In Figure \ref{figure:path.pca.2.02} we see the weights of the 24 features in
the second principal component. The land square footage had the highest
absolute weight. Other weights were much lower in value with household
income and living area being the largest in absolute size. This
principal component was possibly about the size of the land.

% non-rotated txt chart
\begin{figure}[ht!]
%\tiny
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.03}}
\caption{Feature Weights for the Third Principal Component}
\label{figure:path.pca.2.03}
\end{figure}

In Figure \ref{figure:path.pca.2.03} we see the weights of the 24 features
for third principal component. It is mostly composed of the basement
square feet, perhaps a proxy for other size features in the house:
bigger basement probably implies bigger house. This principal component
was possibly about the size of the house.

\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.10.txt}}
\caption{
Table of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Feature Sets Selected by the PCA Analysis
}
\label{figure:path.10.txt}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg2}}
\caption{
Graph of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Feature Sets Selected by the PCA Analysis
}
\label{figure:path.10.gg2}
\end{figure}


In Figure \ref{figure:path.10.txt} we see the estimated generalization
error from using the median household income, land square footage, and
so on up to the 4 features identified as important in the PCA work. The
same data are in Figure \ref{figure:path.10.gg2} which shows the 95
percent confidence intervals graphically. The lowest estimated error was
from using the first 3 features. Adding basement-square-feet slightly
increased the estimated error but yielded a smaller 95 percent confidence
interval, so some may prefer keeping the fourth PCA feature.

\subsubsection{Reduced Feature Models} %%%%%%%%%%%%%%%%%%%%%%%%

% including a pdf
\begin{figure}[ht!]
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg1}}
\caption{
Graph of Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Feature Sets Selected by the LCV and PCA Analyses
}
\label{figure:path.11.gg1}
\end{figure}

<<Reduce, include=FALSE>>=
error.lcv.15 <- 58571
error.pca.3 <- 70369
p <- 1 - (error.lcv.15 / error.pca.3)
@

To find the best reduced-feature model, we tested all the feature sets
suggested by the LCV and PCA analyses. These results are in Figure
\ref{figure:path.11.gg1}, which brings together results previously
shown. The PCA-derived feature sets all underperformed the LCV-derived
feature sets: extremely small feature sets were challenged in this
setting. The best feature set was the one with the first 15 features from
the LCV heuristic. This feature set had an estimated error of
\$\Sexpr{Commas(error.lcv.15)}, which was \Sexpr{Percent(p)} percent
lower that the estimated error of \$\Sexpr{Commas(error.pca.3)} which was
found by PCA.

\subsection{Regularization}    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regularizing models is one way to avoid overfitting and thereby improve
prediction accuracy. So far the models were not regularized.

To assess the extent to which regularization leads to more accuracy, we
took the model using the 15 best features and ran multiple versions using
several choices for the weight of the L2 regularizer. A simple line
search was used. 

\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.12.txt}}
\caption{
Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Selected L2 Regularizers\\
}
\label{figure:path.12.txt}
\end{figure}

\begin{figure}[ht!]
\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg2}}
\caption{Estimated Generalization Error from L2 Regularization }
\label{figure:path.12.gg2}
\end{figure}

Figure \ref{figure:path.12.txt} contains the result in a table. We see
the typical pattern: increasing the regularizer weight first decreases
errors and as the weight continues to increase, errors are increased.
Setting the weight to $\lambda = 55$ seemed like a good choice: it had the
lowest median across the cross-validation folds and from Figure
\ref{figure:path.12.gg2}, one can see the 95 percent
confidence intervals were not signaling warnings.

<<RegularizationImpact,include=FALSE>>=
l2.0 <- 58622
l2.55 <- 58453
diff <- l2.0 - l2.55
p <- 1 - (l2.55 / l2.0)
p
stopifnot(p < 0.003)
@

The impact of the regularizer on the estimated generalization error was
very small: the median was reduced by \$\Sexpr{diff} from an
unregularized error of \$\Sexpr{Commas(l2.0)}, a reduction of less than
three-tenths of a percent. The small impact of the regularizer suggests
that there is not much overfitting in the unregularized model. Perhaps
this was because the models are all local models.

\section{Submarket Models}

We considered three very simple definitions of submarkets. In the first,
the submarkets were defined by the census tracts. Each census tract was its
own submarket. In the second, submarkets were defined by the city names.
Each city was its own submarket. In the third, submarkets were defined by
zip code. Each five-digit zip code was its own submarket.

We considered two approaches to incorporating submarkets into the model.
The first used indicator variables, the second built a separate model
for each submarket.

\subsection{Using Indicator Variables}

The indicator models all had indicators for whether the observation was
in a specific submarket. For example, for zip-code based submarkets, the
model was

% ref: gratzer p.220
\begin{align}
\log(price) & =  \beta_0
            + \beta_1 ZipIs11111 + \beta_2 ZipIs22222 + \ldots \\
            & + \beta_k LotSize + \beta_{k+1} LivingArea + \ldots . 
\end{align}


The indicators were capturing the possibly different average prices in
each submarket. The average price per feature were the same across
submarkets.

\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.indicator}}
\caption{
Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Submarket Indicators
}
\label{figure:path.13.indicator}
\end{figure}

<<Indicators, include=FALSE>>=
no.indicators <- 58453
zip5.indicators <- 41013
p <- 1 - (zip5.indicators / no.indicators)
p
@

Figure \ref{figure:path.13.indicator} shows the estimated generalization
errors from adding indicators variables to the regularized linear model.
We see that adding indicators for zip codes, census tract, and cities
all helped and that the best result was obtained with the five-digit zip
code indicators, which had an error rate \Sexpr{Percent(p)} percent
lower than the entire-market model (the model with no indicators).

\subsection{Separate Models for Each Submarket}

The separate-models approach built a separate model for each submarket.
Each model had the same form:

\[
\log(price) =
\beta_0 
+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots
.
\]

A query transaction was presented, its submarket was determined, for
example, it was in zip code 11111, and then a model was trained using only
transactions in the prior 30 days in the zip code. This fitted model was
used to estimate the value for the query transaction. 

This fitting process resulted in many models that cannot be estimated, most
often because there were too few transactions in the training set. 


\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.submarkets}}
\caption{
Estimated Generalization Errors and 95 Percent Confidence Intervals\\
For Submarket Models
}
\label{figure:path.13.submarkets}
\end{figure}

<<Submarkets, include=FALSE>>=
no.indicators <- 58453  # from prior chart
best.indicators <- 41013
property.city <- 46405
p <- 1 - (property.city / no.indicators)
p
p2 <- 1 - (best.indicators / property.city)
p2
@


Figure \ref{figure:path.13.submarkets} shows the results of this experiment.
The submarket models that used city names had the lowest estimated
generalization error, which was \Sexpr{Percent(p)} percent lower than the whole-market
error (the model from the previous figure with no indicators). The
fitting process, however, succeeded in all folds in only 66 percent of
the city names. (We defined ``covered'' to mean that at least one model
for the submarket could be fit in every test fold. We defined ``coverage'' to
be the fraction of possible submarkets that were covered.)
In terms of the estimated error, the worst model was for zip codes, but
it had the highest coverage. The lowest coverage was for census tracts.

Perhaps the city names had the lowest errors because cities may acquire
brand positioning: they stand for something. Whatever that something
is, it attracts buyers who have affinity for that something, reinforcing
the positioning. The reinforcement leads to homogeneity within a city
and that makes it easier to fit the linear models.

Coverage numbers could be increased most likely by either retraining the
models on more than 30 days of data or forming submarket groups by
splicing together some of the submarkets. The latter approach has been
tried in the literature. The former appears to have not been tried.

Comparing Figures \ref{figure:path.13.indicator} and
\ref{figure:path.13.submarkets} we see that the best indicator model had
an error of \$\Sexpr{Commas(best.indicators)}, which was lower than the
\$\Sexpr{Commas(property.city)} error for the city model. I was
surprised by this, because the indicator-based models were simply
adjusting the average price level for each area and the submarket models
have the freedom to adjust the value of each feature in each submarket.


The estimated errors in Figure \ref{figure:path.13.submarkets} are perhaps
deceptive: they are the median values across all the submarkets. Individual
models for specific submarkets may have much lower or higher errors than
the median for all markets.

% median prices for the 10 lowest cities
% lancaster 222K
% palmdale  240
% norwalk   375
% arleta    405
% compton   290
% lapuente  355
% southgate 355
% pomona    335
% valinda   360
% panorama city 394

% median prices for the 10 largest cities
% rolling hills estates 1.2 million
% santa monica          1.3
% palos verdes penninsula ?
% beverly hills           1.4
% toluca lake             1.0
% hermosa beach           1.0
% pacific palisades       1.6
% marina delrey           1.0
% hidden hills            2.1
% rolling hills           1.9
% malibu                  1.8

\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.examples.RMSE}}
\caption{
Estimated Generalization Errors\\
For Selected Property City Submarket Models\\
Using Metric Median of Root Median Squared Errors
}
\label{figure:path.13.examples.RMSE}
\end{figure}

Let's focus on the city-defined submarkets, which had the lowest median
errors. Error estimates were produced for 126 cities. The median errors
for each city vary widely. Figure \ref{figure:path.13.examples.RMSE}
shows the medians for various cities. We see that the most
accurately-estimated houses were those in Lancaster, where the estimated
generalization error was about \$24,000. A common attribute of the
cities with the ten lowest errors was that they all had relatively
low-priced houses. Median house prices for them from 2003 through 2009
ranged from \$220,000 to \$405,000.

Turning to the cities with the ten highest estimated errors, we see
cities that were expensive: Santa Monica, Beverly Hills, Malibu. The
median prices in this group of cities ranged from \$1.0 million to \$2.1
million (for Hidden Hills).

The larger estimated errors for the more expensive cities may have been
generated in part from the higher prices, so that a 10 percent error in
an estimate was a larger amount than for lower-priced cities.  Another
source of the large errors might be that the more expensive properties
sell based on features that were not in our data. 

To assess the possible sources of error, we re-ran the analysis
underlying the previous figure but changed the metric. Rather than
report the median error, we examined the median of the absolute relative
errors across the test folds. The idea was that if the higher errors were
generated solely by higher prices, then switching to a measuring
relative errors should show more uniform errors when comparing the
low-error submarkets to the high-error submarkets.


\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.examples.MARE}}
\caption{
Estimated Generalization Errors\\
For Selected Property City Submarket Models\\
Using Metric Median Across Folds of Median Absolute Relative Error
}
\label{figure:path.13.examples.MARE}
\end{figure}

The results of this analysis are in Figure
\ref{figure:path.13.examples.MARE}. We see that the the houses with the
lowest error (those in the first group) had much lower relative errors
than the houses with the highest errors (those in the last group). The
group in the middle had relative errors in the middle. Thus we concluded
that the higher absolute errors for the expensive cities in the
submarkets model were probably not caused by the simply higher prices of
these houses. Something else seemed to be going on.

\clearpage



\section{The best linear model is \dots}

Based on the time period from 2003 through the first part of 2009, the
best linear model would predict $\log(price)$ from the features in
natural units, use the 15 features we identified, and incorporate an L2
regularizer.  It would be fitted on 30 days of data. 

If one wanted an entire-market model, one would then add indicator
variables for the zip codes. However, a submarket model might be
preferred, because it would have errors that reflect specific
submarkets. We found that defining submarkets around cities is best. The
model design process that selected the 15 features, 30 days, and the L2
regularizer could be repeated for each submarket. Coverage could be
increased by collapsing some submarkets into bigger submarkets.

Commercial model builders should consider a different focus in selecting
the hyperparameters than we have used here.  Rather than select them
once for the entire time period, they should select hyperparameters
based on recent transactions: we were interested in models that worked in
average time periods, most commercial price estimators are interested in
models that work on tomorrow's transactions.

\section{Coda: Random Forests}

Every study in our literature review that compared linear to non-linear
models claimed that non-linear models generally outperform linear models
when predicting house prices. We have designed a local linear model. It
is also a non-linear model. How well does it perform compared to the
random forests model, a popular non-linear model?

To design a random forests model, we swept these design choices.

\begin{itemize}

\item The number of training days. We tested both the 30 and 60 days.

\item The feature set used: we tested both all 24 features and the 15
features that were best for the linear model. 

\item The number of trees in the random forest (\code{ntree}): more
trees may fit better but may also overfit.

\item The number of features to try when adding a new leaf to the tree
(\code{mtry}). The implementation of random forests first tests that
number of random-selected features to add and then adds the one giving
the best greedy performance. We tested 1, 2, 3, and 4 features.

\end{itemize}


\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.14.rf.5.30}}
\caption{
Estimated Generalization Errors\\
For Random Forests\\
For Selected Hyperparameters \code{ntree} and \code{mtry}\\
For All Features Except Assessment Features and the Best 15 Features from the Linear Models\\
Trained for 30 Days\\
Using a Five Percent Sample of Queries in Folds
}
\label{figure:path.14.rf.5.30}
\end{figure}


\begin{figure}[ht!]
\scriptsize
%\small
%\normalsize
\verbatiminput{\Sexpr{path.14.rf.5.60}}
\caption{
Estimated Generalization Errors\\
For Random Forests\\
For Selected Hyperparameters \code{ntree} and \code{mtry}\\
For All Features Except Assessment Features and the Best 15 Features from the Linear Models\\
Trained for 60 Days\\
Using a Five Percent Sample of Queries in Folds
}
\label{figure:path.14.rf.5.60}
\end{figure}

<<RandomForests, include=TRUE>>=
error.rf <- 47035
error.zip.indicators <- 41013
p <- 1 - (error.rf / error.zip.indicators)
p
@  
Figure \ref{figure:path.14.rf.5.30} shows the results of this experiment
when training on 30 days of data. Figure \ref{figure:path.14.rf.5.60}
shows the results when training on 60 days of data. In order to reduce the
computational burden, rather than test hyperparameter choices on every
transaction in each test fold, we drew a random five percent sample.
For each sampled query, we trained a local model using all of the data
in all of the training folds. 

The lowest error for the random forests models was when training on 60
days of data, using the 15 best predictors from the linear models,
setting $ntree = 300$, and setting $mtry = 4$. This error was
\$\Sexpr{Commas(error.rf)}, which was \Sexpr{Percent(-p)} percent higher
than the best entire-market linear model, the one with the zip code
indicators, which had an error of \$\Sexpr{Commas(error.zip.indicators)}.

From this analysis, one should not conclude that local linear models
beat random forests models in this setting. There is still more design
work to be done around the random forests models. For example, one could
explore whether 90 days of training is better than 60, and repeat the LCV
feature analysis task. Also, the sample size for random forests was
fairly small (five percent), so increasing it would be indicated.
However, the computation burden for the random forests model was much
higher than for the local linear models.

\clearpage

