\chapter{Finding the Best Linear Model}
% vim: textwidth=72
% vim: foldmethod=manual

<<BestLinearModel>>=
working <- control$working
path.cv.criterion <- paste0(working, 'e-cv-chart_chart7.txt')
path.cv.form.ndays <- paste0(working, 'e-cv-chart_chart8.txt')
path.lcv2 <- paste0(working, 'e-features-lcv2-chart_1.txt')
path.9.txt <- paste0(working, 'e-cv-chart_chart9.txt')
path.9.gg1 <- paste0(working, 'e-cv-chart_chart9_1.pdf')
path.9.gg2 <- paste0(working, 'e-cv-chart_chart9_2.pdf')
path.pca.1 <- paste0(working, 'e-features-pca-chart_1.txt')
path.pca.2.01 <- paste0(working, 'e-features-pca-chart_2_01.txt')
path.pca.2.02 <- paste0(working, 'e-features-pca-chart_2_02.txt')
path.pca.2.03 <- paste0(working, 'e-features-pca-chart_2_03.txt')
path.10.txt <- paste0(working, 'e-cv-chart_chart10.txt')
path.10.gg1 <- paste0(working, 'e-cv-chart_chart10_1.pdf')
path.10.gg2 <- paste0(working, 'e-cv-chart_chart10_2.pdf')
path.11.gg1 <- paste0(working, 'e-cv-chart_chart11_1.pdf')
path.11.gg2 <- paste0(working, 'e-cv-chart_chart11_2.pdf')
path.12.gg1 <- paste0(working, 'e-cv-chart_chart12_1.pdf')
path.12.gg2 <- paste0(working, 'e-cv-chart_chart12_2.pdf')
path.12.txt <- paste0(working, 'e-cv-chart_chart12_3.txt')
path.13.indicator  <- paste0(working, 'e-cv-chart_chart13_indicators.txt')
path.13.submarkets <- paste0(working, 'e-cv-chart_chart13_submarkets_summary.txt')
path.13.rf <- paste0(working, 'e-cv-chart_chart14.txt')
@ 

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n. \]

From this simple idea, a huge variety of models can be generated. Which
linear models are most accurate for predicting the prices of houses?

We answer this question by examining data from 2003 and later using the
decennial census from the year 2000 but not the tax assessment for 2008.
Our interest is in estimating the generalization error for unseen
transactions. For this, we use 10-fold cross validation.

Property descriptions come from the tax roll for the year 2008. The
tax roll contains property descriptions as of December 31, 2007. We
need to exclude from our data any houses built or modified after that
date. An analysis shows that there are no such houses.

The plan of attack is reflected in the sections that follow.

\begin{itemize}

\item Section 1 explains our choice of metric to compare models. We
show that using different metrics leads to judging that different
models are better. We decide to focus on median errors.

\item Section 2 studies design choices for linear models that cover the
entire market.  We start by considering model form and the number of
training days that lead to the most accurate models. Having fixed form
and number of days, we then consider which features are best and
compares heuristics for selecting the best features. Finally, fixing
form, number of days, and feature sets, we show that regularization
improves estimation accuracy.

\item Section 3 considers submarket models. We consider both
indicator-based models and separate models built for each submarket. We
find that submarket models outperform entire-market models.

\item Section 4 summarizes our results for linear models and provides
some guidelines for extending the work into a commercial setting.

\item Section 5 contrasts the results from the linear models with an
easy-to-specify model: the random forests model. We show that an
off-the-shelf random forests model outperforms our carefully-designed
linear model.

\end{itemize}

\section{Figure of Merit}

The literature often compares models using the fraction of estimates
within 10 percent of the known values in training data. The motivation
for this figure of merit is in part that Freddie Mac is said
(\cite{fik-03-spatial} footnote on page 642) to use that metric to judge
the effectiveness of automated valuation models. In any case, using
fraction within 10 percent (along with fractions with 20 and 30 percent)
is popular in the real estate pricing literature.

It is a curious choice of metric for at least two reasons. The first
reason is that linear models are often trained to minimize the mean
squared error (MSE). So fraction within 10 percent isn't the criteria
the model was trained to deliver. The second reason is that selecting
fraction within $x$ percent is implicitly a decision to favor models
that are close when accurate and potentially very far from the true
prices when not accurate. 

Here's a contrived example that illustrates that point: suppose there
are two properties both of which sold for \$100,000. Suppose we have two
models to consider. Model A produces estimates of  \$100,000 and \$200,000:
it is either perfect or very wrong. Model B produces estimates of
\$85,000 and \$115,000: it's never super accurate but never very wrong.
Model A has 50 percent of its estimates within 10 percent of the true
value, Model B has 0 percent of its estimates within 10 percent of the
true value. So under the fraction-with-$x$-percent metric, we prefer
model A over model B. 

Now imagine using model A. When it delivers its estimate, we have no
idea whether the estimate is good or bad.  To me, this situation
is untenable if there is an acceptable alternative (and there is: use
MSE). To make the untenable situation acceptable, one would need to
produce both an estimate and a confidence in the estimate.  Now the
model has to be trained to deliver two values, and we have conceivable
more than doubled the difficulty of the problem of selecting the best
model: there are now two figures of merit and how to make tradeoffs
between them is not obvious.

The issue of choice of figure of merit would be moot if using either
metric resulting in selecting the same model as best. To investigate
this, we designed an experiment. In the experiment, we consider
the estimated generalization error from a variety of model. The models
were generated by sweeping these design choices:

\begin{itemize}
\item Response variable: either the price or log(price)
\item The form of the predictors: either level (natural units) or log of
natural units
\item The number of days in the training period, from 30 up to 360 in
steps of 30 days.
\end{itemize}

All of these models used all features always present except for those
derived from the tax assessor data. All models used data from 2003 on.
To keep running times reasonable, a random one percent sample of possible
test transactions (queries) was drawn from each cross-validation fold.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.criterion}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Measurement Approaches Compared }
\label{figure:merit1}
\end{figure}

The results are depicted in Figure \ref{figure:merit1}.

To read the table, consider the first three rows. All of these are for
the price-level form of the linear model: we predict price using the
natural units for each feature. The three rows show the value of a model
selection metric for 30, 60, and more days. An asterisk before a number
indicates that the number is the minimizer for the row. The metrics are
\code{meanRMSE}, the mean of the root mean squared error for each fold;
\code{medRMSE}, the median of the root median squared error for each
fold; and \code{fctWI10}, the mean fraction of estimates within 10
percent of the actual values for each fold.

There are several observations to make:

\begin{itemize}

\item It's never true that the metric \code{fctWI10} would select as best the
same model that would be choosen by either of the other metrics. Often
the chosen models would have very different training periods. For
example, for the price-level model, both \code{meanRMSE} and
\code{medRMSE} suggest training on 60 days of data and \code{fctWI10}
suggests training on 330 days of data. A similar conclusion holds for
the logprice-level model and the logprice-log model. The metric leads to
very different training periods.

\item It's often true that the metric \code{meanRMSE} and
\code{medRMSE} select a similar number of training days. The number of
training days is either identical or differs by at most 60 days. These
metrics have similar choices for models selected.

\item It's always true that the metric \code{meanRMSE} greatly exceeds
\code{medRMSE}. This is because there is right-skew in the prices: the
mean exceeds the median, so that the mean error can greatly exceed the
median error.

\end{itemize}

Going forward, we focus on the \code{medRMSE} metric, judging that it
more intutitively sizes average errors than does \code{medianRMSE} and
that is is more reliable for model selection than \code{fctWI10}.


\section{Entire-market Models}

This section explores linear models that are specified once for the
entire marketplace: there is one model form for every query transaction
and this model is location aware only through features of the census
tract and zip code of the property. I call theses kinds of models
``entire-market models.''

With the class of entire-market models, we explore design choices. In
the first subsection that follows, we examine the form of the equation
in the model and the training period used to fit the model. We then
examine what features give the best predictive performance and examine a
reduced-feature model. We then show that regularizing the linear model
with an L2 regularizer improves performance.

\subsection{Model form and ndays}

In the literature, a popular model for is the log-linear form: one
predicts $log(price)$ using features in their natural units. Other
choices for model form include log-log, in which the features are in the
log domain, level-level in which price is predicted using features in
the natural units, and level-log in which price is predicted using
features in the log domain.

At least three rationales can be invoked to choose a model. First is
that the form of the model should conform to one's prior about the data
generation process. If you suppose that doubling the lot size might
double the price, you can admit this possibility by specifying
$\log(price) = \beta \ log(size)$. If you suppose that adding a bedroom
adds a certain amount of value, you would want to have $price = \beta \
bedrooms$. If you suppose both are true, you are stuck.

A second rationale for choosing the form is to transform the prices to a
form more compatible with the statistical assumptions of the model.
Since real estate prices tend to be right-skewed, a log transformation
would make the transformed distribution more normal.

A third rationale is to transform the price to the log domain in order
to avoid applying equal importance with respect to prediction accuracy
to expensive and less expensive houses. The idea is to fit the model to
minimize average error not price-weighted error. 

% OMIT, SINCE WE DIDN"T TEST THESE log-mixed form
%Sometimes these rationale's are combined. For example, in
%\cite{chopra-09}, prices were transformed to the log domain in order to
%not overweight the importance of accuracy for larger houses, and size
%feature (such as lot size and interior space) were transformed into the
%log domain to allow for the possibility that doubling them would double
%the price.

Here we focus on the third rationale and simply ask: why model form
yields the lowest estimated generalization errors as measured by the
median rMedianSE across folds.

Another issue is to determine the best training period. The linear
models are setting the marginal cost of each feature. If prices are
moving slowly, then training on a longer periods of time might be
beneficial.  If prices are moving rapidly, then training only on recent
data may be necessary.

Our experiment fixed the time period (2003 on) and the feature set
(all features always in transactions excluding the features derived from
the tax assessment). We vary the model form and number of training days.
Because a local model is built for every query point, we take a one
percent random sample of the possible query points in each fold in order
to reduce the computational burden.

Figure \ref{figure:EGEFormNdays} shows the estimated generalization
errors from 10-fold cross validation. Comparing the first two rows, we
see that when price is the response variable, more accuracy is obtained
by working with features in natural units than in the log domain.
Comparing all the rows, we see that estimating $\log(price)$ instead of
$price$ always leads to reduced estimated generalization errors.
Comparing rows three and four shows that the for most training periods,
using a log-level model outperforms a log-log model. Finally, the lowest
estimated generalization error is for the log-level model when trained
for 60 days.

We use this model form and training period as a benchmark going forward.

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.form.ndays}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors\\Across Model Forms and Training
Period}
\label{figure:EGEFormNdays}
\end{figure}

\subsection{Feature selection}

The benchmark model has two features. Which of these are the best
features to use? Here we consider two approached. In the first, we
use a simple heuristic to determine the best features to use. In the
second, we use PCA for the same purpose. We then compare results and
have one surprising findings.

\subsubsection{LCV Heuristic}

We have 24 features that are candidates for inclusion in the model. A
straight-forward way to determine the best feature combinations is to
consider all possible subsets and estimate the generalization error for
each. This would be way too time consuming.

Our idea is to instead use L1 regularization to consider determine the
rank-ordering of the 24 features in terms of their importance. We then
use cross validation to estimate the generalization error from a model
with the first, the first two, \dots, all 24 rank-ordered features. We
call the procedure LCV, for L1 regularization followed by cross
validation.

The implementation is by using the Elastic Net from Zoe and Hastie
(\cite{zou-05}). It solves a linear model by concurrently using an L1
and L2 regularizer. As a by-product, the \code{elasticnet} R package
(\cite{zou-12}) produces a rank-ordering of the features from Zoe and
Hastie (\cite{zou-05}). It fits a linear model by concurrently using
L1 and L2 regularizer. 

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.9.txt}}
\caption{Median rMedianSE Table}
\label{figure:MedianTable}
\end{figure}

The result of the rank-ordering from the Elastic Net procedure is in
Figure \ref{figure:MedianTable}. It shows that the most important
feature is the living area of the house. Real estate agents claim the
most important features of the house its its location, but that isn't
true in these data. Indeed, a features of the location (median household
income and average commute time) are second and third in importance. 

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph1}
\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg1}}
\end{figure}

The same data are shown in \ref{figure:MedianGraph1}, which shows the
estimated generalization error from including the first $n$ features,
together with a 95 percent confidence interval. One sees that the lowest
estimated error is when using the first 20 features, from living area to
census tract has park. The estimated errors then tend to increase and
the placement of the confidence intervals does not compel one to
consider going beyond the first 20 features. 

If one seeks parsimony, one could consider using the first eight
features (through zip5 has industry), after which there is an increase
in estimated generalization error before a subsequent decline. Another
possible feature set is the first 20 feature (through census tract has
park), as that feature set has the lowest estimated error. Another
choice would be 19 features (through factor is new construction) as
though it would have a slightly higher median estimated generalization
error than for the 20-feature choice, the 95% confidence interval is
smaller.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.lcv2}}
\caption{Most Important Features}
\label{figure:LCV}
\end{figure}

Figure \ref{figure:LCV} shows the 24 rank-ordered features split into
house features and location features. Some observations on the house
features:

\begin{itemize}

\item living area: this is the interior living space. Construction is
required to change it, so its not surprising to see it highly-ranked.
That is is more highly ranked than the land square footage is a surprise
to me.

\item fireplace number: the number of fireplaces, including the
possibility of zero. A surprise, perhaps fireplaces are proxies for
overall amenities in the house.

\item year built: the year the house was built. Other studies have
concluded that the age and the squared age of the house are important
features. Our models
are local, so that we need a house age for every query transaction.
Instead of pre-building a training set for each possible query, we
transform year built into age and age squared just before fitting the
model.

\item factor has pool: whether there is a swimming pool. Not a
surprise, as the houses are all in Los Angeles, where the weather is
often pleasant.

\end{itemize} 

Some observations on the location features:

\begin{itemize}

\item median household income and fraction owner occupied: the median
income in the census tract from the year 2000 census and the fraction of
houses in the census tract that are occupied by owners. Both of these
features indicate the wealth of the neighborhood.

\item average commute time: in other studies, longer commutes are
associated with lower prices.

\item zip 5 and census tract have industry: being near industry is not
good for prices.

\item census tract and zip have park: Los Angeles residents are not
there for the parks. Perhaps this is because its a driving-around city,
not a walking-around city.

\end{itemize}

%
%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph2}
%\centering\includegraphics[scale=.4]{\Sexpr{path.9.gg2}}
%\end{figure}
%

\subsubsection{PCA}

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.1}}
\caption{Principal Components\\Cumulative Variance}
\label{figure:PCA1}
\end{figure}

The second approach to determining the importance of features is to use
Principal Components Analysis. Figure \ref{figure:PCA1} shows the the
principle components associated with the 24 features. We see that the
first three principal components account for almost 100 percent of the
variance.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.01}}
\caption{Principal Components\\Component 1}
\label{figure:PCA201}
\end{figure}

In Figure \ref{figure:PCA201} we see the weights of the 24 features in
the first principal component. The median household income has the
highest absolute weight. Other weights are much lower in absolute value,
The only ones of much importance are the land square footage and the
living area. It seems that the first principal component may be about
the wealth of the neighborhood: more income is correlated with larger
plots is correlated with bigger houses.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.02}}
\caption{Principal Components\\Component 2}
\label{figure:PCA202}
\end{figure}

In Figure \ref{figure:PCA202} we see the weights of the 24 features in
the second principal component. The land square footage has the highest
absolute weight. Other weights are much lower in value with household
income and living area being the largest in absolute size. This
principal component is possibly about the size of the land and has the
same correlated highly-weighted features as the first principal
component.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.pca.2.03}}
\caption{Principal Components\\Component 3}
\label{figure:PCA203}
\end{figure}

In Figure \ref{figure:PCA203} we see the weights of the 24 features
for third principal component. It is mostly composed of the basement
square feet, perhaps a proxy for other size features in the house:
bigger basement probably implies bigger house.

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.10.txt}}
\caption{Median rMedianSE Table}
\label{figure:MedianTable10}
\end{figure}

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE\\
Showing Origin
}
\label{figure:MedianGraph102}
\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg1}}
\end{figure}

%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph102}
%\centering\includegraphics[scale=.4]{\Sexpr{path.10.gg2}}
%\end{figure}

In Figure \ref{figure:MedianTable10} we see the estimated generalization
error from using the median household income, median household income
and land square foot, and so on up to the 4 features identified as
important in the PCA work. The same data are in Figure
\ref{figure:MedianGraph102} which shows the 95 percent confidence
intervals graphically. One might choose to use only the first 3 features
based on this graphic.

\subsubsection{Reduced Feature Models} %%%%%%%%%%%%%%%%%%%%%%%%

% including a pdf
\begin{figure}[h]
\caption{
Median rMedianSE
}
\label{figure:MedianGraph111}
\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg1}}
\end{figure}

To find the best reduced-feature model, we test all the feature sets
suggested by the LCV and PCA analyses. These results are in Figure
\ref{figure:MedianGraph111}, which brings together results previously
shown. A surprising conclusion is that the PCA-derived feature sets all
underperform the LCV-derived feature sets: extremely small feature sets
are challenged in this setting. The best feature sets is the
one with the first 20 features in Figure \ref{figure:LCV}

%
%% including a pdf
%\begin{figure}[h]
%\caption{
%Median rMedianSE\\
%Not Showing Origin
%}
%\label{figure:MedianGraph112}
%\centering\includegraphics[scale=.4]{\Sexpr{path.11.gg2}}
%\end{figure}

\subsection{Regularization}    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Regularizing models is one way to avoid overfitting and thereby improve
prediction accuracy. So far the models are all linear and are not
regularized.

To assess the extent to which regularization leads to more accuracy, we
take the model using the 20 best features and run multiple versions using
several choices for the weight of an L2 regularizer. A simple grid
search was used. 

% non-rotated txt chart
\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.12.txt}}
\caption{Estimated Generalization Error from L2 Regularization}
\label{figure:Regularize3}
\end{figure}

% including a pdf \begin{figure}[h]
\begin{figure}[ht]
\caption{Estimated Generalization Error from L2 Regularization }
\label{figure:Regularization1}
\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg1}}
\end{figure}

Figure \ref{figure:Regularize3} contains the result in a table. Figure
\ref{figure:Regularization1} has the same results graphically. We see
the typical pattern, small values for the regularizer importance improve
estimated accuracy and larger values decrease accuracy. Setting the
weight to $\lambda = 4$ seems like a good choice: it has the lowest
median across the cross-validation folds and the 95 percent confidence
intervals are not signally warnings.


%% including a pdf
%\begin{figure}[h]
%\caption{
%Estimated Generalization Error from L2 Regularization
%\\
%Not Showing Origin
%}
%\label{figure:Regularization2}
%\centering\includegraphics[scale=.4]{\Sexpr{path.12.gg2}}
%\end{figure}
%

\section{Submarket Models}

We considered two approaches to incorporating submarkets into the model.
The first uses indicator variables, the second builds a separate model
for each submarket.

\subsection{Using Indicator Variables}

The indicator model all have the form

% ref: Gratzer page 212 (splitting long formulas)

\begin{multline}
\log(price) = \\
\beta_0 + \beta_1 ZipIs11111 + \beta_2 ZipIs22222 + \ldots \\
+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots
.
\end{multline}

The indicators are capturing the possibly different average prices in
each submarket. The average price per feature is the same across
submarkets.

\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.indicator}}
\caption{Estimated Generalization Error Using Submarket Indicators}
\label{figure:Indicator}
\end{figure}

Figure \ref{figure:Indicator} shows the estimated generalization errors
from adding indicators variables to the regularized linear model. We see
that adding indicators for zip codes, census tract, and cities all help
and that the best result is obtained with the five-digit zip code
indicators.

\subsection{Separate Models for Each Submarket}

The separate-models approach builds a separate model for each submarket.
Each model has the form

\begin{multline}
\log(price) = \\
\beta_0 \\
+ \beta_k LotSize + \beta_{k+1} LivingArea + \ldots
.
\end{multline}

A query transaction is presented, its submarket is determined, for
example, it is in zip code 11111, and then a model is trained using only
transactions in the prior 60 days in the zip code. This fitted model is
used to estimate the value for the query transaction. 

This fitting process results in many models that cannot be fit, most
often because there are too few transactions in the training set. 


\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.submarkets}}
\caption{Estimated Generalization Error Using Submarket Models}
\label{figure:Submarkets}
\end{figure}


Figure \ref{figure:Submarkets} shows the results of this experiment. The
submarket models that used city names had the lowest estimated
generalization errors, but the fitting process suceeded in only 69
percent of the city names. (We say the coverage was 69 percent.)
Coverage was higher for the zip code-based submarkets, but estimated
performance was worse than for city-based submarkets. The census tracts
are the worst of both worlds: the highest expected error and the lowest
coverage.

Coverage numbers could be increased most likely by either retraining the
models on more than 60 days of data or forming submarkets groups by
splicing together some of the submarkets. The latter approach has been
tried in the literature. The former appears to have not been tried.

The apparently pleasing results in Figure \ref{figure:Submarkets} hide
the fact the what is reported is the median across all the various
models. Let's focus on the city-defined submarkets. There are 191
separate cities. The median errors for each city vary widely. So, for
example in the city of Los Angeles, the median error is about \$100,000,
while in the city of Norwalk, the median error is about \$25,000. One
interpretation of these results is that prices are much harder to
estimate in some submarkets than in others.

\section{The best linear model is \dots}

Based on time period from 2003 through the first part of 2009, the best
linear model would predict $\log(price)$ from the feeatures in natural
units, use the 20 features we identified, be regularized, be fitted on
60 days of data, and created for each submarket as defined by the city
name. These design choices are in effect hyperparameters for the linear
models.

Commercial model builders should consider being more aggressive in selecting the
hyperparameters than we have been here. Rather than select them once for
the entire time period and for all submarkets, they should select
hyperparameters for each submarket based on very recent transactions
instead of the longer time period studied here: we are interested in
models that work on average, most commercial price estimators are
interested in models that work on tomorrow's transactions.

\section{Coda: A Non-Linear Model, Random Forests}

Every study that we reviewed that compares linear to non-linear models
claims that non-linear models generally outperform linear models when
predicting house prices. We verify that claim in our setting by
developing a number of random forest models by sweeping these design
choices:

\begin{itemize}
\item The feature set used: we testing both all features and the 20
features that were best for the linear model. 
\item The number of trees in the random forest: more trees should lead to
better results, probably with a tailing off in performance at some
point. We tested 1, 100, 300, and 1000 trees in each random forest.
\item The number of features to try when adding a new leaf
to the tree. The implementation of random forests first tests that
number of random-selected features to add and then adds the one giving
the best greedy performance. We test 1, 2, 3, and 4 features.
\end{itemize}


\begin{figure}[ht]
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.13.rf}}
\caption{Estimated Generalization Error Using Random Forests}
\label{figure:Submarkets}
\end{figure}

Figure \ref{figure:Submarkets} shows there results of this experiment.
As expected, increasing the number of trees (column \code{ntree}) and
increasing the number of features tested when adding a leaf (column
\code{mtry}) both always improved prediction accuracy. The model in
which the available feature set was restricted to the 20 features found
to be best for the linear model greatly underperformed the otherwise
equivalent model allowed to select from all features. This is logical:
why should taking away choice improve prediction?

<<random>>=
best.lin <- 39557
rf       <- 35782
ratio    <- best.lin / rf
ratio
@

Finally, the best of the linear models which work on the entire market
is the zip-code based indicator model. It's expected generalization
error is \$39,557 with a 95 percent confidence interval of $[\$36,500;
\$43,108]$. The expected generalization error from the best random
forest model is \$35,782 with a 95 percent confidence intervale of
$[\$32,976; \$37,871]$. The best linear model tested is about 10 percent
worse than the random forest model tested. 

Both can be improved. The linear model can be improved through including
the GPS coordinates and probably through many other means. How do design
these improvements is not obvious. The random forests model can be
improved simply by increasing the two hyperparameters and by adding
features.

