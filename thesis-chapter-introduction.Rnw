\chapter{Introduction}
% vim: textwidth=72

Many parties are interested in accurate predictions of the market
value of residential real estate. Buyers and sellers have a clear
interest in setting prices relative to market values. Mortgage
originators, who use the property as collateral for the loan, want to
know the extent to which the borrower starts with and continues to have
equity in the house.  Investors in mortgages want to know the current
value of the property relative to the current debt on the property.
Local governments use market values in part to set real estate
taxes.

With so much interest, it is not surprising that a business has arisen
in predicting market values of residences. The structure of this
market comprises a few large national players and many regional
players. The players regard their algorithms and their data
as sources of competitive advantage.

Academic studies of real estate pricing models have been limited by data
availability and the absence of benchmarks from the commercial players.
Most academic studies are generated around a data set for one city for
one year. Often the data are licensed only to specific scientists.


This work systematically compares linear models for predicting prices of
single-family residential real estate in Los Angles County over a time
period that precedes and follows the real estate crash. The data are
from CoreLogic, one of the providers of both the data and algorithms
for real estate price prediction. As for other academic work, the data
are proprietary to the study. However, the source code for the work is
licensed under the GPL and is available on the author's Github account.
It is written in R and its models are designed to work on any data set.

We develop a few insights into linear models of real estate prices. All
these findings are for Los Angeles County residential real estate
transactions between 2003 and the first part of 2009.

\begin{itemize}

\item A lot of data doesn't help a lot. Linear models price houses by
pricing each feature of the house. When
house values are changing, the hedonic nature of the linear model must
mean that feature values are also changing. While training a model on a
longer time period will smooth noise, it will also use older
feature prices that are not necessarily relevant to feature prices at
the time of the query. We find an optimal time period is roughly 30 days,
considering both time periods before and after the real estate crash.

\item One might suppose that a very valuable feature would be the value
of the house as estimated by the real estate tax assessor. However, we
found that during the crash, the value of the tax assessment was
negative when used in simple linear models.

\item ``Location, location, location'' are said by some real estate
agents to be the most important features of a house. We find that isn't
true, at least for linear models. The most important feature of the
house is its interior living space. The second most important feature
of the house is a feature related to location: the median income of the
census tract holding the house.

<<IntroFeatures, include=FALSE>>=
# error for given number of features
error.24 <- 58862
error.15 <- 58571
error.6  <- 59198
error.1  <- 79332
count.parsimony.features <- 6
count.min.features <- 15
count.num.features <- 24
p.6.to.15 <- 1 - (error.15 / error.6)
p.6.to.15
p.15.to.6 <- 1 - (error.6 / error.15)
p.15.to.6
@

\item One would hope that a few features of houses would contain most of
the information that is driving prices. If so, models could be simpler.
We found that the minimum prediction error was
\$\Sexpr{Commas(error.15)}, when \Sexpr{count.min.features} features
were used.  The best parsimonious model has
\Sexpr{count.parsimony.features} features and an error of
\$\Sexpr{Commas(error.6)}, a \Sexpr{Percent(-p.15.to.6)} percent
increase from the minimum.

\item Linear models are popular in part because they are relatively easy
to understand and quick to fit. However, they tend to underperform
non-linear models. In this work, we examine local linear models. In a
local linear model, a separate linear model is fit to every query transaction,
resulting in a non-linear model. In our experiments,  a carefully designed
local linear model was outperformed by an off-the-shelf random forests
model.

\end{itemize}

The remaining chapters are these. Chapter 2 contains a review of the
literature, focusing on other studies of real estate price prediction
models. Chapter 3 describes how the Los Angeles data were ingested,
cleaned up, and converted into transactions. We ended up with about 1.2
million transactions mostly dated from 1984 through the early part of
2009.  Chapter 4 describes our work to find a subset of the Los Angeles
data that is both informative for prediction purposes and does not
contain any future information.  Chapter 5 identifies the best local linear
models, at least according to our experiments on these data. It also
describes a random forests model that performs better than the best of
the local linear models, when trained on the same data.  Chapter 6
presents overall conclusions that lead to future work I'd like to do.

%\layout    % see Gratzer p 268

