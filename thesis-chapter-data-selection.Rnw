\chapter{Data Selection}
% vim: textwidth=72
% vim: foldmethod=manual

<<DataSelection>>=
working <- control$working
path.cv.assessment <- paste0(working, 'e-cv-chart_chart5.txt')
path.cv.census <- paste0(working, 'e-cv-chart_chart6.txt')
path.verify <- paste0(working,'e-verify-assessment-chart_chart1.pdf')
path.price.chart2 <- paste0(working, 'e-price-chart_chart2.pdf')
@

We'd like to investigate a range of real estate price prediction
models over as many years as possible. Most academic studies of real
estate prices cover just one year of transactions. Unfortunately, the
overlap in time periods for our data is small.
\begin{itemize}
\item The deeds file contains deeds for 1984 through the first part of
2009. Deeds from a few bonus years before 1984 are thrown in.
\item The taxroll file is for 2008. It was created in late 2007. It
contains property descriptions as well as the tax assessor's estimated
value for the house. It also contains the census tract number for each
house.
\item The census file contains data from the decennial census file in
year 2000. It became available to the public sometime in 2002.
\end{itemize}

Thus, the common time period is starting in 2008 and extending into a
few months in 2009.

This time period can be extended to earlier time periods if we can
concluded that the tax assessment does not carry much predictive value.
If that is so, we can simply not use features derived from the tax
assessment, and extend the analysis back to 2003, when the year 2000
census data became available.

Furthermore, we can extend the time period back before 2003, if the
census data are also not valuable for predictive purposes.

To determine whether feature sets are valuable for predictive purposes,
we employ a cross validation and model-testing process using the training data.

The remainder of this chapter is sectionalized.

\begin{itemize}
\item Section 1 describes the cross validation process and how models
are fitted and use for prediction.
\item Section 2 provides an overview of the real estate taxes in
California.
\item Section 3 answers the question: Are the tax assessment-derived
features valuable for predicting prices?
\item Section 4 provides an overview of the U.S. decennial census and
the year 2000 census in particular.
\item Section 5 answers the questions: Are the census-derived features
valuable for predicting prices?
\item Section 6 provides a brief summary of the findings: the tax
assessment-derived features are not valuable and the census-tract
derived features are valuable.
\end{itemize}

\section{Cross Validation and Model Fitting}

We will be comparing a large number of models with the goal of selecting
the models that are best for predictive purposes. In this work, ``best''
means that the model provides the lowest estimated generalization error,
which is defined to be the estimated error on data that the trained
model has never seen.

The estimate the generalization error, we use 10-fold cross validation,
as described in \cite{hastie-01} Chapter 7 (starting page 214). Our
implementation of cross validation assigns each of the training samples
randomly to one of 10 folds and then trains 10 sets of models. Each set
of models sees 90 percent of the data as training data and the remaining
10 percent as testing data. This is the standard 10-fold cross
validation approach: for each fold, we have a training data set (with 90
percent of the data) and testing data set (with 10 percent of the data).

For each of the samples in the testing set, we then build a local model
just for that sample, which we call the query transaction. To avoid
using the future in the training process, the local model first discards
all data in the training set that occurs on or after the date of the
query transaction. A model is then trained on remaining data in the
training fold. The trained model is then used to predict the query
transaction's price and the results are recorded. After all query
transactions in all folds are recorded, the accuracy of the predictions
is assessed.

There are two optimizations both motivated by potentially long
computational times in the fitting process for numerous query
transactions. The first optimization caches trained models for a
particular fold that have the same query date. All models with the same
query date are fitted to the same training data because the training
process does not use the query transaction or any data on or after the
data of the query transaction. This optimization is always used, because
it doesn't change the results.

The second optimization samples the potential query transactions in the
testing set from a fold. We do this because of the potentially large
size of the testing sets: Los Angeles County is very large and we are
testing potentially over many years of transactions. This optimization
is used often when multiple years of data are studied.

\section{Understanding the Real Estate Tax Assessment}

% including a pdf
\begin{figure}[h]
\caption{
Fraction of Property Sales for Exactly Their 2008 Assessments \\
By Recording Date Month
}
\label{figure:AssessmentExactlyEqual}
\centering\includegraphics[scale=.4]{\Sexpr{path.verify}}
\end{figure}

In the United States, local government is often financed in part through
real estate taxes \cite{wiki-14-property-tax}. The taxing authority,
often a county, creates a tax assessor who values the land and the
improvements on the land. The resulting assessed values are used to
determine the assessment, the amount of taxes due. For example, the
assessment may be calculated as a fraction of the total assessed value
of the land and its improvements.  The assessed value may be the market
value or a stated fraction of the market value or, as in California,
have some other relationship to market values.  How often the
assessments are carried out depends on the taxing authority.

The tax assessment is possibly a useful feature to know when creating
real estate valuation models. Figure \ref{figure:AssessmentExactlyEqual}
depicts the fraction of residential properties recorded in certain
months in Los Angles County relative to what is called the 2008 tax
assessment.  We see that in the period July, 2007 through December,
2007, many of the properties, about 90 percent, sold for exactly their
assessed values. This did not happen in other periods. How did this
happen?

The answers lies in the nature of the property tax system in Los Angeles
County. According to the website of the Los Angeles Country Office of
the Assessor \cite{lac-14-real-property-assessments}, the Assessor
determines the assessed value for property, the Auditor-Controller
applies the appropriate tax rate for the property, and the Treasurer \&
Tax Collector mails out the bills and collects the money.

California Proposition 13 \cite{californiataxdata-14-prop13}, passed in
1978, changed the relationship between market values and assessed
values. Assessments were reset to their 1976 levels.  Increases from the
1976 level were limited to two percent per year, unless the property
sold (with a few exceptions). Properties that sold were assessed at
their selling prices.

%Our focus is on the assessed value. In
%this section, we further focus on the total assessed value, the sum of
%the assessed values for the land and any improvements (buildings and
%structures) on the land. California law defines the assessed value using
%the market value as a starting point. Several major cases are recognized (minor cases
%are not documented here):
%\begin{itemize}
%\item The property sold or was inherited. The assessed value becomes the
%market value. There are exceptions for transfers between husband and
%wife and domestic partners and a few other situations. The date for the
%adjustment is the recording date, not the sale date.
%\item New construction was completed on the property. The improvement
%value is increased to reflect the value of the new construction. The
%value of the improvement comes from construction permits.
%\item The property neither sold nor had completed new construction. This
%is the most interesting case. The previous assessed value is adjusted to
%reflect movement in market values, however the adjustment is never more
%than two percent per annum, no matter the rate of inflation.
%\end{itemize}


The net effect is the that Assessor's total assessed value is below the
market value in periods with inflation of more than two percent for
homes that did not sell in the previous year.

In addition, California Proposition 8, also passed in 1978
\cite{caboe-14-prop8} provides for a temporary reduction in assessed
values when the value of a property falls. The effect is to reinforce
that total assessed values can be below market values.

One final note on timing. The tax bills are mailed between October 1 and
October 31 \cite{lac-14-important-dates-for-homeowners} with initial
payment due on November 1. The fiscal year for Los Angeles Country
begins July 1.

Now we have the background to hypothesize an explanation for the many
zero-error points in the second half of 2007 in Figure
\ref{figure:AssessmentExactlyEqual}. The Assessor sets the assessment
for 2008 equal to the transaction prices for properties transacted in
2007 starting in July. Because property values were changing rapidly in
around the time that the tax bills were sent out, properties transacting
before July in 2007 were assessed at some value other than market value. The figure
reflects recording dates, which tend to lag sales dates by a few months.
Thus the properties reported as transacting in November and December in
many cases would have transacted a few months early, say ending in
roughly October. The last day for publishing tax assessment is October
31.

\section{Testing the Predictive Value of the Assessment}

We turn now to a key question: to what extent is the assessed value
useful in predicting real estate prices when using linear models?

To answer this question, we compare two linear models that are otherwise
identical except for the feature sets used. One model uses the tax data,
one does not. In order to assess which of the two models is better, we
estimate the generalization error from each model using 10-fold cross
validation.

The common feature set across each model contains all the features that
were present in every transaction of subset 1. The features were
structured into six groups along two axis. The first axis is the source
of the feature: from the tax bills, from the U.S.
Census Bureau, or from the deeds file. The second axis is whether the
feature describes the size of the property: yes or no.

% ref: Predictors2.R
These two axis results in six feature sets.
\begin{itemize}

\item From the tax bills presented in late 2007.
  \begin{itemize}
  \item Size features: improvement value, land value.
  \item Non-size features: fraction improvement value (ratio of the
  improvement value to the sum of the improvement and land values).
  \end{itemize}

\item From the U.S. Census Bureau decennial census in year 2000.
  \begin{itemize}
  \item Size features: there are no such features.
  \item Non-size features: average commute time; fraction of houses that
  are owned by the occupants; median houshold income; whether the census
  tract has a park, retail stores, a school, and industry.
  \end{itemize}

\item From the deeds file from the years 1984 through 2009.
  \begin{itemize}
  \item Size features: land square footage, living area,
  basement square feet, number of bathrooms, number of bedrooms, number
  of fireplaces, number of parking spaces, number of stories, and number
  of rooms.
  \item Non-size features: effective year built, year built, whether
  there is a pool, whether the house is newly constructed; whether the
  five-digit zip code has a park, retail stores, a school, and industry.
  When constructing the models, the year and effective year built
  features are converted to age, age squared, effective age, and
  effective age squared using the sale date from the query transaction.
  This conversion is made because age and age squared are popular
  features in the literature. The idea is that houses should depreciate
  for a while and then gain in value as they become classics.
  \end{itemize}

\end{itemize}

In addition to varying the feature set, we varied other design choices
in linear models:
\begin{itemize}

%\item Business scenario for the model. We tested both the AVM and
%mortgage scenarios. In the AVM scenario, the goal is to predict the
%price on the next day and the training data is all the data before the
%query date of the transaction. In the mortgage scenario,the goal is to
%predict the value at some date in the past and the training data is all
%the data excluding the query transaction and hence including
%transactions after the query date. One application for the mortgage
%scenario is to determine the value of a house on every day as input to a
%mortgage default prediction model. The training data for a default
%prediction model may include the value of the house and the value of the
%mortgage on specific dates.

\item Response variable. One can choose to predict the price of the
house or the logarithm of the price. One might choose to predict the
logarithm in order to reduce the range of outcomes to be predicted.
House prices in Los Angeles County are not uniformly distributed across
a range. The log transformation hopes to put the prediction variable on
a more compressed scale and hopes that the training regime results in
better predictions on average by reducing the importance of errors for
very expensive properties.

\item Prediction variable forms. One can choose to predict using the
natural units for the predictor features or by transforming some of
those features into the log domain. At least three ideas might lead one to
transform features into the log domain. One idea is simply to compress a
range of larger values into smaller values. A second idea would be to
transform the input features into a distribution that allowed one to
more easily calculate error bars on resulting predictions. A third idea
is to reflect a hypothetical data generating process: it may be that the
response variable linearly with a percentage change in the predictor
variable or with a unit change in the predictor variable. When the
percent-change model is true, transforming the feature into the log
domain is appropriate. Otherwise, keeping the feature in its natural
units is appropriate. Computationally, the size variables are
partitioned into those that may be zero and those that are always
positive. Those that may be zero are transformed into the log domain by
adding one and taking the log. The positive variables are transformed by
taking the log.

\item Number of days of training data. Prices were moving rapidly
downward in 2007. With linear models, using a longer period for the
training data runs the risk of irrelevance of the training data to the
query date. If prices were stable, more training data could provide a
more accurate average price.

\end{itemize}

% rotated tex
% ref: tex.stackexchange/com/questions/44427/rotate-picture-with-caption

% attempt to rotate: prints outside of margins
%\begin{sidewaysfigure}[ht]
%\caption{Caption for txt}
%\label{figure:AssessmentTxt1}
%%\tiny
%\small
%%\normalsize
%\verbatiminput{\Sexpr{path.cv}}
%\end{sidewaysfigure}

% attempt to rotate: prints outside of margins
%\begin{sidewaystable}[ht]
%\begin{centering}
%\caption{Rotation attempt 2}
%\label{figure:AssessmentTxt1}
%\tiny
%%\small
%%\normalsize
%\verbatiminput{\Sexpr{path.cv}}
%\end{centering}
%\end{sidewaystable}

% attempt to rotate: works (though with small text)
\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.assessment}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors With and Without the Assessed
Value}
\label{figure:CvAssessment}
\end{figure}

% unrotated file: prints outside of margins
%\begin{figure}[ht]
%\caption{Caption for txt}
%\label{figure:AssessmentTxt1}
%%\tiny
%\small
%%\normalsize
%\verbatiminput{\Sexpr{path.cv}}
%\end{figure}

Figure \ref{figure:CvAssessment} contains the result from the cross
validation study comparing a linear model with all combinations of the
design choices. Column one names the scenario: AVM or mortgage. Column
two tells whether the model predicted price or the logarithm of price.
Column three tells whether the size features were transformed into the
log domain (``log'') or left in their natural units (``level''). Column
four states whether the 2008 assessment features were used or not. The
remaining columns vary the number of days in the training period from 30
up to 360 days. Each data cell contains the median of the RMSE values
from 10-fold cross validation. Here RMSE means the square root of the
median square errors.

The models used in the cross validation study are local to each possible
query point. This means that a separate model is built and trained for
every query point. The implementation optimizes the computation by
memoising the fitting process based on the sale date. Thus at most one
model is fit for every sale date.

% including a pdf
\begin{figure}[h]
\caption{
Median Price by Month \\
2006 - 2009
}
\label{figure:PricesByMonth2006On}
\centering\includegraphics[scale=.4]{\Sexpr{path.price.chart2}}
\end{figure}

The key conclusion from the figure is that using the 2008 tax assessment
as a feature never reduced the estimated generalization error. Examining
Figure \ref{figure:PricesByMonth2006On} shows why: median prices in 2008
fell every month, so that the assessed values became increasingly
obsolete.

Thus we reach an unexpected conclusion: the assessed values are not
necessarily useful in linear models. One reason is that in California,
Proposition 13 biases the assessed values to be less than market values.
Another reason is that the assessment may become obsolete if market
values are either increasing or decreasingly rapidly.

\section{Understanding the Census Data}

The United States Census Bureau conducts
\cite{census-bureau-14-website} a decennial census every 10 years. The
year 2000 census (\cite{census-bureau-12-measuring} page 140) began in
March 2000 to survey all 98 million households. The survey technique was
to mail surveys and follow up with non-respondents by sending
``enumerators'' to the household. 

Two survey forms were used in 2000.
\begin{itemize}
\item Short-form: Most households (83 percent) received a questionnaire
asking for information on ``name, sex, age, relationship, Hispanic origin,
and race.''
\item Long-form: Some houesholds (17 percent) received a questionnaire
that asked for both the short-form information and an additional ``52
questions requesting \dots information about housing, social, and
economic characteristics of the household.''
\end{itemize}

Data are captured for each household and then aggregated for reporting
purposes. The lowest level of aggregation is the census block. Data from
census blocks are futher aggregated into several hierarchies
(\cite{census-bureau-10-geodiagram}). The hierarchy of interest to this
work has these levels: nation, region, divisions, states, counties,
census tracts, block groups, and census blocks. All of the census data
we use are from the 2000 census for Los Angeles Country and are at the
census tract level.

Unfortunately, boundaries between census tracts in Los Angeles changed
between the 2000 and 2010 censuses (\cite{proximityone-14-tracts}).
That's because census tracts are designed to contain about 4,000 people:
as populations grow and shrink, census tracts need to be redefined.
Indeed, an analysis of Los Angeles Country census data at the census
tract level for years 1990 and 2000 shows an increase in the number of
census tracts in Los Angeles County.

Unfortunately, the census tract numbers for properties are contained in
the taxroll, which we have only for 2008. Thus we cannot easily
determine census tract numbers for parcels for the 1980 nor 1990
censuses. The implication is that the earliest date we can use the
census data we have is for the date when it first became available.
Rather than determine that date exactly, we have assumed that
transactions on or after January 1, 2003 properly reflect census data
from the year 2000.

\section{Assessing the Predictive Power of the Census Data}

To assess whether features derived from the year 2000 census were
valuable, we used 10-fold cross validation to estimate the
generalization error for a variety of linear models, all trained and
tested on data from 2003 on. The models varied the same design choices
as for the experiments to understand the predictive power of features
derived from the 2008 tax assessment. These choices were:

\begin{itemize}
\item response variable: predict either the price or the logarithm of
the price.
\item prediction variable forms: use as predictors both the natural
units of the predictors (called ``level'' in the chart) or the logarithm
of the natural units.
\item number of training days: vary the training period from 30 days to
360 in steps of 30 days.
\end{itemize}

Figure \ref{figure:EGECensus} shows the estimated generalization error
for each of the design choices, both with and without the features
derived from the census data. The comparison metric is the median of the
root median squared errors from the folds. The feature set used was the
set of features appearing in every transaction. Some observations:

\begin{itemize}
\item Using the census-derived features always leads to better
performance than not using the census-derived features.
\item The best-performing model predicts the log price using the level
features and training for only 30 days. The relatively short training
period was a surprise to me. That it is so short suggests that prices
were changing rapidly since 2003.
\end{itemize}

\begin{figure}[ht]
\begin{sideways}
\begin{minipage}{7in}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{path.cv.census}}
\end{minipage}
\end{sideways}
\caption{Estimated Generalization Errors With and Without Census Data}
\label{figure:EGECensus}
\end{figure}

\section{Discarding Assessments and Keeping Census Data}

Because the assessment data were not valuable for the time period for
which they were relevant, namely the year 2008, we decided to discard all
features derived from the assessment. This reduces our available feature
set and allows us to use transactions before and after 2008.

Because the census data were valuable for the time period in which they
were relevant, namely beginning in 2003, we decided to keep these
features and restrict the analysis year 2003 and later.









