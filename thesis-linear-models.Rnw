% vim: textwidth=72
% vim: foldmethod=manual



% STYLE GUIDE
% not dataset, but data set
%\input{thesis-prologue}

\documentclass[10pt]{amsart}
%\documentclass[10pt]{amsbook}
%\setlength{\textwidth}{5in}

\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
%\usepackage[all]{xy}   %xy-pic

% graphics
\usepackage{graphicx}    % needed for \includegraphics
% Tell \includegraphics where to search for files
% see Gratzer p 317
% The trailing / is required
\graphicspath{%
{/home/roy/Dropbox/nyu-real-estate/repp-repo.git/src/local-weighted-regression-2/src/}
}
\usepackage{epstopdf}   % allow eps files as graphics input
\usepackage{caption}    % allow line breaks \\ in captions

% don't indent first list of a paragraph
%\setlength{\parindent}{0pt}   
% increase spacing between paragraphs
\setlength{\parskip}{1ex plus 0.5ex minus 0.2 ex}
% don't align right margin
\raggedright

% proclamations
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\newtheorem{notation}{Notation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem]{Theorem}

%%%%%% commands local to this document
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\term}[1]{\emph{#1}}
\newcommand{\blanks}{\_\_\_}
\newcommand{\blank}{\textunderscore\textunderscore\textunderscore}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\title{Linear Models}
\author{Roy E. Lowrance}
\date{\today}
\email{roy.lowrance@gmail.com}
\maketitle

<<control>>=
options(warn = 2)  # turn warnings into errors
source('DirectoryWorking.R')
Control <- function() {
    working <- DirectoryWorking()
    control <-
        list( path.in.transactions = paste0(working, 'transactions-al-sfr.RData')
             ,path.in.subset = paste0(working, 'transactions-al-sfr-subset1.RData')
             ,path.in.deeds = paste0(working, 'deeds-al.RData')
             ,path.in.parcels = paste0(working, 'parcels-sfr.RData')
             ,path.in.median.price.by.month.pdf = 
                paste0( working
                       ,'e-median-price-by-month-from-2006-to-2009.pdf'
                       )
             ,path.in.median.price.by.year.pdf=
                paste0( working
                       ,'e-median-price-by-year-from-1984-to-2009.pdf'
                       )
             #,cache = FALSE
             ,cache = TRUE
             ,include = TRUE
        )
    control
}

control <- Control()
str(control)
opts_chunk$set(cache = control$cache)
opts_chunk$set(include = control$include)
@

<<Commas>>=
Commas <- function(i) {
    # insert commas into an integer
    format(i, big.mark = ',')
}
# test
Commas(123456)
@

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

From this simple idea, a huge variety of models can be generated by
varying the features $x_i$, by applying transformations to the outcome
$y$ or feature $x_i$, by considering different scenarios for using the
result model, by varying the training periods, by considering whether to
regularize, and many other factors.

Here we investigate the accuracy of linear models resulting from various
decisions. 


\begin{itemize}
\item Kinds of models
\item Model form
\item Characteristics of the training and testing periods
\item Selecting test data sets: randomly and for 2009
\item Choosing the training period
\item Feature choice
\item Training set choice
\item avm variants experiment
\item regularization
\end{itemize}

\section{Kinds of models: Scenarios}

Real estate price prediction takes place in three scenarios (business
use cases). These differ largely in the business purpose of the estimate
of market value and the training data that can be used.

\subsection{Assessor Scenario}

The first of these is the ``assessor'' scenario. The tax assessor for
Los Angeles County issues tax bills in November of every year. The first
installment is due later in that year and the second installment is due
in the following year. We have the tax roll for 2008, which was used to
generate tax bills that were issued starting November 1, 2008.

The tax bill contains the tax assessor's assessment for the property.
Were it not for California Proposition 13, the assessment would be for
the market value of the property sometimes in 2008. The valuation as-of
date is vague. Because of proposition 13, the assessed value is less
than the market value in periods of high real estate inflation.

Although the tax assessor does not disclose the market value, this value
must be determined, because the assessed value is required to be the
maximum of the market value and value imputed from the last sale
transaction increased at a rate of at most two percent per year.

The ``assessor'' scenario is an assessment of the market value in 2008.
As training data, the assessor can use all of the transactions recorded
before November 1, 2008. Note that the average delay between the sale
and recording of the sale is about 60 days.

\subsection{AVM Scenario}

The second scenario is the ``automated valuation model (AVM)'' scenario.
This scenario arises because many actors need to know the value of the
property before it transacts. Such actors include sellers, buyers, and
lenders on the property. There is a business in providing the price
predictions, which are called AVMs. Here ``automated'' means created
through a computer program to distinguish the estimates from those
created by human supposed experts using data such as the deeds
data.

AVM providers are incentive to have accurate estimates, so there is no
reason to build in systematic bias in their predictions. The can use all
as training data all the sales that are recorded up to date of the
predition. For example, to predict the market value of properties on
January 15, 2009, the AVM providers can use all the data reported up to
and included January 14, 2009. They can also use the tax assessor's
assessment as a feature. The assessment is by law partitioned into the
assessment for the land and the assessment for the improvements on the
land.

\subsection{Mortgage Scenario}

The third scenario is the ``mortgage'' scenario. This scenario arises in
order to train mortgage-default prediction models. 

Borrowers default on their mortgage by not making the payments. After a
certain number of missed payments, the lender may begin legal
proceedings to seize the property, which is collateral for the mortgage.
The process varies by state.

There are two major reasons for not making mortgage payments.
Historically the major reason was that the borrower could not afford to
make the payments, perhaps because of loss of income. If the house is
worth more than the mortgage and the real estate market is operating
quickly, rather than default, the borrower may be able to sell the house and use
the proceeds to pay off the mortgage and so that default would not occur. 
The sell-and-pay strategy does not work if the value of the house less
its selling costs is less than the amount owed on the mortgage. In this
situation, the house is said to be under water. In the 2007 United
States real estate crisis, many houses were under water.

The second reason for not making the payments was historically rare but
became more frequent with the 2007 real estate crisis: the borrower
had the cash to pay but decided to not pay. These ``strategic defaults''
are in part driven the house being under water.

To build a predictive model of defaults, the feature set often includes
the value of the house and the amount owed on the mortgage. In the
mortgage scenario, the value of the house is estimated in every relevant
time period. One is mimicking the borrower's estimate of the value of
the house.

In making the mortgage prediction for January 15, 2009, one can use all
the historic transactions, all the transactions on the date of the
estimate, and all the future transactions as well. One can also use any
transaction on the query house itself, as one is estimating what the
borrower would think the house is worth. An available feature is the
tax assessor's assessment.

\section{Model form}

Our linear model

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

allows the variables to be transformed before entering the equation.

Two popular choices for the response variable $y$ are to set it to the
$price$ or the $\log(price)$. Some researchers including Bourassa
(\cite{bourassa-02} p. 11) transform the price into the log domain as a
means to make the resulting distribution ``more normal'' claiming that
doing so ``helps to normalize the distribution of the error term.'' But
does this transformation improve predictive accuracy? That question is
investigated below.

Another choice is to transform some of the features into the log domain.
One might do so when the price is in the log domain, so that the
regression equation in part becomes

\[ log(price) = \ldots + \beta log(x) + \ldots  .\]

The idea of this transformation (cite the econometrics books) is that a,
say, 10 percent change in the value of x would lead to a 10 percent
change in the value of the price. These transformation are used in
Chopra's work (cite his thesis), where all size-related features were
transformed into the log domain. The intuition is that doubling the size
would double the value.

We consider 4 forms of models and ask which provides the most accurate
predictions. The form and their interpretations are:

\begin{itemize}
\item level-level: $price = \ldots + \beta x_i + \ldots.$ A doubling
of x leads to $\beta$ factor increase in the price.
\item level-log: CHECK THE TEXT
\item log-level:
\item log-log
\end{itemize}

\section{Choosing the training period}

The linear model uses the average value of each feature to estimate the
total value of the house. When prices are moving rapidly, feature values
may also be moving rapidly. It's important to think through the training
data set for a given estimation task.

\begin{figure}[h]
\caption{Median Prices By Year}
\label{figure:medianPricesByYear}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.median.price.by.year}}
\end{figure}

In Figure \ref{figure:medianPricesByYear}, we see the median prices of
arms-length transactions for single-family residences in our subset1.
You can see that prices have not been steady and that there have been
periods of steady price increases followed by periods of price declines.
The decline in prices after 2007 was not the only price decrease: there
was another decline starting in 1990 that was sustained for about six
years. 

The impliciation for linear models is that the model builder faces a
dilema: train on too long a period, and the features-level prices
found by the fitting process for the model are likely to be out-of-date;
train on too short a period, and the volume of training data may be too
low to accuralate fit to feature prices.

One possible way around this dilema is to successfully forecast median
prices for the test period: will they be lower, about the same, or
higher than in the training period. The trained feature prices could
then be adjusted. However, such a forecast is beyond the scope of this work.

\begin{figure}[h]
\caption{Median Prices By Month}
\label{figure:medianPricesByMonth}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.median.price.by.month}}
\end{figure}

Figure \ref{figure:medianPricesByMonth} depicts median prices by month
starting in January 2006. We see approximately level prices starting
early in 2006, a spike in mid July 2007, and then the well-known
decline. The difficulty of forecasting future median prices can been
seen: What is the clue that July 2007 would be the peak? Median prices
increased in February 2008 but declined to a new low in March 2008.


\section{Feature choice}

Features used in the literature.

\subsection{bourassa-02}

paper: bourassa-02 Do housing submarkets really matter? pages 10 and 11.
Estimated log(price). These variables were found to ``contribute
explanatory power to the estimated equations:''

features used in hedonic (linear) models
\begin{itemize}
\item intercept
\item log(floor area)
\item log(land area)
\item cross-leaded or strata-titled (a form of ownership)
\item detached dwelling
\item age of dwelling
\item age of dwelling squared
\item wall condition: good, average, or bad 
\item roof materials: tile, metal, or other
\item wall materials: wood, brick, fibrolite, or other 
\item quality of the principal structure: superior, average, or poor 
\item log(distance to the central business district)
\item quarterly time-period dummy
\item water view
\item modernization
\item landscaping: good, average, or poor
\item driveway
\item neighborhood: very good, good, average, or poor
\item number of attached garages
\end{itemize}

age of dwelling and age squared (because the value is expected to
follow a U-shaped curve ``because very old houses can earn a premium do
to their historic character and distinctive neighborhood.''

Why land area and floor area in log domain? ``to reflect the likelihood
of diminishing returns [to price] as ... the values of those variables
increase in size.''

Use log(distance to CBD) ``in view of theory and empirical evidence."

Estimate log(price) ``as a means for making its distribution more
normal, which in turn helps to normalize the distribution of the error
terms, a desirable characteristic for ordinary least squares
estimators.'' Example of a statistical consideration that may or may not
improve model accuracy.

Evaluation criteria: fraction of out-of-sample predictions that are
within 10 and 20 percent of the trueprices.

Predicted prices were adjusted ``for the average residual for the
neighborhood in which the property is located.'' A neighborhood was
defined as a portion of the tax roll containing about 1,000 properties.

\subsection{bourassa-10}

Predicting house prices with spatial dependence: a comparison of
alternative methods

Primary criterion: percentage of ex-sample predictions with 10 percent
of the transaction price

Dependent variables is log of sales price.

features used in OLS models
\begin{itemize}
\item intercept
\item land area
\item land area squared
\item floor area
\item bathrooms: no more than 1, 1.5 or 2, 2.5 or more
\item age of house
\item age of house squared
\item basement: non, partial, full
\item whether central air-conditioning
\item whether fireplace
\item number of garages
\item quarterly dummy (estimates were for 1999)
\end{itemize}

\subsection{zurada-11}

A comparison of regression and artificial intelligence methods in a mass
appraisal context

Features used
\begin{itemize}
\item number of baths: sub-standard (0), 1, 1 1/2, 2, 2 1/2, 3, more
than 3
\item whether central air
\item lot size: small ( no more than 0.25 acres), medium (up to 0.5
acres), large (up to .75 acres), tract (more than 1 acre)
\item construction type: 1 story, 1.5 stories, 2 stories
\item wall type frame, brick, other
\item basement type: none, partial, full
\item basement code: none, standard
\item garage type: none, carport, detached, attached, garage in
basement, or built-in garage
\end{itemize}

\subsection{chopra-09}

\subsubsection{chopra model 1}

These features were used by Sumit Chopra \cite{chopra-09} page 87 in
developing a  ``relational regression model.'' Chopra predicted
log(price). Features used:
\begin{itemize}
\item living area
\item year buit
\item number of bedrooms
\item number of bathrooms
\item pool or no pool
\item prior sale price
\item parking spaces
\item parking types
\item lot acerage
\item land value
\item improvement value
\item percent improvement value
\item new construction
\item foundation
\item roof type
\item head type
\item site influence
\item latitude
\item longitude
\item census tract median household income
\item census tract proportion of units that are owner occupied
\item census tract average commute time to work
\item academic performance index for the school district
\end{itemize}

Transformation performed
\begin{itemize}
\item Drop all records with one or more missing fields, leaving 42,025
transactions
\item Map all fields for price, area, and income into the log space
\item Code all non-numeric discrete variables (such as pool, parking
type) using 1-of-K coding
\item Normalize all variables to have mean 0 and a standard deviation
between -1 and 1
\end{itemize}

Use only deeds and taxroll info for the year 2004. Treat data in the
first 90 percent of the year as training data. Use data in the last 10
percent of the year as testing samples. As a validation set to calibrate
models, take 10 percent of the 90 percent.

Model form was a relational factor graph. This model assumes the house
price has two components: a ``intrinsic price'' based on the features of
the house and an ``desirability price'' based on the desirability of the
neighborhood. Since the desirability is not observed, it is modeled as a
latent variable. The intrinsic price was modeled with a neural network
having 250 units in the first hidden layer, 80 units in the second
hidden layer, and 1 unit (the log price) in the output
layers.

Predict log(price) not price ``in order to avoid a bias towards
expensive houses.''

\subsubsection{chopra model 2}

This data set is similar to what I have.

Used deeds from January 1984 to April 2008.

Variables used from the deeds tape:
\begin{itemize}
\item sale price
\item recording date
\item sale date
\item transaction category code
\item transaction type coe
\item document type code
\end{itemize}

Variables used from the tax-roll tape:
\begin{itemize}
\item APN number
\item land square footage
\item living square feet
\item mailing address, used to determine GPS coordates
\end{itemize}

Data cleaning process (designed to correspond the the Case-Shiller
index)
\begin{itemize}
\item Keep just these deeds
  \begin{itemize}
  \item Transaction category code was arms-length transaction
  \item Transaction type code was either resale or new construction
  \item Document type code was either grant deed or foreclosure
  \end{itemize}
\item Remove any record with one or more missing deeds or taxroll fields
or GPS fields
\item Remove all transactions on same house on same date with different
sales prices.
\item Remove all but one transaction on same house on same date with the
same sales price.
\item Remove repeat transactions for houses that happen within 7 months
(on the assumption that the house had undergone structural change and
hence is not a valid repeat sales transaction). In addition, decouple
(treat as not repeat sales) transactions ``before and after this dirty
transaction.''
\item Remove transactions that are part of repeat sales pairs in whic
the effective year built changed.
\item Remove repeat sales transaction pairs with sales prices less than
5,000 or more than 100,000,000 dollars.
\item Remove repeat sales transaction pairs with annualized returns les than
-50 or greater than 100 percent.
\item Remove repeat sales transaction pairs that ``[out] perform or
under perform that median house price index by more or less than 25
percent on an annualized basis.''
\item Remove any single sale transaction with a price higher than the
maximimum transaction price in any repeat sales. Also remove single sale
transactions with a price lower than the minimimum in any repeat sale.
\end{itemize}

The cleaning process left 591,239 repeat sales transactions and 367,973
single sales transactions, for a total of 1,550,451 transactions over 24
years in Los Angeles Country.


\section{first experiment}

\end{document}
 
