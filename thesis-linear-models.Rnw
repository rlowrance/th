% vim: textwidth=72
% vim: foldmethod=manual



% STYLE GUIDE
% not dataset, but data set
%\input{thesis-prologue}

\documentclass[10pt]{amsart}
%\documentclass[10pt]{amsbook}
%\setlength{\textwidth}{5in}

\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
\usepackage{verbatim}
%\usepackage[all]{xy}   %xy-pic

% graphics
\usepackage{graphicx}    % needed for \includegraphics
% Tell \includegraphics where to search for files
% see Gratzer p 317
% The trailing / is required
\graphicspath{%
{/home/roy/Dropbox/nyu-real-estate/repp-repo.git/src/local-weighted-regression-2/src/}
}
\usepackage{epstopdf}   % allow eps files as graphics input
\usepackage{caption}    % allow line breaks \\ in captions

% don't indent first list of a paragraph
%\setlength{\parindent}{0pt}   
% increase spacing between paragraphs
\setlength{\parskip}{1ex plus 0.5ex minus 0.2 ex}
% don't align right margin
\raggedright

% proclamations
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\newtheorem{notation}{Notation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem]{Theorem}

%%%%%% commands local to this document
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\term}[1]{\emph{#1}}
\newcommand{\blanks}{\_\_\_}
\newcommand{\blank}{\textunderscore\textunderscore\textunderscore}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\title{Linear Models}
\author{Roy E. Lowrance}
\date{\today}
\email{roy.lowrance@gmail.com}
\maketitle

<<Control>>=
options(warn = 2)  # turn warnings into errors
source('DirectoryWorking.R')
Control <- function() {
    working <- DirectoryWorking()
    Append <- function(name, value, lst) {
      lst[[name]] <- value
      lst
    }
    AppendAvmPath <- function(ndays, suffix, lst) {
      Append(name = sprintf('path.in.e.avm.variants.training.%d.%s'
                            ,ndays
                            ,suffix
                            )
             ,value = sprintf('%se-avm-variants-training-%d.%s'
                              ,working
                              ,ndays
                              ,suffix
                              )
             ,lst = lst
             )
    }
    AppendAvmPaths <- function(ndays, lst) {
      a <- AppendAvmPath(ndays, 'txt', lst)
      b <- AppendAvmPath(ndays, 'RData', a)
      b
    }
    AppendMedianPricePath <- function(by, from, to, suffix, lst) {
      Append(name = sprintf('path.in.e.median.price.by.%s.%s'
                            ,by
                            ,suffix
                            )
            ,value = sprintf('%se-median-price-by-%s-from-%d-to-%d.%s'
                             ,working
                             ,by
                             ,from
                             ,to
                             ,suffix
                             )
            ,lst = lst
            )
    }
    AppendMedianPricePaths <- function(by, from, to, lst) {
      a <- AppendMedianPricePath(by, from, to, 'pdf', lst)
      b <- AppendMedianPricePath(by, from, to, 'RData', a)
      b
    }

    control <- list( cache = FALSE
                    #,cache = TRUE
                    ,include = TRUE
                    )
    control2 <- AppendAvmPaths(30, control)
    control3 <- AppendAvmPaths(60, control2)
    control4 <- AppendAvmPaths(90, control3)
    control5 <- AppendMedianPricePaths('month', 2006, 2009, control4)
    control6 <- AppendMedianPricePaths('year', 1984, 2009, control5)
    control6
#
#    SetAvmPaths <- function(lst) {
#      # add the e.avm.variants paths to the control list lst
#      SetAvmPath <- function(ndays, lst) {
#        Name <- function(ndays, suffix) {
#          sprintf( 'path.in.e.avm.variants.training.%d.%s'
#                  ,ndays
#                  ,suffix
#                  )
#        }
#        Path <- function(ndays, suffix) {
#          sprintf( '%se-avm-variants-training-%d.%s'
#                  ,working
#                  ,ndays
#                  ,suffix
#                  )
#        }
#        lst[[Name(ndays, 'txt')]] <- Path(ndays, 'txt')
#        lst[[Name(ndays, 'rdata')]] <- Path(ndays, 'rdata')
#        lst
#      }
#      SetAvmPath(30, SetAvmPath(60, SetAvmPath(90, lst)))
#    }
#    SetMedianPricePaths <- function(lst) {
#      SetMedianPricePath <- function(by, from, to, lst) {
#        Name <- function() {
#          sprintf('path.in.median.price.by.%s.pdf'
#                  ,by
#                  )
#        }
#        Path <- function() {
#          sprintf('%se-median-price-by-%s-from-%d-to-%d.pdf'
#                  ,working
#                  ,by
#                  ,from
#                  ,to
#                  )
#        }
#        lst[[Name()]] <- Path()
#        lst
#      } 
#      SetMedianPricePath('month'
#                         ,2006
#                         ,2009
#                         ,SetMedianPricePath('year'
#                                             ,1984
#                                             ,2009
#                                             ,lst
#                                             )
#                         )
#    }
#
#    control2 <- SetAvmPaths(control)
#    control3 <- SetMedianPricePaths(control2)
#
#    control3
}

control <- Control()
str(control)
opts_chunk$set(cache = control$cache)
opts_chunk$set(include = control$include)
@

<<Commas>>=
Commas <- function(i) {
    # insert commas into an integer
    format(i, big.mark = ',')
}
# test
Commas(123456)
@

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

From this simple idea, a huge variety of models can be generated by
varying the features $x_i$, by applying transformations to the outcome
$y$ or feature $x_i$, by considering different scenarios for using the
result model, by varying the training periods, by considering whether to
regularize, and many other factors.

Here we investigate the accuracy of linear models resulting from various
decisions. 


\begin{itemize}
\item Kinds of models
\item Model form
\item Characteristics of the training and testing periods
\item Selecting test data sets: randomly and for 2009
\item Choosing the training period
\item Feature choice
\item Training set choice
\item avm variants experiment
\item regularization
\end{itemize}

\section{Kinds of models: Scenarios}

<<KindsOfModels>>=
@

Real estate price prediction takes place in three scenarios (business
use cases). These differ largely in the business purpose of the estimate
of market value and the training data that can be used.

\subsection{Assessor Scenario}

The first of these is the ``assessor'' scenario. The tax assessor for
Los Angeles County issues tax bills in November of every year. The first
installment is due later in that year and the second installment is due
in the following year. We have the tax roll for 2008, which was used to
generate tax bills that were issued starting November 1, 2008.

The tax bill contains the tax assessor's assessment for the property.
Were it not for California Proposition 13, the assessment would be for
the market value of the property sometimes in 2008. The valuation as-of
date is vague. Because of proposition 13, the assessed value is less
than the market value in periods of high real estate inflation.

Although the tax assessor does not disclose the market value, this value
must be determined, because the assessed value is required to be the
maximum of the market value and value imputed from the last sale
transaction increased at a rate of at most two percent per year.

The ``assessor'' scenario is an assessment of the market value in 2008.
As training data, the assessor can use all of the transactions recorded
before November 1, 2008. Note that the average delay between the sale
and recording of the sale is about 60 days.

\subsection{AVM Scenario}

The second scenario is the ``automated valuation model (AVM)'' scenario.
This scenario arises because many actors need to know the value of the
property before it transacts. Such actors include sellers, buyers, and
lenders on the property. There is a business in providing the price
predictions, which are called AVMs. Here ``automated'' means created
through a computer program to distinguish the estimates from those
created by human supposed experts using data such as the deeds
data.

AVM providers are incentive to have accurate estimates, so there is no
reason to build in systematic bias in their predictions. The can use all
as training data all the sales that are recorded up to date of the
predition. For example, to predict the market value of properties on
January 15, 2009, the AVM providers can use all the data reported up to
and included January 14, 2009. They can also use the tax assessor's
assessment as a feature. The assessment is by law partitioned into the
assessment for the land and the assessment for the improvements on the
land.

\subsection{Mortgage Scenario}

The third scenario is the ``mortgage'' scenario. This scenario arises in
order to train mortgage-default prediction models. 

Borrowers default on their mortgage by not making the payments. After a
certain number of missed payments, the lender may begin legal
proceedings to seize the property, which is collateral for the mortgage.
The process varies by state.

There are two major reasons for not making mortgage payments.
Historically the major reason was that the borrower could not afford to
make the payments, perhaps because of loss of income. If the house is
worth more than the mortgage and the real estate market is operating
quickly, rather than default, the borrower may be able to sell the house and use
the proceeds to pay off the mortgage and so that default would not occur. 
The sell-and-pay strategy does not work if the value of the house less
its selling costs is less than the amount owed on the mortgage. In this
situation, the house is said to be under water. In the 2007 United
States real estate crisis, many houses were under water.

The second reason for not making the payments was historically rare but
became more frequent with the 2007 real estate crisis: the borrower
had the cash to pay but decided to not pay. These ``strategic defaults''
are in part driven the house being under water.

To build a predictive model of defaults, the feature set often includes
the value of the house and the amount owed on the mortgage. In the
mortgage scenario, the value of the house is estimated in every relevant
time period. One is mimicking the borrower's estimate of the value of
the house.

In making the mortgage prediction for January 15, 2009, one can use all
the historic transactions, all the transactions on the date of the
estimate, and all the future transactions as well. One can also use any
transaction on the query house itself, as one is estimating what the
borrower would think the house is worth. An available feature is the
tax assessor's assessment.

\section{Model form}

<<ModelForm>>=
@

Our linear model

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

allows the variables to be transformed before entering the equation.

Two popular choices for the response variable $y$ are to set it to the
$price$ or the $\log(price)$. Some researchers including Bourassa
(\cite{bourassa-02} p. 11) transform the price into the log domain as a
means to make the resulting distribution ``more normal'' claiming that
doing so ``helps to normalize the distribution of the error term.'' But
does this transformation improve predictive accuracy? That question is
investigated below.

Another choice is to transform some of the features into the log domain.
One might do so when the price is in the log domain, so that the
regression equation in part becomes

\[ log(price) = \ldots + \beta log(x) + \ldots  .\]

The idea of this transformation (cite the econometrics books) is that a,
say, 10 percent change in the value of x would lead to a 10 percent
change in the value of the price. These transformation are used in
Chopra's work (cite his thesis), where all size-related features were
transformed into the log domain. The intuition is that doubling the size
would double the value.

We consider 4 forms of models and ask which provides the most accurate
predictions. The form and their interpretations are:

\begin{itemize}
\item level-level: $price = \ldots + \beta x_i + \ldots.$ A doubling
of x leads to $\beta$ factor increase in the price.
\item level-log: CHECK THE TEXT
\item log-level:
\item log-log
\end{itemize}

\section{Choosing the training period}

<<ChoosingTrainingPeriod>>=
control
path <- control$path.in.e.median.price.by.month.RData
loaded <- load(path)
loaded
str(median.prices)
MedianPrice <- function(year, month) {
  is.year <- median.prices$sale.year == year
  is.month <- median.prices$sale.month == month
  median.prices[is.year & is.month, 'median.price']
}
MedianPrice(2009, 3) #test
median.test.price <- c( MedianPrice(2008, 11)
                       ,MedianPrice(2008, 12)
                       ,MedianPrice(2009, 01)
                       ,MedianPrice(2009, 02)
                       ,MedianPrice(2009, 03)
                       )
median.test.price
median.median.test.price <- median(median.test.price)
median.median.test.price
@

The linear model uses the average value of each feature to estimate the
total value of the house. When prices are moving rapidly, feature values
may also be moving rapidly. It's important to think through the training
data set for a given estimation task.

\begin{figure}[h]
\caption{Median Prices By Year}
\label{figure:medianPricesByYear}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.e.median.price.by.year.pdf}}
\end{figure}

<<MedianPricesByYear>>=
@

In Figure \ref{figure:medianPricesByYear}, we see the median prices of
arms-length transactions for single-family residences in our subset1.
You can see that prices have not been steady and that there have been
periods of steady price increases followed by periods of price declines.
The decline in prices after 2007 was not the only price decrease: there
was another decline starting in 1990 that was sustained for about six
years. 

\begin{figure}[h]
\caption{Median Prices By Month}
\label{figure:medianPricesByMonth}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.e.median.price.by.month.pdf}}
\end{figure}

Figure \ref{figure:medianPricesByMonth} depicts median prices by month
starting in January 2006. We see approximately level prices starting
early in 2006, a spike in mid July 2007, and then the well-known
decline. The difficulty of forecasting future median prices can been
seen: What is the clue that July 2007 would be the peak? Median prices
increased in February 2008 but declined to a new low in March 2008.

The impliciation for linear models is that the model builder faces a
dilema: train on too long a period, and the features-level prices
found by the fitting process for the model are likely to be out-of-date;
train on too short a period, and the volume of training data may be too
low to accuralate fit to feature prices.

One possible way around this dilema is to successfully forecast median
prices for the test period: will they be lower, about the same, or
higher than in the training period. The trained feature prices could
then be adjusted. However, such a forecast is beyond the scope of this work.

Another possible way around this dilema is to incorporate time periods
explicitly in the model. For example Bourassa in \cite{bourassa-02}
builds a model for real estate prices that includes as dummy features the
quarter of the transaction: the features are \code{is-quarter-1},
\code{is-quarter-2}, and \code{is-quarter-3}. These features capture
average price movement and can be used in our mortgage scenarios,
because training data about the future related to the query transaction
are needed.

A third possibility is to limit the training period to a short enough
period to capture just recent prices instead of historic for the
training. This approach doesn't try to anticipate market movement after
the training period to the query period. It's the approach followed in
this work.

A fourth possibility is to determine the best training period for the
date before the query transaction, the use that training period for the
query transaction. Determine the best through cross validation.


\section{Feature choice}
\label{Section:FeatureChoice}

<<FeatureChoice>>=
@

Features used in the literature.

\subsection{bourassa-02}

paper: bourassa-02 Do housing submarkets really matter? pages 10 and 11.
Estimated log(price). These variables were found to ``contribute
explanatory power to the estimated equations:''

features used in hedonic (linear) models
\begin{itemize}
\item intercept
\item log(floor area)
\item log(land area)
\item cross-leaded or strata-titled (a form of ownership)
\item detached dwelling
\item age of dwelling
\item age of dwelling squared
\item wall condition: good, average, or bad 
\item roof materials: tile, metal, or other
\item wall materials: wood, brick, fibrolite, or other 
\item quality of the principal structure: superior, average, or poor 
\item log(distance to the central business district)
\item quarterly time-period dummy
\item water view
\item modernization
\item landscaping: good, average, or poor
\item driveway
\item neighborhood: very good, good, average, or poor
\item number of attached garages
\end{itemize}

age of dwelling and age squared (because the value is expected to
follow a U-shaped curve ``because very old houses can earn a premium do
to their historic character and distinctive neighborhood.''

Why land area and floor area in log domain? ``to reflect the likelihood
of diminishing returns [to price] as ... the values of those variables
increase in size.''

Use log(distance to CBD) ``in view of theory and empirical evidence."

Estimate log(price) ``as a means for making its distribution more
normal, which in turn helps to normalize the distribution of the error
terms, a desirable characteristic for ordinary least squares
estimators.'' Example of a statistical consideration that may or may not
improve model accuracy.

Evaluation criteria: fraction of out-of-sample predictions that are
within 10 and 20 percent of the true prices.

Predicted prices were adjusted ``for the average residual for the
neighborhood in which the property is located.'' A neighborhood was
defined as a portion of the tax roll containing about 1,000 properties.

\subsection{bourassa-10}

Predicting house prices with spatial dependence: a comparison of
alternative methods

Primary criterion: percentage of ex-sample predictions with 10 percent
of the transaction price

Dependent variables is log of sales price.

features used in OLS models
\begin{itemize}
\item intercept
\item land area
\item land area squared
\item floor area
\item bathrooms: no more than 1, 1.5 or 2, 2.5 or more
\item age of house
\item age of house squared
\item basement: non, partial, full
\item whether central air-conditioning
\item whether fireplace
\item number of garages
\item quarterly dummy (estimates were for 1999)
\end{itemize}

\subsection{zurada-11}

A comparison of regression and artificial intelligence methods in a mass
appraisal context

Features used
\begin{itemize}
\item number of baths: sub-standard (0), 1, 1 1/2, 2, 2 1/2, 3, more
than 3
\item whether central air
\item lot size: small ( no more than 0.25 acres), medium (up to 0.5
acres), large (up to .75 acres), tract (more than 1 acre)
\item construction type: 1 story, 1.5 stories, 2 stories
\item wall type frame, brick, other
\item basement type: none, partial, full
\item basement code: none, standard
\item garage type: none, carport, detached, attached, garage in
basement, or built-in garage
\end{itemize}

\subsection{chopra-09}

\subsubsection{chopra model 1}

These features were used by Sumit Chopra \cite{chopra-09} page 87 in
developing a  ``relational regression model.'' Chopra predicted
log(price). Features used:
\begin{itemize}
\item living area
\item year buit
\item number of bedrooms
\item number of bathrooms
\item pool or no pool
\item prior sale price
\item parking spaces
\item parking types
\item lot acerage
\item land value
\item improvement value
\item percent improvement value
\item new construction
\item foundation
\item roof type
\item head type
\item site influence
\item latitude
\item longitude
\item census tract median household income
\item census tract proportion of units that are owner occupied
\item census tract average commute time to work
\item academic performance index for the school district
\end{itemize}

Transformation performed
\begin{itemize}
\item Drop all records with one or more missing fields, leaving 42,025
transactions
\item Map all fields for price, area, and income into the log space
\item Code all non-numeric discrete variables (such as pool, parking
type) using 1-of-K coding
\item Normalize all variables to have mean 0 and a standard deviation
between -1 and 1
\end{itemize}

Use only deeds and taxroll info for the year 2004. Treat data in the
first 90 percent of the year as training data. Use data in the last 10
percent of the year as testing samples. As a validation set to calibrate
models, take 10 percent of the 90 percent.

Model form was a relational factor graph. This model assumes the house
price has two components: a ``intrinsic price'' based on the features of
the house and an ``desirability price'' based on the desirability of the
neighborhood. Since the desirability is not observed, it is modeled as a
latent variable. The intrinsic price was modeled with a neural network
having 250 units in the first hidden layer, 80 units in the second
hidden layer, and 1 unit (the log price) in the output
layers.

Predict log(price) not price ``in order to avoid a bias towards
expensive houses.''

\subsubsection{chopra model 2}

This data set is similar to what I have.

Used deeds from January 1984 to April 2008.

Variables used from the deeds tape:
\begin{itemize}
\item sale price
\item recording date
\item sale date
\item transaction category code
\item transaction type coe
\item document type code
\end{itemize}

Variables used from the tax-roll tape:
\begin{itemize}
\item APN number
\item land square footage
\item living square feet
\item mailing address, used to determine GPS coordates
\end{itemize}

Data cleaning process (designed to correspond the the Case-Shiller
index)
\begin{itemize}
\item Keep just these deeds
  \begin{itemize}
  \item Transaction category code was arms-length transaction
  \item Transaction type code was either resale or new construction
  \item Document type code was either grant deed or foreclosure
  \end{itemize}
\item Remove any record with one or more missing deeds or taxroll fields
or GPS fields
\item Remove all transactions on same house on same date with different
sales prices.
\item Remove all but one transaction on same house on same date with the
same sales price.
\item Remove repeat transactions for houses that happen within 7 months
(on the assumption that the house had undergone structural change and
hence is not a valid repeat sales transaction). In addition, decouple
(treat as not repeat sales) transactions ``before and after this dirty
transaction.''
\item Remove transactions that are part of repeat sales pairs in whic
the effective year built changed.
\item Remove repeat sales transaction pairs with sales prices less than
5,000 or more than 100,000,000 dollars.
\item Remove repeat sales transaction pairs with annualized returns les than
-50 or greater than 100 percent.
\item Remove repeat sales transaction pairs that ``[out] perform or
under perform that median house price index by more or less than 25
percent on an annualized basis.''
\item Remove any single sale transaction with a price higher than the
maximimum transaction price in any repeat sales. Also remove single sale
transactions with a price lower than the minimimum in any repeat sale.
\end{itemize}

The cleaning process left 591,239 repeat sales transactions and 367,973
single sales transactions, for a total of 1,550,451 transactions over 24
years in Los Angeles Country.


\section{Is the assessment a valuable feature?}

The assessment might have value as a feature in the predictive model,
especially if it was for a recent time period and the assessment was a
statement of the market value. For Los Angeles Country, the assessment
is for a vague date in November 1, 2007 through October 31, 2008 and is
for the market value only if the house sold during that time period.
Hence that the assessment is of value as a feature in predictive models
is not certain.

We ran two experiments to attempt to understand the value of the
assessment as a feature. In the first, we use the actual data from Los
Angeles. The key feature of the first experiment is that the assessment
is not an estimate of the market value.  In the second, we generate
synthetic data that is drawn from the Los Angeles data set but replaces
the assessment with a noisy estimate of the market value. 


\subsection{Experiment: Actual Los Angeles Data}

<<ActualLosAngelesData>>=
@

<<CompareEAvmVariants>>=
print(control)
GenError <- function(path) {
  print(path)
  loaded <- load(path)
  str(loaded)
  str(gen.error)
  gen.error
}
gen.error.30 <-
  GenError(control$path.in.e.avm.variants.training.30.RData)
gen.error.60 <-
  GenError(control$path.in.e.avm.variants.training.60.RData)
gen.error.90 <-
  GenError(control$path.in.e.avm.variants.training.90.RData)

MedianError <- function(scenario, ndays) {
  gen.error <- if (ndays == 30) gen.error.30 
            else (if (ndays == 60) gen.error.60
                  else gen.error.90
                  )
  median.error <- gen.error[[scenario]]$median.RMedianSE
  cat('median.error', median.error, scenario, ndays, '\n')
  median.error
}
MedianError('mortgage with assessment', 90)  # test
# combine and produce a table
# <scenario> <assessment usage> <30 day error rate> <60> <90>
@

Recall that monthly median prices were falling rapidly starting in July
2007, with an uptick in February 2009 and a decline in March 2009.

We expect that longer training periods will pick up increased
out-of-test-period feature prices and hence lead to worse overall
performance.

In these experiments, we use a log-linear model, because these models
are popular in the literature. We estimate the generalization error
using 10-fold cross validation. The feature set is as described above in
section \ref{Section:FeatureChoice}. The test set is all transactions
for which the assessment might be relevant, namely transactions in the
period November 1, 2008 through March 31, 2009. The training period is
30, 60, or 90 days.  For the assessor and avm scenarios, the training
period is before the date of the query transactions. For the mortgage
scenario, the training period is split to be half before and half after
the date of the query transactions. 

\subsubsection{Training on 30 days of data}

\begin{figure}[h]
\caption{Los Angeles Data Trained for 30 Days}
\label{figure:LosAngelesData30}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.avm.variants.training.30.txt}}
\end{figure}

In Figure \ref{figure:LosAngelesData30} we see the averaged test errors
from 10-fold cross validation for the scenarios of interest with and
without the assessment using a training period of 30 days.

The first finding is that estimated generalization errors are very high
when estimated using the mean root-mean-squared error of the
cross-validation folds. The median price during the testing period is
\Sexpr{Commas(median.median.test.price)}
dollars and these mean errors are about the same size. The reason is
that the models badly predict a few very expensive houses by valuing
them at enormous prices, leading to huge mean price differences. To
reduce the impact of these few houses on the estimated generalization
error, we also determine the median of the average errors from the
folds, where the average error from the fold is also the median. These
results are reported in the column ``median RMedianSE'' which stands for
the median of the square root of the median squared errors from the
folds. These error estimates are much lower.

Next we find that the assessment is not always a valuable feature. Using
the assessment slightly improves the median errors for the assessor
scenario and slightly degrades the median errors for the AVM scenario.
When prices are moving rapidly, the assessment becomes out of date and
is not of certain value.

The next finding is that median errors are lower when the training data
are nearer the test transactions. Ignoring the use cases without the
assessments, we see that the median error decreases as we use training
data that are closer to the query date: the median error decreases from
\Sexpr{Commas(MedianError('assessor with assessment', 30))}
for the assessor scenario with the assessment to
\Sexpr{Commas(MedianError('AVM with assessment', 30))}
for the AVM scenario with the assessment. The only difference is
that the assessor data is about two months out of date compared to the
AVM data. We see that the median error for the mortgage scenario is
\Sexpr{Commas(MedianError('mortgage with assessment', 30))}
which is the lowest and uses 30 days of data that is centered on the
date of the query transaction. When prices are moving rapidly, training
on data near the training date is needed in order to price the features
at market values.


\subsubsection{Training on 60 days of data}

\begin{figure}[h]
\caption{Los Angeles Data Trained for 60 Days}
\label{figure:LosAngelesData60}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.avm.variants.training.60.txt}}
\end{figure}

Figure \ref{figure:LosAngelesData60} shows results from using a 60-day
training period. Focusing on the medians, we see that as for the 30-day
training period, the estimated generalization error sometimes increases
when the assessment is used as a feature. 

As in the 30-day use case, training on more contemporary data leads to
more accurate estimates.

\subsubsection{Training on 90 days of data}

\begin{figure}[h]
\caption{Los Angeles Data Trained for 90 Days}
\label{figure:LosAngelesData90}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.avm.variants.training.90.txt}}
\end{figure}

Figure \ref{figure:LosAngelesData90} shows results from using a 90-day
training period. The training data are more out of date compared to the
30 and 60-day training periods. Now the assessment is always valuable.

As in the 30-day use case, training on more contemporary data leads to
more accurate estimates.

\subsubsection{Comparing 30, 60, and 90 training-day accuracy}

<<ME>>=
ME <- function(scenario, ndays)
  as.integer(MedianError(scenario, ndays))
@

\begin{figure}[h]
\caption{Los Angeles Data Median RMedianSE}
\label{figure:LosAngelesDataMedian}
\begin{tabular}{l | r | r | r |}
\hline
         & training & training & training \\ 
scenario & 30 days  & 60 days  & 90 days \\ \hline
assessor without assessment & 
\Sexpr{ME('assessor without assessment', 30)} &
\Sexpr{ME('assessor without assessment', 60)} &
\Sexpr{ME('assessor without assessment', 90)} 
\\ \hline
assessor with assessment & 
\Sexpr{ME('assessor with assessment', 30)} &
\Sexpr{ME('assessor with assessment', 60)} &
\Sexpr{ME('assessor with assessment', 90)} 
\\ \hline
AVM without assessment & 
\Sexpr{ME('AVM without assessment', 30)} &
\Sexpr{ME('AVM without assessment', 60)} &
\Sexpr{ME('AVM without assessment', 90)} 
\\ \hline
AVM with assessment & 
\Sexpr{ME('AVM with assessment', 30)} &
\Sexpr{ME('AVM with assessment', 60)} &
\Sexpr{ME('AVM with assessment', 90)} 
\\ \hline
mortgage with assessment & 
\Sexpr{ME('mortgage with assessment', 30)} &
\Sexpr{ME('mortgage with assessment', 60)} &
\Sexpr{ME('mortgage with assessment', 90)} 
\\ \hline
\end{tabular}
\end{figure}

Figure \ref{figure:LosAngelesDataMedian} compares median errors
across scenarios for 30, 60, and 90 days.

We see that longer training periods usually result in higher errors.

We see that training on more contemporary data always results in lower
errors.

\subsubsection{Some potential myths}

It is a myth that the assessment always has value. We can see in these
experiments that using the assessment as a feature degrades the accuracy
of the estimated price, because the assessment becomes increasingly out
of date as the market moves rapidly. 

It is a myth that more training data leads to more accurate estimates.
While it is true that having more training data available generates
options for model design that may lead to increased accuracy, actually
using more out-of-date data degrades estimation quality in these
experiments.

\subsection{Experiment: Synthetic Data}

\section{Do log-level models outperform other model forms?}

Explain testing on the 25,000 random samples, 1000 from each year.

Q: How to estimate the generalization error?

\section{Which training period is best?}

\end{document}
 
