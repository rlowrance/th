% vim: textwidth=72
% vim: foldmethod=manual



% STYLE GUIDE
% not dataset, but data set
%\input{thesis-prologue}

\documentclass[10pt]{amsart}
%\documentclass[10pt]{amsbook}
%\setlength{\textwidth}{5in}

\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
%\usepackage[all]{xy}   %xy-pic

% graphics
\usepackage{graphicx}    % needed for \includegraphics
% Tell \includegraphics where to search for files
% see Gratzer p 317
% The trailing / is required
\graphicspath{%
{/home/roy/Dropbox/nyu-real-estate/repp-repo.git/src/local-weighted-regression-2/src/}
}
\usepackage{epstopdf}   % allow eps files as graphics input
\usepackage{caption}    % allow line breaks \\ in captions

% don't indent first list of a paragraph
%\setlength{\parindent}{0pt}   
% increase spacing between paragraphs
\setlength{\parskip}{1ex plus 0.5ex minus 0.2 ex}
% don't align right margin
\raggedright

% proclamations
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\newtheorem{notation}{Notation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem]{Theorem}

%%%%%% commands local to this document
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\term}[1]{\emph{#1}}
\newcommand{\blanks}{\_\_\_}
\newcommand{\blank}{\textunderscore\textunderscore\textunderscore}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\title{Linear Models}
\author{Roy E. Lowrance}
\date{\today}
\email{roy.lowrance@gmail.com}
\maketitle

<<control>>=
options(warn = 2)  # turn warnings into errors
source('DirectoryWorking.R')
Control <- function() {
    working <- DirectoryWorking()
    control <-
        list( path.in.transactions = paste0(working, 'transactions-al-sfr.RData')
             ,path.in.subset = paste0(working, 'transactions-al-sfr-subset1.RData')
             ,path.in.deeds = paste0(working, 'deeds-al.RData')
             ,path.in.parcels = paste0(working, 'parcels-sfr.RData')
             ,path.in.median.price.by.month = 
                paste0( working
                       ,'e-median-price_by_month_from_2006_to_2009.RData'
                       )
             ,path.in.median.price.by.year =
                paste0( working
                       ,'e-median-price_by_year_from_1984_to_2009.RData'
                       )
             ,cache = FALSE
             ,include = TRUE
        )
    control
}

control <- Control()
str(control)
opts_chunk$set(cache = control$cache)
opts_chunk$set(include = control$include)
@

<<Commas>>=
Commas <- function(i) {
    # insert commas into an integer
    format(i, big.mark = ',')
}
# test
Commas(123456)
@

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

From this simple idea, a huge variety of models can be generated by
varying the features $x_i$, by applying transformations to the outcome
$y$ or feature $x_i$, by considering different scenarios for using the
result model, by varying the training periods, by considering whether to
regularize, and many other factors.

Here we investigate the accuracy of linear models resulting from various
decisions. 


\begin{itemize}
\item Kinds of models
\item Model form
\item Characteristics of the training and testing periods
\item Selecting test data sets: randomly and for 2009
\item Feature choice
\item avm variants experiment
\item regularization
\end{itemize}

\section{Kinds of models: Scenarios}

Real estate price prediction takes place in three scenarios (business
use cases). These differ largely in the business purpose of the estimate
of market value and the training data that can be used.

\subsection{Assessor Scenario}

The first of these is the ``assessor'' scenario. The tax assessor for
Los Angeles County issues tax bills in November of every year. The first
installment is due later in that year and the second installment is due
in the following year. We have the tax roll for 2008, which was used to
generate tax bills that were issued starting November 1, 2008.

The tax bill contains the tax assessor's assessment for the property.
Were it not for California Proposition 13, the assessment would be for
the market value of the property sometimes in 2008. The valuation as-of
date is vague. Because of proposition 13, the assessed value is less
than the market value in periods of high real estate inflation.

Although the tax assessor does not disclose the market value, this value
must be determined, because the assessed value is required to be the
maximum of the market value and value imputed from the last sale
transaction increased at a rate of at most two percent per year.

The ``assessor'' scenario is an assessment of the market value in 2008.
As training data, the assessor can use all of the transactions recorded
before November 1, 2008. Note that the average delay between the sale
and recording of the sale is about 60 days.

\subsection{AVM Scenario}

The second scenario is the ``automated valuation model (AVM)'' scenario.
This scenario arises because many actors need to know the value of the
property before it transacts. Such actors include sellers, buyers, and
lenders on the property. There is a business in providing the price
predictions, which are called AVMs. Here ``automated'' means created
through a computer program to distinguish the estimates from those
created by human supposed experts using data such as the deeds
data.

AVM providers are incentive to have accurate estimates, so there is no
reason to build in systematic bias in their predictions. The can use all
as training data all the sales that are recorded up to date of the
predition. For example, to predict the market value of properties on
January 15, 2009, the AVM providers can use all the data reported up to
and included January 14, 2009. They can also use the tax assessor's
assessment as a feature. The assessment is by law partitioned into the
assessment for the land and the assessment for the improvements on the
land.

\subsection{Mortgage Scenario}

The third scenario is the ``mortgage'' scenario. This scenario arises in
order to train mortgage-default prediction models. 

Borrowers default on their mortgage by not making the payments. After a
certain number of missed payments, the lender may begin legal
proceedings to seize the property, which is collateral for the mortgage.
The process varies by state.

There are two major reasons for not making mortgage payments.
Historically the major reason was that the borrower could not afford to
make the payments, perhaps because of loss of income. If the house is
worth more than the mortgage and the real estate market is operating
quickly, rather than default, the borrower may be able to sell the house and use
the proceeds to pay off the mortgage and so that default would not occur. 
The sell-and-pay strategy does not work if the value of the house less
its selling costs is less than the amount owed on the mortgage. In this
situation, the house is said to be under water. In the 2007 United
States real estate crisis, many houses were under water.

The second reason for not making the payments was historically rare but
became more frequent with the 2007 real estate crisis: the borrower
had the cash to pay but decided to not pay. These ``strategic defaults''
are in part driven the house being under water.

To build a predictive model of defaults, the feature set often includes
the value of the house and the amount owed on the mortgage. In the
mortgage scenario, the value of the house is estimated in every relevant
time period. One is mimicking the borrower's estimate of the value of
the house.

In making the mortgage prediction for January 15, 2009, one can use all
the historic transactions, all the transactions on the date of the
estimate, and all the future transactions as well. One can also use any
transaction on the query house itself, as one is estimating what the
borrower would think the house is worth. An available feature is the
tax assessor's assessment.

\section{Model form}

Our linear model

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

allows the variables to be transformed before entering the equation.

Two popular choices for the response variable $y$ are to set it to the
$price$ or the $\log(price)$. Some researchers including Bourassa
(\cite{bourassa-02} p. 11) transform the price into the log domain as a
means to make the resulting distribution ``more normal'' claiming that
doing so ``helps to normalize the distribution of the error term.'' But
does this transformation improve predictive accuracy? That question is
investigated below.

Another choice is to transform some of the features into the log domain.
One might do so when the price is in the log domain, so that the
regression equation in part becomes

\[ log(price) = \ldots + \beta log(x) + \ldots  .\]

The idea of this transformation (cite the econometrics books) is that a,
say, 10 percent change in the value of x would lead to a 10 percent
change in the value of the price. These transformation are used in
Chopra's work (cite his thesis), where all size-related features were
transformed into the log domain. The intuition is that doubling the size
would double the value.

We consider 4 forms of models and ask which provides the most accurate
predictions. The form and their interpretations are:

\begin{itemize}
\item linear-linear: $price = \ldots + \beta x_i + \ldots.$ A doubling
of x leads to $\beta$ factor increase in the price.
\item linear-log: CHECK THE TEXT
\item log-linear:
\item log-log
\end{itemize}

\section{Characteristics of the training and testing periods}

In Los Angeles Country, real estate prices mostly increased month to
month. The exception was in 2008 (?) which saw a rapid decrease in
median prices.

PUT IN 2 CHARTS (median price by month for 25 years), median price by
month for 2007- 2009.

Two sets of testing data were developed.
\section{Feature choice}

Features used in the literature.

\subsection{bourassa-02}

paper: bourassa-02 Do housing submarkets really matter? pages 10 and 11.
Estimated log(price). These variables were found to ``contribute
explanatory power to the estimated equations:''

features used in hedonic (linear) models
\begin{itemize}
\item intercept
\item log(floor area)
\item log(land area)
\item cross-leaded or strata-titled (a form of ownership)
\item detached dwelling
\item age of dwelling
\item age of dwelling squared
\item wall condition: good, average, or bad 
\item roof materials: tile, metal, or other
\item wall materials: wood, brick, fibrolite, or other 
\item quality of the principal structure: superior, average, or poor 
\item log(distance to the central business district)
\item quarterly time-period dummy
\item water view
\item modernization
\item landscaping: good, average, or poor
\item driveway
\item neighborhood: very good, good, average, or poor
\item number of attached garages
\end{itemize}

age of dwelling and age squared (because the value is expected to
follow a U-shaped curve ``because very old houses can earn a premium do
to their historic character and distinctive neighborhood.''

Why land area and floor area in log domain? ``to reflect the likelihood
of diminishing returns [to price] as ... the values of those variables
increase in size.''

Use log(distance to CBD) ``in view of theory and empirical evidence."

Estimate log(price) ``as a means for making its distribution more
normal, which in turn helps to normalize the distribution of the error
terms, a desirable characteristic for ordinary least squares
estimators.'' Example of a statistical consideration that may or may not
improve model accuracy.

Evaluation criteria: fraction of out-of-sample predictions that are
within 10 and 20 percent of the trueprices.

Predicted prices were adjusted ``for the average residual for the
neighborhood in which the property is located.'' A neighborhood was
defined as a portion of the tax roll containing about 1,000 properties.

\subsection{bourassa-10}

Predicting house prices with spatial dependence: a comparison of
alternative methods

Primary criterion: percentage of ex-sample predictions with 10 percent
of the transaction price

Dependent variables is log of sales price.

features used in OLS models
\begin{itemize}
\item intercept
\item land area
\item land area squared
\item floor area
\item bathrooms: no more than 1, 1.5 or 2, 2.5 or more
\item age of house
\item age of house squared
\item basement: non, partial, full
\item whether central air-conditioning
\item whether fireplace
\item number of garages
\item quarterly dummy (estimates were for 1999)
\end{itemize}

\subsection{zurada-11}

A comparison of regression and artificial intelligence methods in a mass
appraisal context

Features used
\begin{itemize}
\item number of baths: sub-standard (0), 1, 1 1/2, 2, 2 1/2, 3, more
than 3
\item whether central air
\item lot size: small ( no more than 0.25 acres), medium (up to 0.5
acres), large (up to .75 acres), tract (more than 1 acre)
\item construction type: 1 story, 1.5 stories, 2 stories
\item wall type frame, brick, other
\item basement type: none, partial, full
\item basement code: none, standard
\item garage type: none, carport, detached, attached, garage in
basement, or built-in garage
\end{itemize}

\section{first experiment}

\end{document}
 
