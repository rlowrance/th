% vim: textwidth=72
% vim: foldmethod=manual



% STYLE GUIDE
% not dataset, but data set
%\input{thesis-prologue}

\documentclass[10pt]{amsart}
%\documentclass[10pt]{amsbook}
%\setlength{\textwidth}{5in}

\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
\usepackage{verbatim}
%\usepackage[all]{xy}   %xy-pic

% graphics
\usepackage{graphicx}    % needed for \includegraphics
% Tell \includegraphics where to search for files
% see Gratzer p 317
% The trailing / is required
\graphicspath{%
{/home/roy/Dropbox/nyu-real-estate/repp-repo.git/src/local-weighted-regression-2/src/}
}
\usepackage{epstopdf}   % allow eps files as graphics input
\usepackage{caption}    % allow line breaks \\ in captions

% don't indent first list of a paragraph
%\setlength{\parindent}{0pt}   
% increase spacing between paragraphs
\setlength{\parskip}{1ex plus 0.5ex minus 0.2 ex}
% don't align right margin
\raggedright

% proclamations
%\newtheorem{corollary}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\newtheorem{notation}{Notation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem]{Theorem}

%%%%%% commands local to this document
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\term}[1]{\emph{#1}}
\newcommand{\blanks}{\_\_\_}
\newcommand{\blank}{\textunderscore\textunderscore\textunderscore}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\title{Linear Models}
\author{Roy E. Lowrance}
\date{\today}
\email{roy.lowrance@gmail.com}
\maketitle

<<Control>>=
options(warn = 2)  # turn warnings into errors
source('Directory.R')
Control <- function() {
    drawings <- Directory('drawings')
    working <- Directory('working')

    Append <- function(name, value, lst) {
      lst[[name]] <- value
      lst
    }
    AppendAvmPath <- function(ndays, suffix, lst) {
      Append(name = sprintf('path.in.e.avm.variants.training.%d.%s'
                            ,ndays
                            ,suffix
                            )
             ,value = sprintf('%se-avm-variants-training-%d.%s'
                              ,working
                              ,ndays
                              ,suffix
                              )
             ,lst = lst
             )
    }
    AppendAvmPaths <- function(ndays, lst) {
      a <- AppendAvmPath(ndays, 'txt', lst)
      b <- AppendAvmPath(ndays, 'RData', a)
      b
    }
    AppendMedianPricePath <- function(by, from, to, suffix, lst) {
      Append(name = sprintf('path.in.e.median.price.by.%s.%s'
                            ,by
                            ,suffix
                            )
            ,value = sprintf('%se-median-price-by-%s-from-%d-to-%d.%s'
                             ,working
                             ,by
                             ,from
                             ,to
                             ,suffix
                             )
            ,lst = lst
            )
    }
    AppendMedianPricePaths <- function(by, from, to, lst) {
      a <- AppendMedianPricePath(by, from, to, 'pdf', lst)
      b <- AppendMedianPricePath(by, from, to, 'RData', a)
      b
    }
    AppendEformsPaths <- function(lst) {
      a <- Append( 'path.in.e.forms.trainingDays.30.sample.001'
                  ,paste0(working,
                          'e-forms--trainingDays-30--testSample-0.001000.txt'
                          )
                  ,lst
                  )
      b <- Append( 'path.in.e.forms.trainingDays.30.sample.01'
                  ,paste0(working,
                          'e-forms--trainingDays-30--testSample-0.010000.txt'
                          )
                  ,a
                  )
      c <- Append( 'path.in.e.forms.trainingDays.90.sample.001'
                  ,paste0(working,
                          'e-forms--trainingDays-90--testSample-0.001000.txt'
                          )
                  ,b
                  )
      d <- Append( 'path.in.e.forms.trainingDays.90.sample.01'
                  ,paste0(working,
                          'e-forms--trainingDays-90--testSample-0.010000.txt'
                          )
                  ,c
                  )
      e <- Append( 'path.in.e.forms.trainingDays.120.sample.001'
                  ,paste0(working,
                          'e-forms--trainingDays-120--testSample-0.001000.txt'
                          )
                  ,d
                  )
      f <- Append( 'path.in.e.forms.trainingDays.120.sample.01'
                  ,paste0(working,
                          'e-forms--trainingDays-120--testSample-0.010000.txt'
                          )
                  ,e
                  )
      f
    }
    AppendETrainingPeriodPaths <- function(lst) {
      a <- Append( 'path.in.e.training.period.testSampleFraction.001'
                  ,paste0(working,
                          'e-training-period--testSampleFraction-0.001000.txt'
                          )
                  ,lst
                  )
      b <- Append( 'path.in.e.training.period.testSampleFraction.01'
                  ,paste0(working,
                          'e-training-period--testSampleFraction-0.010000.txt'
                          )
                  ,a
                  )
     b
    }
    AppendDrawings <- function(lst) {
      a <- Append( 'path.in.scenarios'
                  ,paste0(drawings, 'scenarios.pdf')
                  ,lst
                  )
      a
    }
    AppendAdjustTrainingPeriodPaths <- function(lst) {
      name.prefix <- 'path.in.e.adjust.training.period..query.fraction.'
      value.prefix <- paste0( working
                       ,'e-adjust-training-period--query.fraction-'
                       )
      value.suffix <- paste0('.txt')
      a <- 
        Append( paste0(name.prefix, '001')
               ,paste0(value.prefix, '0.001000', value.suffix)
               ,lst
               )
      b <- 
        Append( paste0(name.prefix, '01')
               ,paste0(value.prefix, '0.010000', value.suffix)
               ,a
               )

      b
    }
    AppendPenalizedRegressionPaths <- function(lst) {
      name.prefix <- 'path.in.e.penalized.regression..query.fraction.'
      value.prefix <- paste0( working
                             ,'e-penalized-regression--query.fraction-'
                             )
      value.suffix <- '.txt'
      a <- Append( paste0(name.prefix, '001')
                  ,paste0(value.prefix, '0.001000', value.suffix)
                  ,lst
                  )
      b <- Append( paste0(name.prefix, '01')
                  ,paste0(value.prefix, '0.010000', value.suffix)
                  ,a
                  )
      b
    }
    AppendFeaturePaths <- function(lst) {
     LCV <- function(predictors, query.fraction, prior) {
      cat('LCV', predictors, query.fraction, '\n')
      name <-  sprintf( 'path.in.e.features.lcv.%s.%s'
                       ,predictors
                       ,query.fraction
                       )
      value <- sprintf( '%se-features--approach-lcv--predictors-%s--query.fraction-0.%s.chart1.txt'
                       ,working
                       ,predictors
                       ,query.fraction
                       )
      print(name); print(value); print(substr(value, 50, 200))
      lcv <- Append(name, value, lst)
     }
     PCAChart1 <- function(predictors, prior) {
       cat('PCAChart1', predictors, '\n')
       name <-  sprintf( 'path.in.e.features.pca.%s.chart1'
                        ,predictors
                        )
       value <- sprintf( '%se-features--approach-pca--predictors-%s.chart1.txt'
                        ,working
                        ,predictors
                        )
       print(name); print(value)
       Append(name, value, prior)
     }
     PCAChart2 <- function(predictors, subchart, prior) {
       cat('PCAChart2', predictors, subchart, '\n')
       name <-  sprintf( 'path.in.e.features.pca.%s.chart2.%s'
                        ,predictors
                        ,subchart
                        )
       value <- sprintf( '%se-features--approach-pca--predictors-%s.chart2.%s.txt'
                        ,working
                        ,predictors
                        ,subchart
                        )
       print(name); print(value)
       Append(name, value, prior)
    }
     a <- LCV('always', '001000', lst)
     b <- LCV('always', '010000', a)
     c <- PCAChart1('always', b)
     d <- PCAChart2('always', '01', c)
     e <- PCAChart2('always', '02', d)
     f <- PCAChart2('always', '03', e)
     #b
    }


    control <- list( cache = FALSE
                    #,cache = TRUE
                    ,include = TRUE
                    ,working = working
                    ,drawings = drawings
                    )
    control2 <- AppendAvmPaths(30, control)
    control3 <- AppendAvmPaths(60, control2)
    control4 <- AppendAvmPaths(90, control3)
    control5 <- AppendMedianPricePaths('month', 2006, 2009, control4)
    control6 <- AppendMedianPricePaths('year', 1984, 2009, control5)
    control7 <- AppendEformsPaths(control6)
    control8 <- AppendETrainingPeriodPaths(control7)
    control9 <- AppendDrawings(control8)
    control10 <- AppendAdjustTrainingPeriodPaths(control9)
    control11 <- AppendPenalizedRegressionPaths(control10)
    control12 <- AppendFeaturePaths(control11)
}

control <- Control()
str(control)
opts_chunk$set(cache = control$cache)
opts_chunk$set(include = control$include)
@

<<Commas>>=
Commas <- function(i) {
    # insert commas into an integer
    format(i, big.mark = ',')
}
# test
Commas(123456)
@

Linear models predict the price as a linear function of features:

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

From this simple idea, a huge variety of models can be generated by
varying the features $x_i$, by applying transformations to the outcome
$y$ or feature $x_i$, by considering different scenarios for using the
result model, by varying the training periods, by considering whether to
regularize, and many other factors.

Here we investigate the accuracy of linear models resulting from various
decisions. 


\begin{itemize}
\item Kinds of models
\item Model form
\item Characteristics of the training and testing periods
\item Selecting test data sets: randomly and for 2009
\item Choosing the training period
\item Feature choice
\item Training set choice
\item avm variants experiment
\item regularization
\end{itemize}

\section{Kinds of models: Scenarios}

<<KindsOfModels>>=
@

Real estate price prediction takes place in three scenarios (business
use cases). These differ largely in the business purpose of the estimate
of market value and the training data that can be used.


\begin{figure}[h]
\caption{Training Periods for Scenarios}
\label{figure:scenarios}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.scenarios}}
\end{figure}

\subsection{Assessor Scenario}

The first of these is the ``assessor'' scenario. The tax assessor for
Los Angeles County issues tax bills in November of every year. The first
installment is due later in that year and the second installment is due
in the following year. We have the tax roll for 2008, which was used to
generate tax bills that were issued starting November 1, 2008.

The tax bill contains the tax assessor's assessment for the property.
Were it not for California Proposition 13, the assessment would be for
the market value of the property sometimes in 2008. The valuation as-of
date is vague. Because of proposition 13, the assessed value is less
than the market value in periods of high real estate inflation.

Although the tax assessor does not disclose the market value, this value
must be determined, because the assessed value is required to be the
maximum of the market value and value imputed from the last sale
transaction increased at a rate of at most two percent per year.

The ``assessor'' scenario is an assessment of the market value in 2008.
As training data, the assessor can use all of the transactions recorded
before November 1, 2008. Note that the average delay between the sale
and recording of the sale is about 60 days. Figure \ref{figue:scenarios}
illustrates the various scenarios. For the assessor scenario, there can
be a gap between when a transaction is recorded and the query date for a
transaction. This potential gap is illustrated in the top third of the
figure.

\subsection{AVM Scenario}

The second scenario is the ``automated valuation model (AVM)'' scenario.
This scenario arises because many actors need to know the value of the
property before it transacts. Such actors include sellers, buyers, and
lenders on the property. There is a business in providing the price
predictions, which are called AVMs. Here ``automated'' means created
through a computer program to distinguish the estimates from those
created by human supposed experts using data such as the deeds
data.

AVM providers are incentive to have accurate estimates, so there is no
reason to build in systematic bias in their predictions. The can use all
as training data all the sales that are recorded up to date of the
predition. For example, to predict the market value of properties on
January 15, 2009, the AVM providers can use all the data reported up to
and included January 14, 2009. They can also use the tax assessor's
assessment as a feature. The assessment is by law partitioned into the
assessment for the land and the assessment for the improvements on the
land.

Discussions with Charlies Freeman show that AVM providers can be very
agressive in obtaining up-to-date transaction files. Some rely on
augmenting data from the tax assessor with data from real estate agents.
Some simply invest in obtaining more up-to-date data from the official
government sources.

The middle portion of Figure \ref{figure:scenarios} illustrates that the
training data in the AVM scenario can be up to just before the query
data and can have more recent data than is used for the assessor
scenario.

\subsection{Mortgage Scenario}

The third scenario is the ``mortgage'' scenario. This scenario arises in
order to train mortgage-default prediction models. 

Borrowers default on their mortgage by not making the payments. After a
certain number of missed payments, the lender may begin legal
proceedings to seize the property, which is collateral for the mortgage.
The process varies by state.

There are two major reasons for not making mortgage payments.
Historically the major reason was that the borrower could not afford to
make the payments, perhaps because of loss of income. If the house is
worth more than the mortgage and the real estate market is operating
quickly, rather than default, the borrower may be able to sell the house and use
the proceeds to pay off the mortgage and so that default would not occur. 
The sell-and-pay strategy does not work if the value of the house less
its selling costs is less than the amount owed on the mortgage. In this
situation, the house is said to be under water. In the 2007 United
States real estate crisis, many houses were under water.

The second reason for not making the payments was historically rare but
became more frequent with the 2007 real estate crisis: the borrower
had the cash to pay but decided to not pay. These ``strategic defaults''
are in part driven the house being under water.

To build a predictive model of defaults, the feature set often includes
the value of the house and the amount owed on the mortgage. In the
mortgage scenario, the value of the house is estimated in every relevant
time period. One is mimicking the borrower's estimate of the value of
the house.

In making the mortgage prediction for January 15, 2009, one can use all
the historic transactions, all the transactions on the date of the
estimate, and all the future transactions as well. One can also use any
transaction on the query house itself, as one is estimating what the
borrower would think the house is worth. An available feature is the
tax assessor's assessment.

The ability to use future transactions to predict a query transaction is
illustrated in the bottom third of Figure \ref{figure:scenarios}.

\section{Model form}

<<ModelForm>>=
@

Our linear model

\[ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n .\]

allows the variables to be transformed before entering the equation.

Two popular choices for the response variable $y$ are to set it to the
$price$ or the $\log(price)$. Some researchers including Bourassa
(\cite{bourassa-02} p. 11) transform the price into the log domain as a
means to make the resulting distribution ``more normal'' claiming that
doing so ``helps to normalize the distribution of the error term.'' But
does this transformation improve predictive accuracy? That question is
investigated below.

Another choice is to transform some of the features into the log domain.
One might do so when the price is in the log domain, so that the
regression equation in part becomes

\[ log(price) = \ldots + \beta log(x) + \ldots  .\]

The idea of this transformation (cite the econometrics books) is that a,
say, 10 percent change in the value of x would lead to a 10 percent
change in the value of the price. These transformation are used in
Chopra's work (cite his thesis), where all size-related features were
transformed into the log domain. The intuition is that doubling the size
would double the value.

We consider 4 forms of models and ask which provides the most accurate
predictions. The form and their interpretations are:

\begin{itemize}
\item level-level: $price = \ldots + \beta x_i + \ldots.$ A doubling
of x leads to $\beta$ factor increase in the price.
\item level-log: CHECK THE TEXT
\item log-level:
\item log-log
\end{itemize}

\section{Choosing the training period}

<<ChoosingTrainingPeriod>>=
control
path <- control$path.in.e.median.price.by.month.RData
loaded <- load(path)
loaded
str(median.prices)
MedianPrice <- function(year, month) {
  is.year <- median.prices$sale.year == year
  is.month <- median.prices$sale.month == month
  median.prices[is.year & is.month, 'median.price']
}
MedianPrice(2009, 3) #test
median.test.price <- c( MedianPrice(2008, 11)
                       ,MedianPrice(2008, 12)
                       ,MedianPrice(2009, 01)
                       ,MedianPrice(2009, 02)
                       ,MedianPrice(2009, 03)
                       )
median.test.price
median.median.test.price <- median(median.test.price)
median.median.test.price
@

The linear model uses the average value of each feature to estimate the
total value of the house. When prices are moving rapidly, feature values
may also be moving rapidly. It's important to think through the training
data set for a given estimation task.

\begin{figure}[h]
\caption{Median Prices By Year}
\label{figure:medianPricesByYear}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.e.median.price.by.year.pdf}}
\end{figure}

<<MedianPricesByYear>>=
@

In Figure \ref{figure:medianPricesByYear}, we see the median prices of
arms-length transactions for single-family residences in our subset1.
You can see that prices have not been steady and that there have been
periods of steady price increases followed by periods of price declines.
The decline in prices after 2007 was not the only price decrease: there
was another decline starting in 1990 that was sustained for about six
years. 

\begin{figure}[h]
\caption{Median Prices By Month}
\label{figure:medianPricesByMonth}
\centering\includegraphics[scale=.4]{\Sexpr{control$path.in.e.median.price.by.month.pdf}}
\end{figure}

Figure \ref{figure:medianPricesByMonth} depicts median prices by month
starting in January 2006. We see approximately level prices starting
early in 2006, a spike in mid July 2007, and then the well-known
decline. The difficulty of forecasting future median prices can been
seen: What is the clue that July 2007 would be the peak? Median prices
increased in February 2008 but declined to a new low in March 2008.

The impliciation for linear models is that the model builder faces a
dilema: train on too long a period, and the features-level prices
found by the fitting process for the model are likely to be out-of-date;
train on too short a period, and the volume of training data may be too
low to accuralate fit to feature prices.

One possible way around this dilema is to successfully forecast median
prices for the test period: will they be lower, about the same, or
higher than in the training period. The trained feature prices could
then be adjusted. However, such a forecast is beyond the scope of this work.

Another possible way around this dilema is to incorporate time periods
explicitly in the model. For example Bourassa in \cite{bourassa-02}
builds a model for real estate prices that includes as dummy features the
quarter of the transaction: the features are \code{is-quarter-1},
\code{is-quarter-2}, and \code{is-quarter-3}. These features capture
average price movement and can be used in our mortgage scenarios,
because training data about the future related to the query transaction
are needed.

A third possibility is to limit the training period to a short enough
period to capture just recent prices instead of historic for the
training. This approach doesn't try to anticipate market movement after
the training period to the query period. It's the approach followed in
this work.

A fourth possibility is to determine the best training period for the
date before the query transaction, the use that training period for the
query transaction. Determine the best through cross validation.


\section{Feature choice}
\label{Section:FeatureChoice}

<<FeatureChoice>>=
@

Features used in the literature.

\subsection{bourassa-02}

paper: bourassa-02 Do housing submarkets really matter? pages 10 and 11.
Estimated log(price). These variables were found to ``contribute
explanatory power to the estimated equations:''

features used in hedonic (linear) models
\begin{itemize}
\item intercept
\item log(floor area)
\item log(land area)
\item cross-leaded or strata-titled (a form of ownership)
\item detached dwelling
\item age of dwelling
\item age of dwelling squared
\item wall condition: good, average, or bad 
\item roof materials: tile, metal, or other
\item wall materials: wood, brick, fibrolite, or other 
\item quality of the principal structure: superior, average, or poor 
\item log(distance to the central business district)
\item quarterly time-period dummy
\item water view
\item modernization
\item landscaping: good, average, or poor
\item driveway
\item neighborhood: very good, good, average, or poor
\item number of attached garages
\end{itemize}

age of dwelling and age squared (because the value is expected to
follow a U-shaped curve ``because very old houses can earn a premium do
to their historic character and distinctive neighborhood.''

Why land area and floor area in log domain? ``to reflect the likelihood
of diminishing returns [to price] as ... the values of those variables
increase in size.''

Use log(distance to CBD) ``in view of theory and empirical evidence."

Estimate log(price) ``as a means for making its distribution more
normal, which in turn helps to normalize the distribution of the error
terms, a desirable characteristic for ordinary least squares
estimators.'' Example of a statistical consideration that may or may not
improve model accuracy.

Evaluation criteria: fraction of out-of-sample predictions that are
within 10 and 20 percent of the true prices.

Predicted prices were adjusted ``for the average residual for the
neighborhood in which the property is located.'' A neighborhood was
defined as a portion of the tax roll containing about 1,000 properties.

\subsection{bourassa-10}

Predicting house prices with spatial dependence: a comparison of
alternative methods

Primary criterion: percentage of ex-sample predictions with 10 percent
of the transaction price

Dependent variables is log of sales price.

features used in OLS models
\begin{itemize}
\item intercept
\item land area
\item land area squared
\item floor area
\item bathrooms: no more than 1, 1.5 or 2, 2.5 or more
\item age of house
\item age of house squared
\item basement: non, partial, full
\item whether central air-conditioning
\item whether fireplace
\item number of garages
\item quarterly dummy (estimates were for 1999)
\end{itemize}

\subsection{zurada-11}

A comparison of regression and artificial intelligence methods in a mass
appraisal context

Features used
\begin{itemize}
\item number of baths: sub-standard (0), 1, 1 1/2, 2, 2 1/2, 3, more
than 3
\item whether central air
\item lot size: small ( no more than 0.25 acres), medium (up to 0.5
acres), large (up to .75 acres), tract (more than 1 acre)
\item construction type: 1 story, 1.5 stories, 2 stories
\item wall type frame, brick, other
\item basement type: none, partial, full
\item basement code: none, standard
\item garage type: none, carport, detached, attached, garage in
basement, or built-in garage
\end{itemize}

\subsection{chopra-09}

\subsubsection{chopra model 1}

These features were used by Sumit Chopra \cite{chopra-09} page 87 in
developing a  ``relational regression model.'' Chopra predicted
log(price). Features used:
\begin{itemize}
\item living area
\item year buit
\item number of bedrooms
\item number of bathrooms
\item pool or no pool
\item prior sale price
\item parking spaces
\item parking types
\item lot acerage
\item land value
\item improvement value
\item percent improvement value
\item new construction
\item foundation
\item roof type
\item head type
\item site influence
\item latitude
\item longitude
\item census tract median household income
\item census tract proportion of units that are owner occupied
\item census tract average commute time to work
\item academic performance index for the school district
\end{itemize}

Transformation performed
\begin{itemize}
\item Drop all records with one or more missing fields, leaving 42,025
transactions
\item Map all fields for price, area, and income into the log space
\item Code all non-numeric discrete variables (such as pool, parking
type) using 1-of-K coding
\item Normalize all variables to have mean 0 and a standard deviation
between -1 and 1
\end{itemize}

Use only deeds and taxroll info for the year 2004. Treat data in the
first 90 percent of the year as training data. Use data in the last 10
percent of the year as testing samples. As a validation set to calibrate
models, take 10 percent of the 90 percent.

Model form was a relational factor graph. This model assumes the house
price has two components: a ``intrinsic price'' based on the features of
the house and an ``desirability price'' based on the desirability of the
neighborhood. Since the desirability is not observed, it is modeled as a
latent variable. The intrinsic price was modeled with a neural network
having 250 units in the first hidden layer, 80 units in the second
hidden layer, and 1 unit (the log price) in the output
layers.

Predict log(price) not price ``in order to avoid a bias towards
expensive houses.''

\subsubsection{chopra model 2}

This data set is similar to what I have.

Used deeds from January 1984 to April 2008.

Variables used from the deeds tape:
\begin{itemize}
\item sale price
\item recording date
\item sale date
\item transaction category code
\item transaction type coe
\item document type code
\end{itemize}

Variables used from the tax-roll tape:
\begin{itemize}
\item APN number
\item land square footage
\item living square feet
\item mailing address, used to determine GPS coordates
\end{itemize}

Data cleaning process (designed to correspond the the Case-Shiller
index)
\begin{itemize}
\item Keep just these deeds
  \begin{itemize}
  \item Transaction category code was arms-length transaction
  \item Transaction type code was either resale or new construction
  \item Document type code was either grant deed or foreclosure
  \end{itemize}
\item Remove any record with one or more missing deeds or taxroll fields
or GPS fields
\item Remove all transactions on same house on same date with different
sales prices.
\item Remove all but one transaction on same house on same date with the
same sales price.
\item Remove repeat transactions for houses that happen within 7 months
(on the assumption that the house had undergone structural change and
hence is not a valid repeat sales transaction). In addition, decouple
(treat as not repeat sales) transactions ``before and after this dirty
transaction.''
\item Remove transactions that are part of repeat sales pairs in whic
the effective year built changed.
\item Remove repeat sales transaction pairs with sales prices less than
5,000 or more than 100,000,000 dollars.
\item Remove repeat sales transaction pairs with annualized returns les than
-50 or greater than 100 percent.
\item Remove repeat sales transaction pairs that ``[out] perform or
under perform that median house price index by more or less than 25
percent on an annualized basis.''
\item Remove any single sale transaction with a price higher than the
maximimum transaction price in any repeat sales. Also remove single sale
transactions with a price lower than the minimimum in any repeat sale.
\end{itemize}

The cleaning process left 591,239 repeat sales transactions and 367,973
single sales transactions, for a total of 1,550,451 transactions over 24
years in Los Angeles Country.


\section{Is the assessment a valuable feature?}

The assessment might have value as a feature in the predictive model,
especially if it was for a recent time period and the assessment was a
statement of the market value. For Los Angeles Country, the assessment
is for a vague date in November 1, 2007 through October 31, 2008 and is
for the market value only if the house sold during that time period.
Hence that the assessment is of value as a feature in predictive models
is not certain.



<<ActualLosAngelesData>>=
@

<<CompareEAvmVariants>>=
print(control)
GenError <- function(path) {
  print(path)
  loaded <- load(path)
  str(loaded)
  str(gen.error)
  gen.error
}
gen.error.30 <-
  GenError(control$path.in.e.avm.variants.training.30.RData)
gen.error.60 <-
  GenError(control$path.in.e.avm.variants.training.60.RData)
gen.error.90 <-
  GenError(control$path.in.e.avm.variants.training.90.RData)

MedianError <- function(scenario, ndays) {
  gen.error <- if (ndays == 30) gen.error.30 
            else (if (ndays == 60) gen.error.60
                  else gen.error.90
                  )
  median.error <- gen.error[[scenario]]$median.RMedianSE
  cat('median.error', median.error, scenario, ndays, '\n')
  median.error
}
MedianError('mortgage with assessment', 90)  # test
# combine and produce a table
# <scenario> <assessment usage> <30 day error rate> <60> <90>
@

Recall that monthly median prices were falling rapidly starting in July
2007, with an uptick in February 2009 and a decline in March 2009.

We expect that longer training periods will pick up increased
out-of-test-period feature prices and hence lead to worse overall
performance.

In these experiments, we use a log-linear model, because these models
are popular in the literature. We estimate the generalization error
using 10-fold cross validation. The feature set is as described above in
section \ref{Section:FeatureChoice}. The test set is all transactions
for which the assessment might be relevant, namely transactions in the
period November 1, 2008 through March 31, 2009. The training period is
30, 60, or 90 days.  For the assessor and avm scenarios, the training
period is before the date of the query transactions. For the mortgage
scenario, the training period is split to be half before and half after
the date of the query transactions. 

\subsection{Training on 30 days of data}

<<TraingingOn30Days>>=
@


\begin{figure}[h]
\caption{Los Angeles Data Trained for 30 Days}
\label{figure:LosAngelesData30}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.avm.variants.training.30.txt}}
\end{figure}

In Figure \ref{figure:LosAngelesData30} we see the averaged test errors
from 10-fold cross validation for the scenarios of interest with and
without the assessment using a training period of 30 days.

The first finding is that estimated generalization errors are very high
when estimated using the mean root-mean-squared error of the
cross-validation folds. The median price during the testing period is
\Sexpr{Commas(median.median.test.price)}
dollars and these mean errors are about the same size. The reason is
that the models badly predict a few very expensive houses by valuing
them at enormous prices, leading to huge mean price differences. To
reduce the impact of these few houses on the estimated generalization
error, we also determine the median of the average errors from the
folds, where the average error from the fold is also the median. These
results are reported in the column ``median RMedianSE'' which stands for
the median of the square root of the median squared errors from the
folds. These error estimates are much lower.

Next we find that the assessment is not always a valuable feature. Using
the assessment slightly improves the median errors for the assessor
scenario and slightly degrades the median errors for the AVM scenario.
When prices are moving rapidly, the assessment becomes out of date and
is not of certain value.

The next finding is that median errors are lower when the training data
are nearer the test transactions. Ignoring the use cases without the
assessments, we see that the median error decreases as we use training
data that are closer to the query date: the median error decreases from
\Sexpr{Commas(MedianError('assessor with assessment', 30))}
for the assessor scenario with the assessment to
\Sexpr{Commas(MedianError('AVM with assessment', 30))}
for the AVM scenario with the assessment. The only difference is
that the assessor data is about two months out of date compared to the
AVM data. We see that the median error for the mortgage scenario is
\Sexpr{Commas(MedianError('mortgage with assessment', 30))}
which is the lowest and uses 30 days of data that is centered on the
date of the query transaction. When prices are moving rapidly, training
on data near the training date is needed in order to price the features
at market values.


\subsection{Training on 60 days of data}

<<TrainingOn60Days>>=
@

\begin{figure}[h]
\caption{Los Angeles Data Trained for 60 Days}
\label{figure:LosAngelesData60}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.avm.variants.training.60.txt}}
\end{figure}
 
Figure \ref{figure:LosAngelesData60} shows results from using a 60-day
training period. Focusing on the medians, we see that as for the 30-day
training period, the estimated generalization error sometimes increases
when the assessment is used as a feature. 

As in the 30-day use case, training on more contemporary data leads to
more accurate estimates.

\subsection{Training on 90 days of data}

<<TrainingOn90Days>>=
@

\begin{figure}[h]
\caption{Los Angeles Data Trained for 90 Days}
\label{figure:LosAngelesData90}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.avm.variants.training.90.txt}}
\end{figure}

Figure \ref{figure:LosAngelesData90} shows results from using a 90-day
training period. The training data are more out of date compared to the
30 and 60-day training periods. Now the assessment is always valuable.

As in the 30-day use case, training on more contemporary data leads to
more accurate estimates.

\subsection{Comparing 30, 60, and 90 training-day accuracy}

<<Comparing306090>>=
@

<<ME>>=
ME <- function(scenario, ndays)
  as.integer(MedianError(scenario, ndays))
@

\begin{figure}[h]
\caption{Los Angeles Data Median RMedianSE}
\label{figure:LosAngelesDataMedian}
\begin{tabular}{| l | r | r | r |}
\hline
         & training & training & training \\ 
scenario & 30 days  & 60 days  & 90 days \\ \hline
assessor without assessment & 
\Sexpr{ME('assessor without assessment', 30)} &
\Sexpr{ME('assessor without assessment', 60)} &
\Sexpr{ME('assessor without assessment', 90)} 
\\ \hline
assessor with assessment & 
\Sexpr{ME('assessor with assessment', 30)} &
\Sexpr{ME('assessor with assessment', 60)} &
\Sexpr{ME('assessor with assessment', 90)} 
\\ \hline
AVM without assessment & 
\Sexpr{ME('AVM without assessment', 30)} &
\Sexpr{ME('AVM without assessment', 60)} &
\Sexpr{ME('AVM without assessment', 90)} 
\\ \hline
AVM with assessment & 
\Sexpr{ME('AVM with assessment', 30)} &
\Sexpr{ME('AVM with assessment', 60)} &
\Sexpr{ME('AVM with assessment', 90)} 
\\ \hline
mortgage with assessment & 
\Sexpr{ME('mortgage with assessment', 30)} &
\Sexpr{ME('mortgage with assessment', 60)} &
\Sexpr{ME('mortgage with assessment', 90)} 
\\ \hline
\end{tabular}
\end{figure}

Figure \ref{figure:LosAngelesDataMedian} compares median errors
across scenarios for 30, 60, and 90 days.

We see that longer training periods usually result in higher errors.

We see that training on more contemporary data always results in lower
errors.

\subsection{Some potential myths}

It is a myth that the assessment always has value. We can see in these
experiments that using the assessment as a feature degrades the accuracy
of the estimated price, because the assessment becomes increasingly out
of date as the market moves rapidly. 

It is a myth that more training data leads to more accurate estimates.
While it is true that having more training data available generates
options for model design that may lead to increased accuracy, actually
using more out-of-date data degrades estimation quality in these
experiments.

\subsection{Implications for this work}

Because the tax assessor's assessment often does not have value, we can
examine all the transaction data we have, instead of just the data for
which we have the assessments. Thus we can examine 25 years of data
instead of just the data in late 2008 and early 2009.


\section{Do log-level models outperform other model forms?}

<<ChooseModelForm>>=
@

We compare the estimated generalization performance of 4 model forms for
the AVM scenario for several choices of training periods. The model
forms are:
\begin{itemize}
\item level-level: $price = \beta_0 + \ldots + \beta_i f_i + \ldots$
\item level-log: $price = \beta_0 + \ldots + \beta_{si} f_{si} + \ldots
+ \beta_{oi} f_{oi} + \ldots$

\item log-level: $\log(price) = \beta_0 + \ldots + \beta_i f_i + \ldots$
\item log-log: $\log(price) = \beta_0 + \ldots + \beta_{si} f_{si} + \ldots
+ \beta_{oi} f_{oi} + \ldots$
\end{itemize}
where $i$ indexes a feature, $si$ indexes size features such as the lot
size, interior space in the house, number of bedrooms, number of
bathrooms, and $oi$ indices non-size (other) features such as whether the
house has a swimming pool.

The training periods considered are 30 days before the query date (the
date of the test transaction), 90 days before the query date, and 120
days before the query date.

\subsection{Training on 30 days of data}

\begin{figure}[h]
\caption{Model Form Comparison 30 Days 0.1 Percent Sample}
\label{figure:Form30001}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.forms.trainingDays.30.sample.001}}
\end{figure}

In Figure \ref{figure:Form30001} we see estimated generalization errors
derived from 10-fold cross validation for the AVM scenario trained for a
30-day period. There are about 100,000 test samples over the 25 year
period. A separate model is fit to each of them and that fitted model is
used to predict the test sample.  Fitting 100,000 models was judged to
take too much time, so instead a random sample of 0.1 percent was drawn
and used as the test sample.

The median of the square root of the median squared errors form each
fold (``Median RMedianSE'') is taken as one measure of the
generalization error. For this measurement, the best model has a
log-level form: the log of the price is estimated from the other
variables, which are not transformed. The second best model is the
log-log model, which estimates the log of the price from a linear
combination of size variables, for which the log is taken, and non-size
variables, which enter the model without transformation.

Another popular metric for measuring accuracy of house price model is
the determine the fraction of estimates in the test sample that lie
within 10, 20, and 30 percent of the known true price. A larger value is
better. Using this metric we see that the log-level model is again best
and that the log-log model performs close to the log-level model.

The third metric is the fraction of samples in the test samples that
could be estimated. The inability to measure a test sample is primarily
caused by having training data that is not rich enough for a specific
test sample. A common failure is to have a training sample that contains
only one value for a factor such as whether the house is new
construction or whether the house has a swimming pool. Features that are
not rich enough (like these two) could be omitted from the model, at
perhaps some loss of predictive power. We see that the mean coverage is
best for the log-log model, but since mean coverage values are so close,
the differences may be due to the specific random samples drawn.

Thus we conclude based on a 0.1 percent sample of the available training
data using a 30-day training period that the log-level model is best and
that the log-log model is second best.

\begin{figure}[h]
\caption{Model Form Comparison 30 Days 1 Percent Sample}
\label{figure:Form30010}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.forms.trainingDays.30.sample.01}}
\end{figure}

Figure \ref{figure:Form30010} is the same as the previous figure except
that the size of the training sample is increased to 1 percent of all
available samples. We see that with a bigger random sample, the log-log
model now performs slightly better than the log-level model as measured
by both the median RMedianSE and mean fraction within 10 percent.
Coverages are all about the same.

\subsection{Training on 90 days of data}

\begin{figure}[h]
\caption{Model Form Comparison 90 Days 0.1 Percent Sample}
\label{figure:Form90001}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.forms.trainingDays.90.sample.001}}
\end{figure}

Figure \ref{figure:Form90001} shows that the log-level model outperforms
the log-log model as measured by the median RMedianSE metric and that
log-log outperforms log-linear as measured by the mean fraction within
10 percent. These conclusions are for a 0.1 percent random sample of the
test data in the cross validation process.

\begin{figure}[h]
\caption{Model Form Comparison 90 Days 1 Percent Sample}
\label{figure:Form90010}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.forms.trainingDays.90.sample.01}}
\end{figure}

For a 1 percent random sample, Figure \ref{figure:Form90010} shows the
log-log model outperforming the log-linear model as measured by median
RMedianSE and the log-level model outperforming the log-log model as
measured by mean fraction with 10 percent.

\subsection{Training on 120 days of data}

\begin{figure}[h]
\caption{Model Form Comparison 120 Days 0.1 Percent Sample}
\label{figure:Form120001}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.forms.trainingDays.120.sample.001}}
\end{figure}

Figure \ref{figure:Form120001} shows that the log-level model outperforms
the log-log model as measured by the median RMedianSE metric and that
log-log outperforms log-linear as measured by the mean fraction within
10 percent. These conclusions are for a 0.1 percent random sample of the
test data in the cross validation process.

\begin{figure}[h]
\caption{Model Form Comparison 120 Days 1 Percent Sample}
\label{figure:Form120010}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.forms.trainingDays.120.sample.01}}
\end{figure}

For a 1 percent random sample, Figure \ref{figure:Form120010} shows the
log-log model outperforming the log-linear model as measured by median
RMedianSE and the log-level model outperforming the log-log model as
measured by mean fraction with 10 percent.

\subsection{Looking across training periods and sample sizes}

In these experiments, the models that predict $price$ instead of
$log(price)$ always were the worse performers: One should predict
$log(price)$ in any of the experiments.

A clear choice between choosing between a log-level and log-log model is
not seen in these experiments: sometimes one is slightly better,
sometimes the other. Which is better may depend on the metric choses to
gauge the best model: median RMedianSE or mean fraction within 10
percent.

\section{Which training period is best?}

We examine which training period is best. There is a clear tradeoff in
setting the size of the training period: if it is short, the prices
discovered by the linear model are more contemporaneous and capture
recent price levels. If the training period is longer, the prices may be
more reliable because they were trained on more data.

Here we examine various training periods using a log-level model form.

We start with an examination of experimental results and then present
some ideas for dynamically tuning the training period.

\subsection{Experimental Results}


\begin{figure}[h]
\caption{Model Training Period Comparison 0.1 Percent Sample}
\label{figure:TrainingPeriod001}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.training.period.testSampleFraction.001}}
\end{figure}

Figure \ref{figure:TrainingPeriod001} presents the estimated
generalization error found through 10-fold cross validation for time
periods ranging from 30 to 180 days for a log-linear model. The best
performance as measured by the median RMedianSE is for 90 days. The best
performance as measured by mean fraction within 10 percent is for 30
days. Coverage is 100 percent except for the 30 day period. This
experiment used a 0.1 percent random sample of the test data in each
cross validation fold.

\begin{figure}[h]
\caption{Model Training Period Comparison 1 Percent Sample}
\label{figure:TrainingPeriod01}
\tiny
%\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.training.period.testSampleFraction.01}}
\end{figure}

Figure \ref{figure:TrainingPeriod01} repeats the analysis but uses a 1
percent sample of the data in the test folds. As before, the best
performance as measured by the median RMedianSE is for 90 days and the
best performance as measured by the mean fraction within 10 percent is
for 30 days. For the mean fraction within 10 percent, the differences
between the 30 and 90 day performance is very small, however. Coverage
is again 100 percent except for the 30 day period.

One might choose a 90 day training period for the 25 year period,
perhaps giving up a bit of accuracy for increased coverage.

\subsection{Tuning the Training Period}

The experiments just above find the best training period on average
across all the samples. One could select 90 days as the best training
period.

The question is can one develop a more accurate model by tuning the
training period to the query transaction. One idea is to determine the
best training period at the time of the query transaction and use that
best training period for the query transaction. Under this idea, every
query transaction would have a specifically-selected training period.

We implemented this idea by considering all the transactions the day
before the query date. For these transactions, we determined the
RMedianSE for training periods of 30, 60, 90, 120, 150, and 180 days and
selected the training period with the lowest error. We then used that
training period for the query transaction. For example, if using a
60-day training period leads to the lowest errors on transactions the
day before the query, we used a 60-day training period for the query
transaction. There is one refinement: if there are no transactions the
day before the query transaction, then the test period for determining
the best training period is extended day by day into the past until we
have found some test transactions.

<<TuningTheTrainingPeriod>>=
control$path.in.e.adjust.training.period..query.fraction.001
control$path.in.e.adjust.training.period..query.fraction.01
@

\begin{figure}[h]
\caption{Tuned Training Period 0.1 Percent Sample}
\label{figure:TunedTrainingPeriod001}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.adjust.training.period..query.fraction.001}}
\end{figure}

\begin{figure}[h]
\caption{Tuned Training Period 1 Percent Sample}
\label{figure:TunedTrainingPeriod01}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{control$path.in.e.adjust.training.period..query.fraction.01}}
\end{figure}

TODO: capture the figures programmatically

TODO: check that the results are true programmatically

Figure \ref{figure:TunedTrainingPeriod001} shows the results for a 0.1
percent random sample. The RMedianSE is 32,396, which is higher than the
RMedianSE when fixing the training period at 90 days (30,844).

Figure \ref{figure:TunedTrainingPeriod01} shows similar results for a 1
percent random sample. The RMedianSE is 32,066, which is again higher
than the RMedianSE when fixing the training period at 90 days (31,013).

The most straight-forward interpretation of these results is that the
proposed tuning procedure does not work. A possible reason for it failing
is that the best number of training days to use is not a
smoothly-changing function of the query date: small changes in the date
lead to large changes in the best number of training days to use.
On the other hand, it may be that the best number of training days to
use is smooth, but that our granularity in estimating it is too large:
we examine only 30, 60, 90, 120, 150, and 180 day periods.

Another possible interpretation is that the differences in RMedianSE
values is caused in part by selection of different 0.1 percent and 1
percent random samples from the data. 

Returning to the interpretation of the results for the two figures, we
see that the best training period is often 30 days or 180 days. The
modal nature of the extreme values is consistent with supposing that a
short training period is appropriate when prices are violating and a long
training period is appropriate when prices are stable. The modal nature
of the extreme values also suggests assessing the accuracy of models with
both shorter and longer training periods than were done in these
experiments. Thus there may be merit to the idea of tuning the training
period to the volatility of prices at the time of the query, and the
proposed method simply fails to implement successfully that idea.

\subsection{Other Ideas for Tuning the Training Period}

This work has explored tuning the training period based on transactions
just before the query period. The proposed method does not give more
accurate results than simply selecting a best-on-average training
period. The work does suggest that there is merit in the idea of tuning
the training period.

Another approach, not implemented here, is to directly determine the
volatility of the prices in the time period before the query
transaction. The volatility would be an input to a function that yielded
shorter training periods for times of higher volatility and longer training
periods for times of lower volatility.

\section{Which features are best?}

So far is this work, we have used all 24 features that occur in every
one of our 1.2 million transactions. We now consider which of these
features is best, where ``best'' means leads to the lowest expected
generalization error.

We used two approaches to identify these features: a heuristic and
principle components analysis.

\subsection{The Heuristic}

<<Naive>>=
# count number of models in naive approach
sum <- 0
num.features <- 24
for (k in 1:num.features) sum <- sum + choose(num.features, k)
sum
@

A straight-forward way to determine which features are most valuable as
measured by an estimated generalization error is to select every combination
of the 24 features taken 1, 2, \ldots, 24 at a time and use cross
validation to compare errors in the folds. There would be
\Sexpr{Commas(sum)} such models.

Our alternative idea is to use the lasso (\cite{hastie-01} page 64) to
select subsets of the 24 features. The version of lasso that we use is
the elastic net (\cite{zou-05}), which combines $L2$ regularization and
feature selection, and is claimed to ``outperform the lasso, while
enjoying a similar sparsity of representation.'' The implementation is
in the R \cite{r-14} package \code{elasticnet} (\cite{zou-12}) which
provides a rank-ordering of the features in terms of their
importance. 

<<FeaturesLCV>>=
pgm <- 'e-features-lcv-chart'
options <- '--query-100'
base <- paste0(control$working, pgm, options)
path.1 <- paste0(base, '_1.txt')
#path.2 <- paste0(base, '_2.txt')
#path.3 <- paste0(base, '_3.pdf')
path.4 <- paste0(base, '_4.pdf')
#path.5 <- paste0(base, '_5.pdf')

path.1
substr(path.1, 50, 200)

path.4
substr(path.4, 50, 200)
@




\begin{figure}[h]
\caption{Dropping Features\\1 Percent Sample\\Features Rank Ordered}
\label{figure:LCVNames}
%\tiny
%\small
\normalsize
\verbatiminput{\Sexpr{path.1}}
\end{figure}

For our transaction set, the rank-ordering of the features is in Figure
\ref{figure:LCVNames}. The rank-ordering is obtained by setting the $L2$
regularization hyperparameter of the elastic net procedure to $0$,
resulting in a pure lasso-type regularization. The figure depicts the
features in the order that the entered the model.

We see that the most important feature is the living area. For these
data, it is not true that the most important feature is location,
location, location, as claimed by real estate agents. In fact,
median.household.income, a feature of the location, is the second-most
important feature.

Examining the 10 most highly-ranked features, we see that these features
of the house itself are the most important: living area, fireplace
number, the year the house was built, land square footage, and whether there is a swimming pool. These
features are consistent with my intuition, except for the number of
fireplaces. Perhaps the number of fireplaces is correlated with other
desirable features of the house. 

The most important features of the location are the median household
income, the average commuting time, and whether there is industry in the
zip code and census tract. These are again intuitively consistent for
me.

Looking at the lowly-ranked features, a surprise is that the
least-important feature is the number of rooms. Perhaps this is because
living area is very important and adding rooms to the living area simply
cuts up the space into smaller rooms. Whether the zip code has a store
is of low importance, but whether the census tract has a store is of
medium importance.


\begin{figure}[h]
\caption{Dropping Features\\1 Percent Sample\\Cross Validation Results}
\label{figure:LCVEstGenError}
\centering\includegraphics[scale=.8]{\Sexpr{path.4}}
\end{figure}

We built 24 models as input to a 10-fold cross validation procedure. The
first model has just the most highly-ranked feature: the living area.
The second model has the two most highly-ranked features: living area
and median household income. The 24th model had all the features.

Figure \ref{figure:LCVEstGenError} depicts the cross validation results.
We measure the median value across the folds of the square root of the
median square errors (which we call code{medianRMedianSE}). Standard
errors are depicted by the tick marks above and below the plot points.

We see that the estimated generalization error falls rapidly until the
sixth feature is added. There are local minima at 9 features, 14
features, and 21 features. The standard errors are small.

All these models might be appealing from a sparsity perspective. Later
we will compare these models to models derived from the PCA analysis.



\subsection{Using PCA to deduce feature importance}

<<PCA>>=
pgm <- 'e-features-pca-chart'
options <- ''
base <- paste0(control$working, pgm, options)
path.1 <-   paste0(base, '_1.txt')
path.2.1 <- paste0(base, '_2_01.txt')
path.2.2 <- paste0(base, '_2_02.txt')
path.2.3 <- paste0(base, '_2_03.txt')

path.1
substr(path.1, 50, 200)
@

We used principal components analysis to inform the discussion about
which features are most important. 


\begin{figure}[h]
\caption{PCA For Features Always Present}
\label{figure:PCAAlways1}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.1}}
\end{figure}

Figure \ref{figure:PCAAlways1} shows the cumulative fraction of variance
accounted for the the principal components derived from using the
features that are always present. We see that the first three
principal components account for almost all of the variance; hence we
focus on understanding these components.


\begin{figure}[h]
\caption{PCA For Features Always Present: Principal Component 1}
\label{figure:PCAAlways21}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.2.1}}
\end{figure}

Figure \ref{figure:PCAAlways21} shows the weights of the features in the
first principal component. By far the largest weight is for the median
household income, a feature of the census tract and hence the location.

\begin{figure}[h]
\caption{PCA For Features Always Present: Principal Component 2}
\label{figure:PCAAlways22}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.2.2}}
\end{figure}

Figure \ref{figure:PCAAlways22} shows the weights for the features in
the second principal component. This time the most important feature by
far is the land square footage, which is a feature of the property. It
is an enduring feature of the property, because it cannot be redefined
through construction except by merging the property with another.


\begin{figure}[h]
\caption{PCA For Features Always Present: Principal Component 3}
\label{figure:PCAAlways23}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.2.3}}
\end{figure}

Figure \ref{figure:PCAAlways23} shows the weights for the features in
the third principal component. This time the most important feature by
far is the size of the basement, a feature of the construction on the
property. This feature may have more information than it may seem to
have: it indicates something about the land (perhaps no basement means
the land is damp) and something about the total square footage of the
house (the other levels are built on top of the basement).


\subsection{A reduced-feature model}

The LCV analysis suggests testing these models with the first 6, 9, 14,
and 21 features.

The PCA analysis suggest testing models with these feature sets:
\begin{itemize}
\item Only the median household income
\item Both the median household income and the land square footage
\item All three: income, land size, and basement size. 
\item All features in the first 3 principal components with an absolute
weight above 0.01. The features are the three already discussed
plus the living area.
\end{itemize}

<<ReducedFeatureModel>>=
pgm <- 'e-reduced-features-chart'
options <- '--query-100'
base <- paste0(
control$working
,pgm
,options
)
path.1 <-   paste0(base, '_1.txt')
path.4 <-   paste0(base, '_4.pdf')
path.6 <-   paste0(base, '_6.pdf')

path.1
substr(path.1, 50, 200)
@


\begin{figure}[h]
\caption{Reduced Feature Models: Identification}
\label{figure:ReducedFeatureIdentification}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.1}}
\end{figure}

Figue \ref{figure:ReducedFeatureIdentification} identifies the 7 models
tested. For each, we estimated the generalization error using 10-fold
cross validation on a 1 percent sample for the test transactions in each
fold.

\begin{figure}[h]
\caption{Reduced Feature Models: Cross Validated Median RMedianSE
Values}
\label{figure:ReducedFeatureCrossValidated}
\centering\includegraphics[scale=.8]{\Sexpr{path.4}}
\end{figure}

\begin{figure}[h]
\caption{Reduced Feature Models: Cross Validated Median RMedianSE
Values}
\label{figure:ReducedFeatureCrossValidated}
\centering\includegraphics[scale=.8]{\Sexpr{path.6}}
\end{figure}

Figure \ref{figure:ReducedFeatureCrossValidated} shows that all the
models derived from the heuristic (called ``LCV'') outperform all the
models derived from PCA. The best model is the first model, which has
the six most important features found through the heuristic. The
features are:
\begin{itemize}
\item living area
\item median household income
\item avg commute time
\item fireplace number
\item year built
\item fraction owner occupied.
\end{itemize}

Half of these are features of the house, the other half are features of
the location.


OLDER STUFF BELOW ME



Thus we ran a cross validation study on the seven suggested models as
identified in Figure \ref{figure:ReducedFeatureIdentification}. We see
in Figure \ref{figure:ReducedFeatureCrossValidated} that
all of the reduced-feature models identified through our LCV heuristic
outperformed all the models identified through our PCA experiments. The
best model is the model with 23 features.

\section{Ridge Regression}

The best model identified so far as based on 21 features. It is not
regularized. In these experiments, we regularize it to detemine if
regularization leads to better peformance.


\section{Submarket}

<<Submarkets>>=
pgm <- 'e-submarkets-chart'
options <- '--query-100'
base<- paste0(control$working, pgm, options)
base
path.1 <- paste0(base, '_1.txt')
path.4 <- paste0(base, '_4.pdf')
@


\begin{figure}[h]
\caption{Submarket Models: Identification}
\label{figure:SubmarketsIdentification}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.1}}
\end{figure}

\begin{figure}[h]
\caption{Submarket Models: Cross Validated Median RMedianSE
Values}
\label{figure:SubmarketsCrossValidated}
\centering\includegraphics[scale=.4]{\Sexpr{path.4}}
\end{figure}

In the literature (WRITE ME) say that models built for submarkets are
said to perform better. Quote barrasa saying that what the realtors know
in Aukland is best (and that is neighborhood names).

Figure \ref{figure:SubmarketsIdentification} contains the identifiers
for several models designed to test whether submarkets-based models lead
to improved prediction accuracy. The first model is the model we have
been discussion up to now, except we now call it the ``global model.''
It is one model for every location. The other models add an indicator
variable for locations. The variable is either an indicator for each
census tract (model 2), an indicator for each city (model 3), or an
indicator for each zip code (model 4).

Figure \ref{figure:SubmarketsCrossValidated} contains the results of a
cross validation study of the four models. We see that the third model,
with the city indicators, outperforms the other models.

<<Cities>>=
pgm <- 'e-city-chart'
options <- ''
base<- paste0(control$working, pgm, options)
base
path.1 <- paste0(base, '_1.txt')
path.2 <- paste0(base, '_2.txt')
path.3 <- paste0(base, '_3.txt')
path.4 <- paste0(base, '_4.txt')

path.1
path.2
@

\begin{figure}[h]
\caption{Submarket City Model: Non-city Indicator Estimated Coefficients}
\label{figure:CityIndicatorsNoncity}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.1}}
\end{figure}

To examine the model with city indicators, we fitted an OLS model to all
the training data (1.2 million transactions) using all the features
inclusive of the city indicators. Figure
\ref{figure:CityIndicatorsNoncity} shows the estimated coefficients for
all the features except for the city indicator features. The p-values of
all but two of the features are consistent with rejecting the null
hypothesis that the coefficient is zero. Many of the signs of the
estimated coefficients are consistent with my prior beliefs, except for
these:
\begin{itemize}
\item effective.year.built: I had expected more recently-built
properties to be worth more. Perhaps there is some interaction with
location that causes the sign to be inverted relative to my expectation.
\item census.tract.has.schoolTRUE: I had thought this sign would be
positive.
\item bedrooms. The negative sign may reflect that more rooms cut up the
living space into smaller rooms, and that may be unattractive.
\item zip5.has.parkTRUE: but the coefficient for
census.tract.has.parkTRUE is positive
\item factor.is.new.constructionTRUE: I had expected new construction to
be more valuable than existing construction; perhaps new construction is
to on average lower standards
\item zip5.has.schoolTRUE: the similar census.tract.has.schoolTRUE is
also negative; both together being negative is a surprise to me
\item fraction.owner.occupied: I had expected higher rates of owner
occupancy to be positively valued, not negatively valued.
\end{itemize}

\begin{figure}[h]
\caption{Submarket City Model: Largest Postive City-Indicator Estimated Coefficients}
\label{figure:CityIndicatorsPositive}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.3}}
\end{figure}

There are many cities in Los Angeles Country. We have fit a model with
an indicator variable for each. To examine the outcome, we divide all
the coefficients into two groups: positive coefficients (indicating the
city has higher-priced properties than on average) and negative
coefficients (indicating lower-priced properties). Within this two
group, we order by the size of the coeffficients.

Figure \ref{figure:CityIndicatorsPositive} shows the largest positive
coefficients in order of decreasing size. We see that the highest
average prices are in Malibu, Beverly Hills, West Hollywood, and Santa
Monica. To me these are not a surprise.

\begin{figure}[h]
\caption{Submarket City Model: Negative Postive City-Indicator Estimated Coefficients}
\label{figure:CityIndicatorsNegative}
%\tiny
\small
%\normalsize
\verbatiminput{\Sexpr{path.4}}
\end{figure}

Figure \ref{figure:CityIndicatorsNegative} shows the largest negative
coefficients in order of decreasing size. I'm not familiar enough with
Los Angeles to judge whether these cities are appropriately on the list.

\section{Census Value}

<<CensusValue>>=
pgm <- 'e-census-value-chart'
options <- '--query-100'
base<- paste0(control$working, pgm, options)
base
path.1 <- paste0(base, '_1.txt')
path.4 <- paste0(base, '_4.pdf')

path.1
path.4
@

The taxroll contains the census tract number for each parcel. We have
the taxroll for year 2008 only, which means we have the census tracts
for year 2008. An investigation into census data determined that census
tract numbers are not stable over time because census tracts are split
by the census bureau when they become too large. Thus we need to
determine if the census tract data are valuable in improving price
estimation accuracy.

\begin{figure}[h]
\caption{
Estimated Generalization Errors from 10-fold Cross Validation \\
Model 1: With Census Tract Data\\
Model 2: Without Census Tract Data
}
\label{figure:CensusTractCv}
\centering\includegraphics[scale=.4]{\Sexpr{path.4}}
\end{figure}

To make this assessment, we estimated the generalization error for two
models, one using the census tract-derived features and one not using
these features. The estimate was created using 10-fold cross validation
and taking a 1 percent sample of the query points in each fold. The
results are in Figure \ref{figure:CensusTractCv}, which shows that using
the census tract features greatly improves the accuracy of the
estimates.

The census tract features are:
\begin{itemize}
\item avg.commute.time: average time to travel to work for workers who
do not work at home
\item fraction.owner.occupied: fraction of houses that are occupied by
their owner
\item median.household.income: the median value for household income
\item census.tract.has.industry: whether any parcel in the census tract
has a land use value consistent with containing industry
\item census.tract.has.park: whether any parcel in the censsus tract has
a land use value consistent with containing a park
\item census.tract.has.retail: whether any parcel in the census tract
has a land use value consistent with containing a retail store
\item census.tract.has.school: whether any parcel in the census tract
has a land use value consistent with containing a school
\end{itemize}

As a result of this analysis, we restrict our work to transactions that
occur on or after January 1, 2003. Thus our training and test data sets
encompass almost 6 years: 2003, 2004, and so forth up to the first 9 months of
2009.

%%% BIBLIOGRAPHY
\bibliographystyle{amsplain}
\bibliography{../../roy-bib/roy-bib}

\end{document}
 
